{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f97659",
   "metadata": {},
   "source": [
    "# Simple Seldonian Algorithm Example\n",
    "\n",
    "#### Author: Dasha Asienga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0470b3c",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to work through the Seldonian algorithm tutorial and understand the computational aspects of the framework.\n",
    "\n",
    "We will be running linear regression on simulated data.\n",
    "\n",
    "** Fill in details from notes on the experiment :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87508d39",
   "metadata": {},
   "source": [
    "## Import Necessary Packages \n",
    "\n",
    "`math` provides access to the standard mathematical functions. \n",
    "\n",
    "`numpy` supports large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "`sys` provides functions and variables used to manipulate different parts of the Python runtime environment.\n",
    "\n",
    "`sklearn` features various classification, regression and clustering algorithms.\n",
    "\n",
    "`scipy.stats` contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more.\n",
    "\n",
    "`scipy.optimize` provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programing, constrained and nonlinear least-squares, root finding, and curve fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ab7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import t\n",
    "from scipy.optimize import minimize # The black-box optimization algorithm used to find a candidate solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db086dc",
   "metadata": {},
   "source": [
    "Now, let's configure how floating-point numbers are displayed when printed to the console. `precision=5` will display 5 decimal places. `suppress=True` suppresses the use of scientific notation for very large or very small numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68437b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbd9b5",
   "metadata": {},
   "source": [
    "## Implement Simple Functions\n",
    "These functions are not specific to Seldonian algorithms, but they will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5236678",
   "metadata": {},
   "source": [
    "### tinv\n",
    "\n",
    "This function returns the inverse of `Student's t` CDF using the degrees of freedom in `nu` for the corresponding probabilities in `p`. It is a Python implementation of Matlab's tinv function: https://www.mathworks.com/help/stats/tinv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d1bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinv(p, nu):\n",
    "    return t.ppf(p, nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433bc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "?t.ppf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3956b9",
   "metadata": {},
   "source": [
    "Find the 95th percentile of the Student's t distribution with 50 degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb923e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6759050245283311"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinv(0.95,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3521c4",
   "metadata": {},
   "source": [
    "### stddev\n",
    "\n",
    "This function computes the sample standard deviation of the vector v, with Bessel's correction. In statistics, Bessel's correction is the use of `n − 1` instead of `n` in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08497c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stddev(v):\n",
    "    n = v.size\n",
    "    variance = (np.var(v) * n) / (n-1) # Variance with Bessel's correction\n",
    "    return np.sqrt(variance)           # Compute the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9d0a7",
   "metadata": {},
   "source": [
    "### ttestUpperBound\n",
    "\n",
    "This function computes a (1-delta)-confidence upper bound on the expected value of a random variable using Student's t-test. It analyzes the data in v, which holds i.i.d. samples of the random variable. The upper confidence bound is given by `sampleMean + sampleStandardDeviation/sqrt(n) * tinv(1-delta, n-1)`, where n is the number of points in v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5edadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttestUpperBound(v, delta):\n",
    "    n  = v.size\n",
    "    res = v.mean() + stddev(v) / math.sqrt(n) * tinv(1.0 - delta, n - 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df357925",
   "metadata": {},
   "source": [
    "### predictTTestUpperBound\n",
    "\n",
    "This function works similarly to `ttestUpperBound`, but returns a more conservative upper bound. This is useful to make the Seldonian algorithm less confident that a given candidate solution is safe, thus making the generated candidate solutions more conservative. Such behavior helps when searching for candidate solutions that are likely to pass the safety test. This function uses data in the vector v to compute all relevant statistics (mean and standard deviation) but assumes that the number of points being analyzed is k instead of |v|.\n",
    "\n",
    "This function is used to estimate what the output of ttestUpperBound would be if it were to be run on a new vector, v, containing values sampled from the same distribution as the points in v. The 2.0 factor in the calculation is used to double the width of the confidence interval when predicting the outcome of the safety test in order to make the algorithm less confident/ more conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1b4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTTestUpperBound(v, delta, k):\n",
    "    # conservative prediction of what the upper bound will be in the safety test for the a given constraint\n",
    "    res = v.mean() + 2.0 * stddev(v) / math.sqrt(k) * tinv(1.0 - delta, k - 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec5660",
   "metadata": {},
   "source": [
    "## Run a Simple Experiment\n",
    "\n",
    "The function `main()` below is set up to run a simple experiment. Notice that it relies on some things that we will need to write:\n",
    "\n",
    "- `generateData` will be a function that generates a data set for us to run the algorithm on.\n",
    "- `gHat1` will be $ĝ_1$ and `gHat2` will be $ĝ_2$.\n",
    "- `QSA` will be our quasi-Seldonian algorithm. The pair of objects returned by QSA is the solution (first element) and a Boolean flag indicating whether a solution that satisfies all behavioral constraints was found (second element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17bab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    np.random.seed(0)  # Create the random number generator to use, with seed zero\n",
    "    numPoints = 5000   # Let's use 5000 points\n",
    "\n",
    "    (X,Y)  = generateData(numPoints)  # Generate the data\n",
    "\n",
    "    # Create the behavioral constraints - each is a gHat function and a confidence level delta\n",
    "    gHats  = [gHat1, gHat2] # The 1st gHat requires MSE < 2.0. The 2nd gHat requires MSE > 1.25\n",
    "    deltas = [0.1, 0.1]\n",
    "\n",
    "    (result, found) = QSA(X, Y, gHats, deltas) # Run the Quasi-Seldonian algorithm\n",
    "    \n",
    "    if found:\n",
    "        print(\"A solution was found: [%.10f, %.10f]\" % (result[0], result[1]))\n",
    "        print(\"fHat of solution (computed over all data, D):\", fHat(result, X, Y))\n",
    "    else:\n",
    "        print(\"No solution found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e18021",
   "metadata": {},
   "source": [
    "## Problem Implementation\n",
    "\n",
    "We now implement the regression problem that we defined earlier: https://aisafety.cs.umass.edu/tutorial2.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971f48",
   "metadata": {},
   "source": [
    "### generateData\n",
    "\n",
    "First, let's write the `generateData` function, which samples data as described in the problem description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f592ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate numPoints data points\n",
    "def generateData(numPoints):\n",
    "    X =     np.random.normal(0.0, 1.0, numPoints) # Sample x from a standard normal distribution\n",
    "    Y = X + np.random.normal(0.0, 1.0, numPoints) # Set y to be x, plus noise from a standard normal distribution\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d625e99",
   "metadata": {},
   "source": [
    "### predict\n",
    "\n",
    "Now, let's write the function that takes in a solution $\\theta$ and an input $X$, and produces as output the prediction of $Y$. In other words, this function will implement $\\hat{y}(X, \\theta)$.\n",
    "\n",
    "Recall $\\hat{y}(X, \\theta) = \\theta_1 X + \\theta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a957b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the weights in theta to predict the output value, y, associated with the provided x.\n",
    "# This function assumes we are performing linear regression, so that theta has two elements: \n",
    "# the y-intercept (first parameter) and slope (second parameter)\n",
    "def predict(theta, x):\n",
    "    return theta[0] + theta[1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbad686",
   "metadata": {},
   "source": [
    "### fHat\n",
    "\n",
    "Next, we write a function $\\hat{f}$, which specifies our primary objective: to minimize the sample mean squared error. since we are attempting to maximize $\\hat{f}$, however, we need to return the negative sample mean squared error, so that maximizing $\\hat{f}$ corresponds to minimizing the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad47857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator of the primary objective, in this case, the negative sample mean squared error\n",
    "def fHat(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = 0.0           # Used to store the sample MSE we are computing\n",
    "    for i in range(n):  # For each point X[i] in the data set ...\n",
    "        prediction = predict(theta, X[i])                # Get the prediction using theta\n",
    "        res += (prediction - Y[i]) * (prediction - Y[i]) # Add the squared error to the result\n",
    "    res /= n            # Divide by the number of points to obtain the sample mean squared error\n",
    "    return -res         # Returns the negative sample mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b424b",
   "metadata": {},
   "source": [
    "### gHat\n",
    "\n",
    "Next, we write the functions $\\hat{g}_1$ and $\\hat{g}_2$ that will be provided as input to the Seldonian algorithm. \n",
    "\n",
    "Recall:\n",
    "- $\\hat{g}_{1,j}(\\theta, D) = (\\hat{y}(X_j, \\theta) - Y_j)^2 - 2.0$\n",
    "- $\\hat{g}_{2,j}(\\theta, D) = 1.25 - (\\hat{y}(X_j, \\theta) - Y_j)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6a3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns unbiased estimates of g_1(theta), computed using the provided data\n",
    "def gHat1(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n",
    "    for i in range(n):\n",
    "        prediction = predict(theta, X[i])                   # Compute the prediction for the i-th data point\n",
    "        res[i] = (prediction - Y[i]) * (prediction - Y[i])  # Compute the squared error for the i-th data point\n",
    "    res = res - 2.0     # We want the MSE to be less than 2.0, so g(theta) = MSE-2.0\n",
    "    return res\n",
    "\n",
    "# Returns unbiased estimates of g_2(theta), computed using the provided data\n",
    "def gHat2(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n",
    "    for i in range(n):\n",
    "        prediction = predict(theta, X[i])                   # Compute the prediction for the i-th data point\n",
    "        res[i] = (prediction - Y[i]) * (prediction - Y[i])  # Compute the squared error for the i-th data point\n",
    "    res = 1.25 - res    # We want the MSE to be at least 1.25, so g(theta) = 1.25-MSE\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3dc93",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "Later in this tutorial we will want the ordinary least-squares solution to be used as a starting point during the search for a candidate solution. The following code implements least squares linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "889c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ordinary least squares linear regression on data (X,Y)\n",
    "def leastSq(X, Y):\n",
    "    X = np.expand_dims(X, axis=1) # Places the input  data in a matrix\n",
    "    Y = np.expand_dims(Y, axis=1) # Places the output data in a matrix\n",
    "    reg = LinearRegression().fit(X, Y)\n",
    "    theta0 = reg.intercept_[0]   # Gets theta0, the y-intercept coefficient\n",
    "    theta1 = reg.coef_[0][0]     # Gets theta0, the slope coefficient\n",
    "    return np.array([theta0, theta1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599341c",
   "metadata": {},
   "source": [
    "We now have all of the libraries that we need and all of the functions to implement the problem that we specified earlier. Now we're ready to start writing our Seldonian algorithm! From here it's easy—by line-count, we've already written most of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f5fd4",
   "metadata": {},
   "source": [
    "## Safety Test\n",
    "\n",
    "Before we implement the safety test, let us write a shell for our quasi-Seldonian algorithm, which we will call QSA. This shell code will show how the safety test will be used. At a high level, we are simply partitioning the data, getting a candidate solution, and running the safety test.\n",
    "\n",
    "Notice also that we are placing 40% of the data in candidateData and 60% in safetyData. This train/ test split is an arbitrary choice and it remains an open question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f04467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Quasi-Seldonian linear regression algorithm operating over data (X,Y).\n",
    "# The pair of objects returned by QSA is the solution (first element) \n",
    "# and a Boolean flag indicating whether a solution was found (second element).\n",
    "def QSA(X, Y, gHats, deltas):\n",
    "  # Put 40% of the data in candidateData (D1), and the rest in safetyData (D2)\n",
    "    candidateData_len = 0.40\n",
    "    candidateData_X, safetyData_X, candidateData_Y, safetyData_Y = train_test_split(\n",
    "      X, Y, test_size=1-candidateData_len, shuffle=False)\n",
    "  \n",
    "  # Get the candidate solution\n",
    "    candidateSolution = getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyData_X.size)\n",
    "\n",
    "  # Run the safety test\n",
    "    passedSafety      = safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas)\n",
    "\n",
    "  # Return the result and success flag\n",
    "    return [candidateSolution, passedSafety]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb760af",
   "metadata": {},
   "source": [
    "Now, let's write the function for the safety test using the helper functions we already have.\n",
    "\n",
    "Recall the pseudocode for the safety test:\n",
    "\n",
    "Return $\\theta_c$ if $$\\forall i \\in \\{1,2,3,...,n\\}, \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_2)) + \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_2))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1} \\leq 0,$$ and No Solution Found (NSF) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eecfaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the safety test on a candidate solution. Returns true if the test is passed.\n",
    "#   candidateSolution: the solution to test. \n",
    "#   (safetyData_X, safetyData_Y): data set D2 to be used in the safety test.\n",
    "#   (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n",
    "def safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas):\n",
    "\n",
    "    for i in range(len(gHats)):  # Loop over behavioral constraints, checking each\n",
    "        g         = gHats[i]  # The current behavioral constraint being checked\n",
    "        delta     = deltas[i] # The confidence level of the constraint\n",
    "\n",
    "    # This is a vector of unbiased estimates of g(candidateSolution) -- defined above\n",
    "        g_samples = g(candidateSolution, safetyData_X, safetyData_Y) \n",
    "\n",
    "    # Check if the i-th behavioral constraint is satisfied\n",
    "        upperBound = ttestUpperBound(g_samples, delta) \n",
    "\n",
    "        if upperBound > 0.0: # If the current constraint was not satisfied, the safety test failed\n",
    "            return False\n",
    "\n",
    "  # If we get here, all of the behavioral constraints were satisfied      \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e7079",
   "metadata": {},
   "source": [
    "We're almost there. All that's left is the the function `getCandidateSolution`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbc185",
   "metadata": {},
   "source": [
    "## Candidate Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab63ad0",
   "metadata": {},
   "source": [
    "Recall the pseudocode for computing the candidate solution:\n",
    "\n",
    "Use a black-box optimization algorithm to compute $\\theta_c$ that approximates a solution to $$\\theta_c \\in arg \\: \\underset{\\theta \\in \\Theta}{max} \\hat{f}(\\theta, D_1)$$ $$s.t. \\forall i \\in \\{1,2,3,...,n\\}, \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_1)) + 2 \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_1))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1} \\leq 0.$$\n",
    "\n",
    "The black box algorithm used to search for a candidate solution is called Powell, which is simply an algorithm designed for finding a local minimum of a function using a bi-directional linear search. Powell, however, is not a constrained algorithm. One way of addressing this limitation is by incorporating the constraint into the objective function as a barrier function. In constrained optimization, a field of mathematics, barrier functions are used to replace inequality constraints by a penalizing term in the objective function that is easier to handle. \n",
    "\n",
    "That is, we will now find an approximate solution to the following unconstrained problem:\n",
    "\n",
    "$$\\theta_c \\in arg \\: \\underset{\\theta \\in \\mathbb{R}^2}{max} \n",
    "    \\begin{cases} \n",
    "      \\hat{f}(\\theta, D_1) &  \\text{if} \\:\\: \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_1)) + 2 \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_1))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1} \\leq 0 \\\\\n",
    "      -100,000 - \\sum_{i=1}^n max(0, \\hat{f}(\\theta, D_1)  \\text{if} \\:\\: \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_1)) + 2 \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_1))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1})) & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "In this case, solutions that are predicted not to pass the safety test will not be selected by the optimization algorithm because we assign a large negative performance to them. This barrier functions encourages Powell to tend towards solutions that will pass the safety test. \n",
    "\n",
    "Let us now write the objective function that Powell will attempt to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7cc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function maximized by getCandidateSolution.\n",
    "#     thetaToEvaluate: the candidate solution to evaluate.\n",
    "#     (candidateData_X, candidateData_Y): the data set D1 used to evaluated the solution.\n",
    "#     (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n",
    "#     safetyDataSize: |D2|, used when computing the conservative upper bound on each behavioral constraint.\n",
    "def candidateObjective(thetaToEvaluate, candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize): \n",
    "\n",
    "  # Get the primary objective of the solution, fHat(thetaToEvaluate)\n",
    "    result = fHat(thetaToEvaluate, candidateData_X, candidateData_Y)\n",
    "\n",
    "    predictSafetyTest = True     # Prediction of what the safety test will return. Initialized to \"True\" = pass\n",
    "    \n",
    "    for i in range(len(gHats)):  # Loop over behavioral constraints, checking each\n",
    "        g         = gHats[i]       # The current behavioral constraint being checked\n",
    "        delta     = deltas[i]      # The confidence level of the constraint\n",
    "\n",
    "    # This is a vector of unbiased estimates of g_i(thetaToEvaluate)\n",
    "        g_samples = g(thetaToEvaluate, candidateData_X, candidateData_Y)\n",
    "\n",
    "    # Get the conservative prediction of what the upper bound on g_i(thetaToEvaluate) will be in the safety test\n",
    "        upperBound = predictTTestUpperBound(g_samples, delta, safetyDataSize)\n",
    "\n",
    "    # We don't think the i-th constraint will pass the safety test if we return this candidate solution\n",
    "        if upperBound > 0.0:\n",
    "\n",
    "            if predictSafetyTest:\n",
    "        # Set this flag to indicate that we don't think the safety test will pass\n",
    "                predictSafetyTest = False  \n",
    "    \n",
    "        # Put a barrier in the objective. Any solution that we think will fail the safety test will have a\n",
    "        # large negative performance associated with it\n",
    "                result = -100000.0    \n",
    "\n",
    "      # Add a shaping to the objective function that will push the search toward solutions that will pass \n",
    "      # the prediction of the safety test\n",
    "            result = result - upperBound\n",
    "\n",
    "  # Negative because our optimizer (Powell) is a minimizer, but we want to maximize the candidate objective\n",
    "    return -result  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c76f0a",
   "metadata": {},
   "source": [
    "Now that we have our candidate objective function, we can write `getCandidateSolution`, which uses Powell to search for a solution that maximizes `candidateObjective`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b6d9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided data to get a candidate solution expected to pass the safety test.\n",
    "#    (candidateData_X, candidateData_Y): data used to compute a candidate solution.\n",
    "#    (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n",
    "#    safetyDataSize: |D2|, used when computing the conservative upper bound on each behavioral constraint.\n",
    "def getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize):\n",
    "  \n",
    "  # Chooses the black-box optimizer we will use (Powell)\n",
    "    minimizer_method = 'Powell'\n",
    "    minimizer_options={'disp': False}\n",
    "\n",
    "  # Initial solution given to Powell: simple linear fit we'd get from ordinary least squares linear regression\n",
    "    initialSolution = leastSq(candidateData_X, candidateData_Y)\n",
    "\n",
    "  # Use Powell to get a candidate solution that tries to maximize candidateObjective\n",
    "    res = minimize(candidateObjective, x0=initialSolution, method=minimizer_method, options=minimizer_options, \n",
    "    args=(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize))\n",
    "\n",
    "  # Return the candidate solution we believe will pass the safety test\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e1dfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "?minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8e939",
   "metadata": {},
   "source": [
    "## Find a Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4d9ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A solution was found: [0.5844721756, 1.0560192943]\n",
      "fHat of solution (computed over all data, D): -1.3494829214565587\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db548654",
   "metadata": {},
   "source": [
    "In other words, our Quasi-Seldonian algorithm found a solution that minimizes the sample mean squared error, while ensuring (with high probability) that all behavioral constraints are satisfied!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccebc20",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4799c",
   "metadata": {},
   "source": [
    "The next tutorial will go through how we can change the code in main() to repeatedly run our Quasi-Seldonian algorithm QSA using different amounts of data and printing results to files that can be used to create plots like Fig. 3 in the original Seldonian paper. This will allow us to analyze:\n",
    "\n",
    "- How much performance (mean square error minimization) is lost due to the behavioral constraints?\n",
    "- How much data does it take for the algorithm to frequently return solutions?\n",
    "- How often does the algorithm exhibit undesirable behavior?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f97659",
   "metadata": {},
   "source": [
    "# Simple Seldonian Algorithm Example\n",
    "\n",
    "#### Author: Dasha Asienga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0470b3c",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to work through the Seldonian algorithm tutorial and understand the computational aspects of the framework.\n",
    "\n",
    "We will be running linear regression on simulated data.\n",
    "\n",
    "** Fill in details from notes on the experiment :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87508d39",
   "metadata": {},
   "source": [
    "## Import Necessary Packages \n",
    "\n",
    "`math` provides access to the standard mathematical functions. \n",
    "\n",
    "`numpy` supports large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "`sys` provides functions and variables used to manipulate different parts of the Python runtime environment.\n",
    "\n",
    "`sklearn` features various classification, regression and clustering algorithms.\n",
    "\n",
    "`scipy.stats` contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more.\n",
    "\n",
    "`scipy.optimize` provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programing, constrained and nonlinear least-squares, root finding, and curve fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ab7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import t\n",
    "from scipy.optimize import minimize # The black-box optimization algorithm used to find a candidate solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db086dc",
   "metadata": {},
   "source": [
    "Now, let's configure how floating-point numbers are displayed when printed to the console. `precision=5` will display 5 decimal places. `suppress=True` suppresses the use of scientific notation for very large or very small numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68437b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbd9b5",
   "metadata": {},
   "source": [
    "## Implement Simple Functions\n",
    "These functions are not specific to Seldonian algorithms, but they will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5236678",
   "metadata": {},
   "source": [
    "### tinv\n",
    "\n",
    "This function returns the inverse of `Student's t` CDF using the degrees of freedom in `nu` for the corresponding probabilities in `p`. It is a Python implementation of Matlab's tinv function: https://www.mathworks.com/help/stats/tinv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d1bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinv(p, nu):\n",
    "    return t.ppf(p, nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433bc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "?t.ppf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3956b9",
   "metadata": {},
   "source": [
    "Find the 95th percentile of the Student's t distribution with 50 degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb923e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6759050245283311"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinv(0.95,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3521c4",
   "metadata": {},
   "source": [
    "### stddev\n",
    "\n",
    "This function computes the sample standard deviation of the vector v, with Bessel's correction. In statistics, Bessel's correction is the use of `n − 1` instead of `n` in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08497c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stddev(v):\n",
    "    n = v.size\n",
    "    variance = (np.var(v) * n) / (n-1) # Variance with Bessel's correction\n",
    "    return np.sqrt(variance)           # Compute the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9d0a7",
   "metadata": {},
   "source": [
    "### ttestUpperBound\n",
    "\n",
    "This function computes a (1-delta)-confidence upper bound on the expected value of a random variable using Student's t-test. It analyzes the data in v, which holds i.i.d. samples of the random variable. The upper confidence bound is given by `sampleMean + sampleStandardDeviation/sqrt(n) * tinv(1-delta, n-1)`, where n is the number of points in v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5edadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttestUpperBound(v, delta):\n",
    "    n  = v.size\n",
    "    res = v.mean() + stddev(v) / math.sqrt(n) * tinv(1.0 - delta, n - 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df357925",
   "metadata": {},
   "source": [
    "### predictTTestUpperBound\n",
    "\n",
    "This function works similarly to `ttestUpperBound`, but returns a more conservative upper bound. This is useful to make the Seldonian algorithm less confident that a given candidate solution is safe, thus making the generated candidate solutions more conservative. Such behavior helps when searching for candidate solutions that are likely to pass the safety test. This function uses data in the vector v to compute all relevant statistics (mean and standard deviation) but assumes that the number of points being analyzed is k instead of |v|.\n",
    "\n",
    "This function is used to estimate what the output of ttestUpperBound would be if it were to be run on a new vector, v, containing values sampled from the same distribution as the points in v. The 2.0 factor in the calculation is used to double the width of the confidence interval when predicting the outcome of the safety test in order to make the algorithm less confident/ more conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a1b4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTTestUpperBound(v, delta, k):\n",
    "    # conservative prediction of what the upper bound will be in the safety test for the a given constraint\n",
    "    res = v.mean() + 2.0 * stddev(v) / math.sqrt(k) * tinv(1.0 - delta, k - 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec5660",
   "metadata": {},
   "source": [
    "## Run a Simple Experiment\n",
    "\n",
    "The function `main()` below is set up to run a simple experiment. Notice that it relies on some things that we will need to write:\n",
    "\n",
    "- `generateData` will be a function that generates a data set for us to run the algorithm on.\n",
    "- `gHat1` will be $ĝ_1$ and `gHat2` will be $ĝ_2$.\n",
    "- `QSA` will be our quasi-Seldonian algorithm. The pair of objects returned by QSA is the solution (first element) and a Boolean flag indicating whether a solution that satisfies all behavioral constraints was found (second element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17bab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    np.random.seed(0)  # Create the random number generator to use, with seed zero\n",
    "    numPoints = 5000   # Let's use 5000 points\n",
    "\n",
    "    (X,Y)  = generateData(numPoints)  # Generate the data\n",
    "\n",
    "    # Create the behavioral constraints - each is a gHat function and a confidence level delta\n",
    "    gHats  = [gHat1, gHat2] # The 1st gHat requires MSE < 2.0. The 2nd gHat requires MSE > 1.25\n",
    "    deltas = [0.1, 0.1]\n",
    "\n",
    "    (result, found) = QSA(X, Y, gHats, deltas) # Run the Quasi-Seldonian algorithm\n",
    "    \n",
    "    if found:\n",
    "        print(\"A solution was found: [%.10f, %.10f]\" % (result[0], result[1]))\n",
    "        print(\"fHat of solution (computed over all data, D):\", fHat(result, X, Y))\n",
    "    else:\n",
    "        print(\"No solution found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e18021",
   "metadata": {},
   "source": [
    "## Problem Implementation\n",
    "\n",
    "We now implement the regression problem that we defined earlier: https://aisafety.cs.umass.edu/tutorial2.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971f48",
   "metadata": {},
   "source": [
    "### generateData\n",
    "\n",
    "First, let's write the `generateData` function, which samples data as described in the problem description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f592ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate numPoints data points\n",
    "def generateData(numPoints):\n",
    "    X =     np.random.normal(0.0, 1.0, numPoints) # Sample x from a standard normal distribution\n",
    "    Y = X + np.random.normal(0.0, 1.0, numPoints) # Set y to be x, plus noise from a standard normal distribution\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d625e99",
   "metadata": {},
   "source": [
    "### predict\n",
    "\n",
    "Now, let's write the function that takes in a solution $\\theta$ and an input $X$, and produces as output the prediction of $Y$. In other words, this function will implement $\\hat{y}(X, \\theta)$.\n",
    "\n",
    "Recall $\\hat{y}(X, \\theta) = \\theta_1 X + \\theta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a957b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the weights in theta to predict the output value, y, associated with the provided x.\n",
    "# This function assumes we are performing linear regression, so that theta has two elements: \n",
    "# the y-intercept (first parameter) and slope (second parameter)\n",
    "def predict(theta, x):\n",
    "    return theta[0] + theta[1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbad686",
   "metadata": {},
   "source": [
    "### fHat\n",
    "\n",
    "Next, we write a function $\\hat{f}$, which specifies our primary objective: to minimize the sample mean squared error. since we are attempting to maximize $\\hat{f}$, however, we need to return the negative sample mean squared error, so that maximizing $\\hat{f}$ corresponds to minimizing the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad47857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator of the primary objective, in this case, the negative sample mean squared error\n",
    "def fHat(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = 0.0           # Used to store the sample MSE we are computing\n",
    "    for i in range(n):  # For each point X[i] in the data set ...\n",
    "        prediction = predict(theta, X[i])                # Get the prediction using theta\n",
    "        res += (prediction - Y[i]) * (prediction - Y[i]) # Add the squared error to the result\n",
    "    res /= n            # Divide by the number of points to obtain the sample mean squared error\n",
    "    return -res         # Returns the negative sample mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b424b",
   "metadata": {},
   "source": [
    "### gHat\n",
    "\n",
    "Next, we write the functions $\\hat{g}_1$ and $\\hat{g}_2$ that will be provided as input to the Seldonian algorithm. \n",
    "\n",
    "Recall:\n",
    "- $\\hat{g}_{1,j}(\\theta, D) = (\\hat{y}(X_j, \\theta) - Y_j)^2 - 2.0$\n",
    "- $\\hat{g}_{2,j}(\\theta, D) = 1.25 - (\\hat{y}(X_j, \\theta) - Y_j)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da6a3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns unbiased estimates of g_1(theta), computed using the provided data\n",
    "def gHat1(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n",
    "    for i in range(n):\n",
    "        prediction = predict(theta, X[i])                   # Compute the prediction for the i-th data point\n",
    "        res[i] = (prediction - Y[i]) * (prediction - Y[i])  # Compute the squared error for the i-th data point\n",
    "    res = res - 2.0     # We want the MSE to be less than 2.0, so g(theta) = MSE-2.0\n",
    "    return res\n",
    "\n",
    "# Returns unbiased estimates of g_2(theta), computed using the provided data\n",
    "def gHat2(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n",
    "    for i in range(n):\n",
    "        prediction = predict(theta, X[i])                   # Compute the prediction for the i-th data point\n",
    "        res[i] = (prediction - Y[i]) * (prediction - Y[i])  # Compute the squared error for the i-th data point\n",
    "    res = 1.25 - res    # We want the MSE to be at least 1.25, so g(theta) = 1.25-MSE\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3dc93",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "Later in this tutorial we will want the ordinary least-squares solution to be used as a starting point during the search for a candidate solution. The following code implements least squares linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "889c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ordinary least squares linear regression on data (X,Y)\n",
    "def leastSq(X, Y):\n",
    "    X = np.expand_dims(X, axis=1) # Places the input  data in a matrix\n",
    "    Y = np.expand_dims(Y, axis=1) # Places the output data in a matrix\n",
    "    reg = LinearRegression().fit(X, Y)\n",
    "    theta0 = reg.intercept_[0]   # Gets theta0, the y-intercept coefficient\n",
    "    theta1 = reg.coef_[0][0]     # Gets theta0, the slope coefficient\n",
    "    return np.array([theta0, theta1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599341c",
   "metadata": {},
   "source": [
    "We now have all of the libraries that we need and all of the functions to implement the problem that we specified earlier. Now we're ready to start writing our Seldonian algorithm! From here it's easy—by line-count, we've already written most of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f5fd4",
   "metadata": {},
   "source": [
    "## Safety Test\n",
    "\n",
    "Before we implement the safety test, let us write a shell for our quasi-Seldonian algorithm, which we will call QSA. This shell code will show how the safety test will be used. At a high level, we are simply partitioning the data, getting a candidate solution, and running the safety test.\n",
    "\n",
    "Notice also that we are placing 40% of the data in candidateData and 60% in safetyData. This train/ test split is an arbitrary choice and it remains an open question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69f04467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Quasi-Seldonian linear regression algorithm operating over data (X,Y).\n",
    "# The pair of objects returned by QSA is the solution (first element) \n",
    "# and a Boolean flag indicating whether a solution was found (second element).\n",
    "def QSA(X, Y, gHats, deltas):\n",
    "  # Put 40% of the data in candidateData (D1), and the rest in safetyData (D2)\n",
    "    candidateData_len = 0.40\n",
    "    candidateData_X, safetyData_X, candidateData_Y, safetyData_Y = train_test_split(\n",
    "      X, Y, test_size=1-candidateData_len, shuffle=False)\n",
    "  \n",
    "  # Get the candidate solution\n",
    "    candidateSolution = getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyData_X.size)\n",
    "\n",
    "  # Run the safety test\n",
    "    passedSafety      = safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas)\n",
    "\n",
    "  # Return the result and success flag\n",
    "    return [candidateSolution, passedSafety]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb760af",
   "metadata": {},
   "source": [
    "Now, let's write the function for the safety test using the helper functions we already have.\n",
    "\n",
    "Recall the pseudocode for the safety test:\n",
    "\n",
    "Return $\\theta_c$ if $$\\forall i \\in \\{1,2,3,...,n\\}, \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_2)) + \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_2))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1} \\leq 0,$$ and No Solution Found (NSF) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eecfaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the safety test on a candidate solution. Returns true if the test is passed.\n",
    "#   candidateSolution: the solution to test. \n",
    "#   (safetyData_X, safetyData_Y): data set D2 to be used in the safety test.\n",
    "#   (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n",
    "def safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas):\n",
    "\n",
    "    for i in range(len(gHats)):  # Loop over behavioral constraints, checking each\n",
    "        g         = gHats[i]  # The current behavioral constraint being checked\n",
    "        delta     = deltas[i] # The confidence level of the constraint\n",
    "\n",
    "    # This is a vector of unbiased estimates of g(candidateSolution) -- defined above\n",
    "        g_samples = g(candidateSolution, safetyData_X, safetyData_Y) \n",
    "\n",
    "    # Check if the i-th behavioral constraint is satisfied\n",
    "        upperBound = ttestUpperBound(g_samples, delta) \n",
    "\n",
    "        if upperBound > 0.0: # If the current constraint was not satisfied, the safety test failed\n",
    "            return False\n",
    "\n",
    "  # If we get here, all of the behavioral constraints were satisfied      \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e7079",
   "metadata": {},
   "source": [
    "We're almost there. All that's left is the the function `getCandidateSolution`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbc185",
   "metadata": {},
   "source": [
    "## Candidate Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab63ad0",
   "metadata": {},
   "source": [
    "Recall the pseudocode for computing the candidate solution:\n",
    "\n",
    "Use a black-box optimization algorithm to compute $\\theta_c$ that approximates a solution to $$\\theta_c \\in arg \\: \\underset{\\theta \\in \\Theta}{max} \\hat{f}(\\theta, D_1)$$ $$s.t. \\forall i \\in \\{1,2,3,...,n\\}, \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_1)) + 2 \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_1))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1} \\leq 0.$$\n",
    "\n",
    "The black box algorithm used to search for a candidate solution is called Powell, which is simply an algorithm designed for finding a local minimum of a function using a bi-directional linear search. Powell, however, is not a constrained algorithm. One way of addressing this limitation is by incorporating the constraint into the objective function as a barrier function. In constrained optimization, a field of mathematics, barrier functions are used to replace inequality constraints by a penalizing term in the objective function that is easier to handle. \n",
    "\n",
    "That is, we will now find an approximate solution to the following unconstrained problem:\n",
    "\n",
    "$$\\theta_c \\in arg \\: \\underset{\\theta \\in \\mathbb{R}^2}{max} \n",
    "    \\begin{cases} \n",
    "      \\hat{f}(\\theta, D_1) &  \\text{if} \\:\\: \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_1)) + 2 \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_1))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1} \\leq 0 \\\\\n",
    "      -100,000 - \\sum_{i=1}^n max(0, \\hat{f}(\\theta, D_1)  \\text{if} \\:\\: \\hat{\\mu}(\\hat{g_i}(\\theta_c, D_1)) + 2 \\frac{\\hat{\\sigma}(\\hat{g_i}(\\theta_c, D_1))}{\\sqrt{|D_2|}} t_{1-\\delta_i, |D_2|-1})) & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "In this case, solutions that are predicted not to pass the safety test will not be selected by the optimization algorithm because we assign a large negative performance to them. This barrier functions encourages Powell to tend towards solutions that will pass the safety test. \n",
    "\n",
    "Let us now write the objective function that Powell will attempt to maximize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7cc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The objective function maximized by getCandidateSolution.\n",
    "#     thetaToEvaluate: the candidate solution to evaluate.\n",
    "#     (candidateData_X, candidateData_Y): the data set D1 used to evaluated the solution.\n",
    "#     (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n",
    "#     safetyDataSize: |D2|, used when computing the conservative upper bound on each behavioral constraint.\n",
    "def candidateObjective(thetaToEvaluate, candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize): \n",
    "\n",
    "  # Get the primary objective of the solution, fHat(thetaToEvaluate)\n",
    "    result = fHat(thetaToEvaluate, candidateData_X, candidateData_Y)\n",
    "\n",
    "    predictSafetyTest = True     # Prediction of what the safety test will return. Initialized to \"True\" = pass\n",
    "    \n",
    "    for i in range(len(gHats)):  # Loop over behavioral constraints, checking each\n",
    "        g         = gHats[i]       # The current behavioral constraint being checked\n",
    "        delta     = deltas[i]      # The confidence level of the constraint\n",
    "\n",
    "    # This is a vector of unbiased estimates of g_i(thetaToEvaluate)\n",
    "        g_samples = g(thetaToEvaluate, candidateData_X, candidateData_Y)\n",
    "\n",
    "    # Get the conservative prediction of what the upper bound on g_i(thetaToEvaluate) will be in the safety test\n",
    "        upperBound = predictTTestUpperBound(g_samples, delta, safetyDataSize)\n",
    "\n",
    "    # We don't think the i-th constraint will pass the safety test if we return this candidate solution\n",
    "        if upperBound > 0.0:\n",
    "\n",
    "            if predictSafetyTest:\n",
    "        # Set this flag to indicate that we don't think the safety test will pass\n",
    "                predictSafetyTest = False  \n",
    "    \n",
    "        # Put a barrier in the objective. Any solution that we think will fail the safety test will have a\n",
    "        # large negative performance associated with it\n",
    "                result = -100000.0    \n",
    "\n",
    "      # Add a shaping to the objective function that will push the search toward solutions that will pass \n",
    "      # the prediction of the safety test\n",
    "            result = result - upperBound\n",
    "\n",
    "  # Negative because our optimizer (Powell) is a minimizer, but we want to maximize the candidate objective\n",
    "    return -result  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c76f0a",
   "metadata": {},
   "source": [
    "Now that we have our candidate objective function, we can write `getCandidateSolution`, which uses Powell to search for a solution that maximizes `candidateObjective`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b6d9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided data to get a candidate solution expected to pass the safety test.\n",
    "#    (candidateData_X, candidateData_Y): data used to compute a candidate solution.\n",
    "#    (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n",
    "#    safetyDataSize: |D2|, used when computing the conservative upper bound on each behavioral constraint.\n",
    "def getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize):\n",
    "  \n",
    "  # Chooses the black-box optimizer we will use (Powell)\n",
    "    minimizer_method = 'Powell'\n",
    "    minimizer_options={'disp': False}\n",
    "\n",
    "  # Initial solution given to Powell: simple linear fit we'd get from ordinary least squares linear regression\n",
    "    initialSolution = leastSq(candidateData_X, candidateData_Y)\n",
    "\n",
    "  # Use Powell to get a candidate solution that tries to maximize candidateObjective\n",
    "    res = minimize(candidateObjective, x0=initialSolution, method=minimizer_method, options=minimizer_options, \n",
    "    args=(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize))\n",
    "\n",
    "  # Return the candidate solution we believe will pass the safety test\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1dfb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "?minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c8e939",
   "metadata": {},
   "source": [
    "## Find a Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4d9ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A solution was found: [0.5844721756, 1.0560192943]\n",
      "fHat of solution (computed over all data, D): -1.3494829214565587\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db548654",
   "metadata": {},
   "source": [
    "In other words, our Quasi-Seldonian algorithm found a solution that minimizes the sample mean squared error, while ensuring (with high probability) that all behavioral constraints are satisfied!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccebc20",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d4799c",
   "metadata": {},
   "source": [
    "The next tutorial will go through how we can change the code in main() to repeatedly run our Quasi-Seldonian algorithm QSA using different amounts of data and printing results to files that can be used to create plots like Fig. 3 in the original Seldonian paper. This will allow us to analyze:\n",
    "\n",
    "- How much performance (mean square error minimization) is lost due to the behavioral constraints?\n",
    "- How much data does it take for the algorithm to frequently return solutions?\n",
    "- How often does the algorithm exhibit undesirable behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679bdd11",
   "metadata": {},
   "source": [
    "## Import Additional Packages\n",
    "\n",
    "`timeit` allows us to time the execution of our experiments.\n",
    "\n",
    "`ray` allows us to execute experiments in parallel. \n",
    "\n",
    "`number` allows us to use `jit`, which is a Just-in-Time (JIT) compiler to accelerate Python code.\n",
    "\n",
    "Note that the pip install statements below should not be run twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b92bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bed06cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "814d364c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 21:08:31,703\tINFO worker.py:1642 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import timeit               \n",
    "import ray                  \n",
    "ray.init()    \n",
    "from numba import jit       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de2aba",
   "metadata": {},
   "source": [
    "## Run Multiple Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725292d",
   "metadata": {},
   "source": [
    "Let's first create a folder where the experiment results will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48178ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_path = 'experiment_results_dasha/bin/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8ff4d",
   "metadata": {},
   "source": [
    "Now, let's set up the experiments.\n",
    "\n",
    "We want to investigate performance loss, probability of a solution, and probability of undesired behavior.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "1. The `.npz` file type is a file format used in NumPy, a popular library for numerical and scientific computing in Python. It is primarily used for saving and loading multiple NumPy arrays into a single compressed archive file. The `.npz` format is a way to store multiple arrays in a single file, which can be useful for data storage and sharing in scientific and data analysis applications. It's commonly used in scientific research and data analysis workflows to save and load experimental data, model parameters, or any other numerical data that needs to be preserved in a structured and efficient manner.\n",
    "\n",
    "2. `np.savez()` is a function in the NumPy library for Python that is used to save multiple NumPy arrays into a single compressed archive file with the `.npz` extension. \n",
    "\n",
    "3. We will have `numTrials` trials for each value of `ms` data set size. That is, in trial 1, we will generate training data sets of size ms and run QSA. Then, we will move on to trial 2 and do the same. At the end, we will have multiple trials for each data set size we are interested in. We will keep the random seed consistent within a trial but different between different trials. \n",
    "\n",
    "4. We will keep the test set constant across the entire experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15fd4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def run_experiments(worker_id, nWorkers, ms, numM, numTrials, mTest):\n",
    "    \n",
    "    # Results of the Seldonian algorithm runs\n",
    "    ## The following code initializes an array filled with 0's. The resulting array will have numTrials rows (each trial) and numM columns (each data set size). Default is 0=False.\n",
    "    \n",
    "    seldonian_solutions_found = np.zeros((numTrials, numM)) # Stores whether a solution was found (1=True,0=False)\n",
    "    seldonian_failures_g1     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 1st constraint, g_1\n",
    "    seldonian_failures_g2     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2\n",
    "    seldonian_fs              = np.zeros((numTrials, numM)) # Stores the primary objective values (fHat) if a solution was found\n",
    "    \n",
    "    # Results of the Least-Squares (LS) linear regression runs\n",
    "    LS_solutions_found = np.ones((numTrials, numM))  # Stores whether a solution was found. These will all be true (=1)\n",
    "    LS_failures_g1     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 1st constraint, g_1\n",
    "    LS_failures_g2     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2\n",
    "    LS_fs              = np.zeros((numTrials, numM)) # Stores the primary objective values (f) if a solution was found\n",
    "    \n",
    "    \n",
    "    # Prepares file where experiment results will be saved\n",
    "    experiment_number = worker_id\n",
    "    outputFile = bin_path + 'results%d.npz' % experiment_number\n",
    "    print(\"Writing output to\", outputFile)\n",
    "    \n",
    "    \n",
    "    # Generate the data used to evaluate the primary objective and failure rates\n",
    "    np.random.seed( (experiment_number+1) * 9999 )\n",
    "    (testX, testY) = generateData(mTest) #we defined this above & mTest is the number of points in the test set \n",
    "    \n",
    "    \n",
    "    for trial in range(numTrials): #numTrials trials for each value of m \n",
    "        for (mIndex, m) in enumerate(ms): #different amounts of data to evaluate in each trial\n",
    "            # Generate the training data, D\n",
    "            base_seed         = (experiment_number * numTrials)+1\n",
    "            np.random.seed(base_seed+trial) # done to obtain common random numbers for all values of m (all data set sizes) in the same trial but different numbers for different trials\n",
    "            (trainX, trainY)  = generateData(m)\n",
    "            \n",
    "            # Run the Quasi-Seldonian algorithm\n",
    "            (result, passedSafetyTest) = QSA(trainX, trainY, gHats, deltas)\n",
    "            \n",
    "            if passedSafetyTest:\n",
    "                seldonian_solutions_found[trial, mIndex] = 1                        # A solution was found \n",
    "                trueMSE = -fHat(result, testX, testY)                               # Get the \"true\" mean squared error using the testData\n",
    "                seldonian_failures_g1[trial, mIndex] = 1 if trueMSE > 2.0  else 0   # Check if the first behavioral constraint was violated\n",
    "                seldonian_failures_g2[trial, mIndex] = 1 if trueMSE < 1.25 else 0   # Check if the second behavioral constraint was violated\n",
    "                seldonian_fs[trial, mIndex] = -trueMSE                              # Store the \"true\" negative mean-squared error (goal of maximizing)\n",
    "                print(f\"[(worker {worker_id}/{nWorkers}) Seldonian trial {trial+1}/{numTrials}, m {m}] A solution was found: [{result[0]:.10f}, {result[1]:.10f}]\\tfHat over test data: {trueMSE:.10f}\")\n",
    "            else:\n",
    "                seldonian_solutions_found[trial, mIndex] = 0             # A solution was not found\n",
    "                seldonian_failures_g1[trial, mIndex]     = 0             # Returning NSF means the first constraint was not violated\n",
    "                seldonian_failures_g2[trial, mIndex]     = 0             # Returning NSF means the second constraint was not violated\n",
    "                seldonian_fs[trial, mIndex]              = None          # This value should not be used later. We use None and later remove the None values\n",
    "                print(f\"[(worker {worker_id}/{nWorkers}) Seldonian trial {trial+1}/{numTrials}, m {m}] No solution found\")\n",
    "\n",
    "            # Run the Least Squares algorithm\n",
    "            theta = leastSq(trainX, trainY)                              # Run least squares linear regression\n",
    "            trueMSE = -fHat(theta, testX, testY)                         # Get the \"true\" mean squared error using the testData\n",
    "            LS_failures_g1[trial, mIndex] = 1 if trueMSE > 2.0  else 0   # Check if the first behavioral constraint was violated\n",
    "            LS_failures_g2[trial, mIndex] = 1 if trueMSE < 1.25 else 0   # Check if the second behavioral constraint was violated\n",
    "            LS_fs[trial, mIndex] = -trueMSE                              # Store the \"true\" negative mean-squared error\n",
    "            print(f\"[(worker {worker_id}/{nWorkers}) LeastSq trial {trial+1}/{numTrials}, m {m}] LS fHat over test data: {trueMSE:.10f}\")\n",
    "            \n",
    "        print()\n",
    "        \n",
    "        \n",
    "    # Save the arrays in a compressed format\n",
    "    np.savez(outputFile, \n",
    "             ms=ms, \n",
    "             seldonian_solutions_found=seldonian_solutions_found,\n",
    "             seldonian_fs=seldonian_fs, \n",
    "             seldonian_failures_g1=seldonian_failures_g1, \n",
    "             seldonian_failures_g2=seldonian_failures_g2,\n",
    "             LS_solutions_found=LS_solutions_found,\n",
    "             LS_fs=LS_fs,\n",
    "             LS_failures_g1=LS_failures_g1,\n",
    "             LS_failures_g2=LS_failures_g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf3aa8",
   "metadata": {},
   "source": [
    "Finally, let's run the experiments.\n",
    "\n",
    "In parallel computing or multithreading applications, setting the number of workers or threads can have a significant impact on performance, as it determines how many tasks can be executed simultaneously, , assuming there are enough computing resources available to support this level of parallelism. This can potentially lead to faster execution of experiments or computations.\n",
    "\n",
    "In a logarithmic scale, values are not linearly spaced but instead increase or decrease by a constant multiplicative factor, typically represented as a power of some base value. We will use a base value of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "431cb251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments on 16 threads\n",
      "\u001b[2m\u001b[36m(run_experiments pid=7517)\u001b[0m [(worker 1/16) LeastSq trial 5/70, m 65536] LS fHat over test data: 0.9995542631\n",
      "\u001b[2m\u001b[36m(run_experiments pid=7517)\u001b[0m \n",
      "\u001b[2m\u001b[36m(run_experiments pid=7517)\u001b[0m [(worker 1/16) Seldonian trial 6/70, m 32] No solution found\n",
      "\u001b[2m\u001b[36m(run_experiments pid=7516)\u001b[0m [(worker 2/16) Seldonian trial 5/70, m 65536] A solution was found: [0.5431828326, 0.9985814431]\tfHat over test data: 1.2946872851\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ny/8q3t_m8d3hgbv7qr1gsnvnhm0000gn/T/ipykernel_7501/2302806299.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Start 'nWorkers' threads in parallel, each one running 'numTrials' trials. Each thread saves its results to a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_experiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnWorkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumTrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmTest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mworker_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnWorkers\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtime_parallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;31m# Elapsed time in seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2541\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         )\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the behavioral constraints: each is a gHat function and a confidence level delta\n",
    "gHats  = [gHat1, gHat2]\n",
    "deltas = [0.1, 0.1]\n",
    "\n",
    "nWorkers = 16                # Workers is the number of threads running experiments in parallel\n",
    "print(f\"Running experiments on {nWorkers} threads\") \n",
    "\n",
    "# We will use different amounts of data, m. The different values of m will be stored in ms.\n",
    "# These values correspond to the horizontal axis locations in all three plots we will make.\n",
    "# We will use a logarithmic horizontal axis, so the amounts of data we use shouldn't be evenly spaced.\n",
    "ms   = [2**i for i in range(5, 17)]  # ms = [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]\n",
    "numM = len(ms)\n",
    "    \n",
    "# How many trials should we average over?\n",
    "#numTrials = 70 # We pick 70 because with 70 trials per worker, and 16 workers, we get >1000 trials for each value of m\n",
    "numTrials = 5\n",
    "\n",
    "# How much data should we generate to compute the estimates of the primary objective and behavioral constraint function values \n",
    "# that we call \"ground truth\"? Each candidate solution deemed safe, and identified using limited training data, will be evaluated \n",
    "# over this large number of points to check whether it is really safe, and to compute its \"true\" mean squared error.\n",
    "mTest = ms[-1] * 100 # about 5,000,000 test samples\n",
    "\n",
    "# Start 'nWorkers' threads in parallel, each one running 'numTrials' trials. Each thread saves its results to a file\n",
    "tic = timeit.default_timer()\n",
    "_ = ray.get([run_experiments.remote(worker_id, nWorkers, ms, numM, numTrials, mTest) for worker_id in range(1,nWorkers+1)])\n",
    "toc = timeit.default_timer()\n",
    "time_parallel = toc - tic # Elapsed time in seconds\n",
    "print(f\"Time ellapsed: {time_parallel}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

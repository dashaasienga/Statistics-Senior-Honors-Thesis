{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f97659",
   "metadata": {},
   "source": [
    "# Simple Seldonian Algorithm Example\n",
    "\n",
    "#### Author: Dasha Asienga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0470b3c",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to work through the Seldonian algorithm tutorial and understand the computational aspects of the framework.\n",
    "\n",
    "We will be running linear regression on simulated data.\n",
    "\n",
    "** Fill in details from notes on the experiment :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87508d39",
   "metadata": {},
   "source": [
    "## Import Necessary Packages \n",
    "\n",
    "`math` provides access to the standard mathematical functions. \n",
    "\n",
    "`numpy` supports large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "`sys` provides functions and variables used to manipulate different parts of the Python runtime environment.\n",
    "\n",
    "`sklearn` features various classification, regression and clustering algorithms.\n",
    "\n",
    "`scipy.stats` contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more.\n",
    "\n",
    "`scipy.optimize` provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programing, constrained and nonlinear least-squares, root finding, and curve fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83ab7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import t\n",
    "from scipy.optimize import minimize # The black-box optimization algorithm used to find a candidate solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db086dc",
   "metadata": {},
   "source": [
    "Now, let's configure how floating-point numbers are displayed when printed to the console. `precision=5` will display 5 decimal places. `suppress=True` suppresses the use of scientific notation for very large or very small numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68437b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbd9b5",
   "metadata": {},
   "source": [
    "## Implement Simple Functions\n",
    "These functions are not specific to Seldonian algorithms, but they will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5236678",
   "metadata": {},
   "source": [
    "### tinv\n",
    "\n",
    "This function returns the inverse of `Student's t` CDF using the degrees of freedom in `nu` for the corresponding probabilities in `p`. It is a Python implementation of Matlab's tinv function: https://www.mathworks.com/help/stats/tinv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2d1bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tinv(p, nu):\n",
    "    return t.ppf(p, nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433bc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "?t.ppf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3956b9",
   "metadata": {},
   "source": [
    "Find the 95th percentile of the Student's t distribution with 50 degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb923e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6759050245283311"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinv(0.95,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3521c4",
   "metadata": {},
   "source": [
    "### stddev\n",
    "\n",
    "This function computes the sample standard deviation of the vector v, with Bessel's correction. In statistics, Bessel's correction is the use of `n − 1` instead of `n` in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08497c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stddev(v):\n",
    "    n = v.size\n",
    "    variance = (np.var(v) * n) / (n-1) # Variance with Bessel's correction\n",
    "    return np.sqrt(variance)           # Compute the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9d0a7",
   "metadata": {},
   "source": [
    "### ttestUpperBound\n",
    "\n",
    "This function computes a (1-delta)-confidence upper bound on the expected value of a random variable using Student's t-test. It analyzes the data in v, which holds i.i.d. samples of the random variable. The upper confidence bound is given by `sampleMean + sampleStandardDeviation/sqrt(n) * tinv(1-delta, n-1)`, where n is the number of points in v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5edadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttestUpperBound(v, delta):\n",
    "    n  = v.size\n",
    "    res = v.mean() + stddev(v) / math.sqrt(n) * tinv(1.0 - delta, n - 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df357925",
   "metadata": {},
   "source": [
    "### predictTTestUpperBound\n",
    "\n",
    "This function works similarly to `ttestUpperBound`, but returns a more conservative upper bound. This is useful to make the Seldonian algorithm less confident that a given candidate solution is safe, thus making the generated candidate solutions more conservative. Such behavior helps when searching for candidate solutions that are likely to pass the safety test. This function uses data in the vector v to compute all relevant statistics (mean and standard deviation) but assumes that the number of points being analyzed is k instead of |v|.\n",
    "\n",
    "This function is used to estimate what the output of ttestUpperBound would be if it were to be run on a new vector, v, containing values sampled from the same distribution as the points in v. The 2.0 factor in the calculation is used to double the width of the confidence interval when predicting the outcome of the safety test in order to make the algorithm less confident/ more conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1b4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTTestUpperBound(v, delta, k):\n",
    "    # conservative prediction of what the upper bound will be in the safety test for the a given constraint\n",
    "    res = v.mean() + 2.0 * stddev(v) / math.sqrt(k) * tinv(1.0 - delta, k - 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec5660",
   "metadata": {},
   "source": [
    "## Run a Simple Experiment\n",
    "\n",
    "The function `main()` below is set up to run a simple experiment. Notice that it relies on some things that we will need to write:\n",
    "\n",
    "- `generateData` will be a function that generates a data set for us to run the algorithm on.\n",
    "- `gHat1` will be $ĝ_1$ and `gHat2` will be $ĝ_2$.\n",
    "- `QSA` will be our quasi-Seldonian algorithm. The pair of objects returned by QSA is the solution (first element) and a Boolean flag indicating whether a solution that satisfies all behavioral constraints was found (second element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17bab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    np.random.seed(0)  # Create the random number generator to use, with seed zero\n",
    "    numPoints = 5000   # Let's use 5000 points\n",
    "\n",
    "    (X,Y)  = generateData(numPoints)  # Generate the data\n",
    "\n",
    "    # Create the behavioral constraints - each is a gHat function and a confidence level delta\n",
    "    gHats  = [gHat1, gHat2] # The 1st gHat requires MSE < 2.0. The 2nd gHat requires MSE > 1.25\n",
    "    deltas = [0.1, 0.1]\n",
    "\n",
    "    (result, found) = QSA(X, Y, gHats, deltas) # Run the Quasi-Seldonian algorithm\n",
    "    \n",
    "    if found:\n",
    "        print(\"A solution was found: [%.10f, %.10f]\" % (result[0], result[1]))\n",
    "        print(\"fHat of solution (computed over all data, D):\", fHat(result, X, Y))\n",
    "    else:\n",
    "        print(\"No solution found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e18021",
   "metadata": {},
   "source": [
    "## Problem Implementation\n",
    "\n",
    "We now implement the regression problem that we defined earlier: https://aisafety.cs.umass.edu/tutorial2.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a971f48",
   "metadata": {},
   "source": [
    "### generateData\n",
    "\n",
    "First, let's write the `generateData` function, which samples data as described in the problem description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f592ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate numPoints data points\n",
    "def generateData(numPoints):\n",
    "    X =     np.random.normal(0.0, 1.0, numPoints) # Sample x from a standard normal distribution\n",
    "    Y = X + np.random.normal(0.0, 1.0, numPoints) # Set y to be x, plus noise from a standard normal distribution\n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d625e99",
   "metadata": {},
   "source": [
    "### predict\n",
    "\n",
    "Now, let's write the function that takes in a solution $\\theta$ and an input $X$, and produces as output the prediction of $Y$. In other words, this function will implement $\\hat{y}(X, \\theta)$.\n",
    "\n",
    "Recall $\\hat{y}(X, \\theta) = \\theta_1 X + \\theta_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a957b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the weights in theta to predict the output value, y, associated with the provided x.\n",
    "# This function assumes we are performing linear regression, so that theta has two elements: \n",
    "# the y-intercept (first parameter) and slope (second parameter)\n",
    "def predict(theta, x):\n",
    "  return theta[0] + theta[1] * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbad686",
   "metadata": {},
   "source": [
    "### fHat\n",
    "\n",
    "Next, we write a function $\\hat{f}$, which specifies our primary objective: to minimize the sample mean squared error. since we are attempting to maximize $\\hat{f}$, however, we need to return the negative sample mean squared error, so that maximizing $\\hat{f}$ corresponds to minimizing the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad47857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator of the primary objective, in this case, the negative sample mean squared error\n",
    "def fHat(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = 0.0           # Used to store the sample MSE we are computing\n",
    "    for i in range(n):  # For each point X[i] in the data set ...\n",
    "        prediction = predict(theta, X[i])                # Get the prediction using theta\n",
    "        res += (prediction - Y[i]) * (prediction - Y[i]) # Add the squared error to the result\n",
    "    res /= n            # Divide by the number of points to obtain the sample mean squared error\n",
    "    return -res         # Returns the negative sample mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b424b",
   "metadata": {},
   "source": [
    "### gHat\n",
    "\n",
    "Next, we write the functions $\\hat{g}_1$ and $\\hat{g}_2$ that will be provided as input to the Seldonian algorithm. \n",
    "\n",
    "Recall:\n",
    "- $\\hat{g}_{1,j}(\\theta, D) = (\\hat{y}(X_j, \\theta) - Y_j)^2 - 2.0$\n",
    "- $\\hat{g}_{2,j}(\\theta, D) = 1.25 - (\\hat{y}(X_j, \\theta) - Y_j)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da6a3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns unbiased estimates of g_1(theta), computed using the provided data\n",
    "def gHat1(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n",
    "    for i in range(n):\n",
    "        prediction = predict(theta, X[i])                   # Compute the prediction for the i-th data point\n",
    "        res[i] = (prediction - Y[i]) * (prediction - Y[i])  # Compute the squared error for the i-th data point\n",
    "    res = res - 2.0     # We want the MSE to be less than 2.0, so g(theta) = MSE-2.0\n",
    "    return res\n",
    "\n",
    "# Returns unbiased estimates of g_2(theta), computed using the provided data\n",
    "def gHat2(theta, X, Y):\n",
    "    n = X.size          # Number of points in the data set\n",
    "    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n",
    "    for i in range(n):\n",
    "        prediction = predict(theta, X[i])                   # Compute the prediction for the i-th data point\n",
    "        res[i] = (prediction - Y[i]) * (prediction - Y[i])  # Compute the squared error for the i-th data point\n",
    "    res = 1.25 - res    # We want the MSE to be at least 1.25, so g(theta) = 1.25-MSE\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3dc93",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "Later in this tutorial we will want the ordinary least-squares solution to be used as a starting point during the search for a candidate solution. The following code implements least squares linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "889c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ordinary least squares linear regression on data (X,Y)\n",
    "def leastSq(X, Y):\n",
    "    X = np.expand_dims(X, axis=1) # Places the input  data in a matrix\n",
    "    Y = np.expand_dims(Y, axis=1) # Places the output data in a matrix\n",
    "    reg = LinearRegression().fit(X, Y)\n",
    "    theta0 = reg.intercept_[0]   # Gets theta0, the y-intercept coefficient\n",
    "    theta1 = reg.coef_[0][0]     # Gets theta0, the slope coefficient\n",
    "    return np.array([theta0, theta1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3599341c",
   "metadata": {},
   "source": [
    "We now have all of the libraries that we need and all of the functions to implement the problem that we specified earlier. Now we're ready to start writing our Seldonian algorithm! From here it's easy—by line-count, we've already written most of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f5fd4",
   "metadata": {},
   "source": [
    "## Safety Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bbc185",
   "metadata": {},
   "source": [
    "## Candidate Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

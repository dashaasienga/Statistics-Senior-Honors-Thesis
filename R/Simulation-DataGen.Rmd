---
title: "Thesis Simulation Data Generation Document for Chapter 4"
author: "Dasha Asienga"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: TRUE

---

```{r setup, include=FALSE}
library(mosaic)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(knitr)
library(kableExtra)
library(ggplot2)
library(GGally)
library(MASS)
library(reshape2)
library(pROC)
```

$\\$

This file is intended to try out and test different data generation mechanisms.

$\\$

We're interested in creating a data set that has 50-50 class balance, even across the demographic group, and also has better predictive performance than the COMPAS tool. For this set-up, we will only use 2 variables from the COMPAS data set: 1 continuous variable and 1 categorical variable.

# Reading in the Data

First, let's read in the data. 

```{r}
compas_path <- "/home/dasienga24/Statistics-Senior-Honors-Thesis/Data Sets/COMPAS/compas_seldonian_bw.csv"
compas_sim <- read.csv(compas_path)
```

# Data Subsetting

Next, let's plot the distributions of the continuous variables to choose which one we'll proceed with.

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim %>%
  ggplot(mapping = aes(x = age)) +
  geom_histogram() +
  theme_minimal() +
  labs(title = "Age")
```

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim %>%
  ggplot(mapping = aes(x = priors_count)) +
  geom_histogram() +
  theme_minimal() +
  labs(title = "Priors Count",
       x = "priors count")
```

$\\$

Because age has more variation, we'll use it as our continuous variable. We'll convert `priors_count` into a categorical variable.

```{r}
compas_sim <- compas_sim %>%
  mutate(prior_offense = ifelse(priors_count > 0, 1, 0)) %>%
  dplyr::select(c(race, prior_offense, age, is_recid))
```

```{r}
count(compas_sim$prior_offense == 1)/7924
```

$\\$

`age` seems to be a useful predictor for recidivism.

```{r}
favstats(age ~ is_recid, data = compas_sim)
```

$\\$

Whether a defendant has committed a prior offense or not appears to be a useful predictor for recidivism as well. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim %>%
  ggplot(mapping = aes(x = as.factor(prior_offense), fill = as.factor(is_recid))) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Committed a Prior Offense",
       fill = "Recidivism",
       x = "prior offense")
```

$\\$

We'll proceed with these 2 variables -- `age` and `prior_offense` -- for the simulation study. A glimpse of the data is shown below.

```{r}
compas_sim <- compas_sim %>%
  mutate(prior_offense = as.factor(prior_offense),
         is_recid = as.factor(is_recid))

glimpse(compas_sim)
head(compas_sim)
```

# Data Generation Mechanism #1 (class balance)

## Generating the Parent Simulation Data Set 

We want a setting with 50-50 class balance for each combination of race and recidivism status. To achieve that, we'll perform sample observations with replacement. Let's create a data set with 1250 observations in each of these 4 groups, hence, 5000 observations total.

$\\$

First, let's subset these 4 groups.

```{r}
compas_b_y <- compas_sim %>%
  filter(race == "African-American" & is_recid == 1)

compas_b_n <- compas_sim %>%
  filter(race == "African-American" & is_recid == 0)

compas_w_y <- compas_sim %>%
  filter(race == "Caucasian" & is_recid == 1)

compas_w_n <- compas_sim %>%
  filter(race == "Caucasian" & is_recid == 0)
```

$\\$

Next, let's randomly sample 1250 observations from each of these groups.

```{r}
set.seed(93)
compas_b_y_balanced <- compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE),]
compas_b_n_balanced <- compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE),]
compas_w_y_balanced <- compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE),]
compas_w_n_balanced <- compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE),]
```

$\\$

Finally, let's union all these together into a single data set.

```{r}
compas_sim_balanced <- rbind(compas_b_y_balanced, 
                             compas_b_n_balanced, 
                             compas_w_y_balanced, 
                             compas_w_n_balanced)
```

$\\$

Let's also shuffle the data set row orderings to aid the machine learning algorithms later. Some algorithms face convergence problems, especially in Python, when the classes are ordered.

```{r}
set.seed(123)
compas_sim_balanced <- compas_sim_balanced[sample(nrow(compas_sim_balanced), 
                                                  5000, 
                                                  replace = FALSE),]
```

```{r}
count(compas_sim_balanced$prior_offense == 1)/5000
```

The data set is ready.

## Examining Distributions of the Recidivism in the Parent Data Set

The bar plot below shows that we've achieved perfect class balance.

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced %>%
  ggplot(mapping = aes(x = is_recid)) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Recidivism",
       title = "Recidivism Prevalence")
```

$\\$

The bar plot below reveals that the balance is preserved by race as well. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced %>%
  ggplot(mapping = aes(x = is_recid, fill = race)) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Recidivism",
       title = "Recidivism Prevalence by Race")
```

## Assessing Baseline Predictive Performance of the Parent Data Set 

We want to make sure that our data set also has good predictive performance. We'll fit a logistic regression and assess baseline accuracy. 

```{r}
glm1 <- glm(is_recid ~ age + prior_offense,
            data = compas_sim_balanced,
            family = binomial(logit))

msummary(glm1)
```


```{r}
glm1augment <- glm1 %>% 
  broom::augment(type.predict = "response")
glm1augment <- mutate(glm1augment, binprediction = round(.fitted, 0)) 
with(glm1augment, table(is_recid, binprediction))
```

```{r}
(1339 + 1699)/5000
```

This model has 60.76% accuracy -- this is 10% higher than a random model would achieve. Our previous data was only 4% higher in accuracy than what a blind model would achieve, so in some sense, this is better and provides more room for the Seldonian results to vary.

$\\$

Discrepancy across race is also preserved, signifying that there is room to demonstrate the Seldonian algorithm's ability to satisfy fairness constraints.

```{r, warning = FALSE, message = FALSE}
preds <- predict(glm1, newdata = compas_sim_balanced, type="response")

compas_sim_balanced <- compas_sim_balanced %>% 
  mutate(preds = preds,
         prediction = round(preds, 0),
         pred_risk = ifelse(prediction == 0, 'Low', 'High')) 

compas_sim_balanced %>%
  dplyr::select(race, pred_risk, is_recid) %>%
  rename("Risk" = pred_risk,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  rename("Black" = `African-American`,
         "White" = `Caucasian`) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(booktabs = TRUE)
```



# Data Generation Mechanism #2 (better predictive performance)

Using the balanced data set created above, we'll simulate $Y$ values that have a stronger correlation with the 2 predictors. This is in an aim to hopefully ge better predictive performance, in addition to the class balance.

$\\$

## Generating the Parent Simulation Data Set 

```{r}
set.seed(123)
# define a linear combination of predictors as desired
linear_combination = - 15 - 24 * compas_sim_balanced$age + ifelse(compas_sim_balanced$prior_offense == 1, 1000, -10)

# pass through an inverse-logit function
probs = exp(linear_combination) / (1 + exp(linear_combination))  

# generate 5000 Bernoulli RVs for y
is_recid_sim = rbinom(5000, 1, probs) 

# join to original data frame
compas_sim_balanced_2 <- cbind(compas_sim_balanced, is_recid_sim)
```

$\\$

There are `r count(compas_sim_balanced_2$is_recid_sim == 0)` defendants who did not recidivate in this data set. 


$\\$

There are `r count(compas_sim_balanced_2$is_recid_sim == 1)` defendants who recidivated in this data set. 


$\\$

The data set seems pretty balanced. Let's look at this distribution in more detail, though. 

## Examining Distributions of the Recidivism in the Parent Data Set

The bar plot below confirms the balanced nature of this new data set. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_2 %>%
  ggplot(mapping = aes(x = as.factor(is_recid_sim))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Recidivism",
       title = "Simulated Recidivism Prevalence")
```

$\\$

However, the balance is not perfectly achieved by race. Some of the proxy relationships present in the predictor variables have influenced the distribution of the $Y$ variable by race. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_2 %>%
  ggplot(mapping = aes(x = race, fill = as.factor(is_recid_sim))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Race",
       title = "Simulated Recidivism Prevalence by Race",
       fill = "Recidivism")
```

## Assessing Baseline Predictive Performance of the Parent Data Set 

We want to make sure that our data set also has good predictive performance. We'll fit a logistic regression and assess baseline accuracy. 

```{r, warning = FALSE, message = FALSE}
glm2 <- glm(is_recid_sim ~ age + prior_offense,
            data = compas_sim_balanced_2,
            family = binomial(logit))

msummary(glm2)
```

```{r}
glm2augment <- glm2 %>% 
  broom::augment(type.predict = "response")
glm2augment <- mutate(glm2augment, binprediction = round(.fitted, 0)) 
with(glm2augment, table(is_recid_sim, binprediction))
```

```{r}
(2458 + 2529)/ 5000
```

This data set has 99.74% accuracy, although the distribution of race is affected. There is barely any error, so this may not be too useful for our fairness definition. 

# Data Generation Mechanism #3 (class balance and better predictive performance)

Let's introduce some balance across racial lines.

## Generating the Parent Data Set

First, let's subset these 4 groups.

```{r}
compas_b_y <- compas_sim_balanced_2 %>%
  filter(race == "African-American" & is_recid_sim == 1)

compas_b_n <- compas_sim_balanced_2 %>%
  filter(race == "African-American" & is_recid_sim == 0)

compas_w_y <- compas_sim_balanced_2 %>%
  filter(race == "Caucasian" & is_recid_sim == 1)

compas_w_n <- compas_sim_balanced_2 %>%
  filter(race == "Caucasian" & is_recid_sim == 0)
```

$\\$

Next, let's randomly sample 1250 observations with replacement from each of the 4 groups in the new data set.

```{r}
set.seed(123)
compas_b_y_balanced <- compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE),]
compas_b_n_balanced <- compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE),]
compas_w_y_balanced <- compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE),]
compas_w_n_balanced <- compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE),]
```

$\\$

Finally, let's union all these together into a single data set.

```{r}
compas_sim_balanced_3 <- rbind(compas_b_y_balanced, 
                               compas_b_n_balanced, 
                               compas_w_y_balanced, 
                               compas_w_n_balanced)
```

$\\$

Let's also shuffle the data set row orderings to aid the machine learning algorithms later.

```{r}
set.seed(123)
compas_sim_balanced_3 <- compas_sim_balanced_3[sample(nrow(compas_sim_balanced_3), 
                                                      5000, 
                                                      replace = FALSE),]
```


$\\$

The data set is now ready. 

## Examining Distributions of the Recidivism in the Parent Data Set

The bar plot below confirms the balanced nature of this new data set. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_3 %>%
  ggplot(mapping = aes(x = as.factor(is_recid_sim))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Recidivism",
       title = "Simulated Recidivism Prevalence")
```

$\\$

The balance is also preserved by race. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_3 %>%
  ggplot(mapping = aes(x = race, fill = as.factor(is_recid_sim))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Race",
       title = "Simulated Recidivism Prevalence by Race",
       fill = "Recidivism")
```

## Assessing Baseline Predictive Performance of the Parent Data Set 

We want to make sure that our data set also has good, but not perfect, predictive performance. We'll fit a logistic regression and assess baseline accuracy. 

```{r, warning = FALSE, message = FALSE}
glm3 <- glm(is_recid_sim ~ age + prior_offense,
            data = compas_sim_balanced_3,
            family = binomial(logit))

msummary(glm3)
```

```{r}
glm3augment <- glm3 %>% 
  broom::augment(type.predict = "response")
glm3augment <- mutate(glm3augment, binprediction = round(.fitted, 0)) 
with(glm3augment, table(is_recid_sim, binprediction))
```

$\\$

```{r}
(2478 + 2500)/5000
```


The accuracy is a little bit too good. For the final mechanism, we will follow this framework, but weaken the strengths of the correlations. 

# Data Generation Mechanism #4 (adjusting coefficients of Method #2)

## Generating the Parent Data Set

```{r}
set.seed(123)
# define a linear combination of predictors as desired
linear_combination = - 0.34 - 2 * compas_sim$age + ifelse(compas_sim$prior_offense == 1, 70, 0)

# pass through an inverse-logit function
probs = exp(linear_combination) / (1 + exp(linear_combination))  

# generate 5000 Bernoulli RVs for y
is_recid_sim = rbinom(nrow(compas_sim), 1, probs) 

# join to original data frame
compas_sim_balanced_4 <- cbind(compas_sim, is_recid_sim) 
```

$\\$

There are `r count(compas_sim_balanced_4$is_recid_sim == 0)` defendants who did not recidivate in this data set. 


$\\$

There are `r count(compas_sim_balanced_4$is_recid_sim == 1)` defendants who recidivated in this data set. 


$\\$

Next, let's induce balance into this data set. First, let's subset these 4 groups.

```{r}
compas_b_y <- compas_sim_balanced_4 %>%
  filter(race == "African-American" & is_recid_sim == 1)

compas_b_n <- compas_sim_balanced_4 %>%
  filter(race == "African-American" & is_recid_sim == 0)

compas_w_y <- compas_sim_balanced_4 %>%
  filter(race == "Caucasian" & is_recid_sim == 1)

compas_w_n <- compas_sim_balanced_4 %>%
  filter(race == "Caucasian" & is_recid_sim == 0)
```

$\\$

Next, let's randomly sample 1250 observations with replacement from each of the 4 groups in the new data set.

```{r}
set.seed(123)
compas_b_y_balanced <- compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE),]
compas_b_n_balanced <- compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE),]
compas_w_y_balanced <- compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE),]
compas_w_n_balanced <- compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE),]
```

$\\$

Finally, let's union all these together into a single data set.

```{r}
compas_sim_balanced_4 <- rbind(compas_b_y_balanced, 
                               compas_b_n_balanced, 
                               compas_w_y_balanced, 
                               compas_w_n_balanced)
```

$\\$

Let's also shuffle the data set row orderings to aid the machine learning algorithms later.

```{r}
set.seed(123)
compas_sim_balanced_4 <- compas_sim_balanced_4[sample(nrow(compas_sim_balanced_4), 
                                                      5000, 
                                                      replace = FALSE),]
```


$\\$

The data set is now ready. 

## Examining Distributions of the Recidivism in the Parent Data Set

The bar plot below confirms the balanced nature of this new data set. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_4 %>%
  ggplot(mapping = aes(x = as.factor(is_recid_sim))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Recidivism",
       title = "Simulated Recidivism Prevalence")
```
$\\$

The balance is also preserved by race. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_4 %>%
  ggplot(mapping = aes(x = race, fill = as.factor(is_recid_sim))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Race",
       title = "Simulated Recidivism Prevalence by Race",
       fill = "Recidivism")
```
## Assessing Baseline Predictive Performance of the Parent Data Set 

We want to make sure that our data set also has good, but not perfect, predictive performance. We'll fit a logistic regression and assess baseline accuracy. 

```{r, warning = FALSE, message = FALSE}
glm4 <- glm(is_recid_sim ~ age + prior_offense,
            data = compas_sim_balanced_4,
            family = binomial(logit))

msummary(glm4)
```

```{r}
glm4augment <- glm4 %>% 
  broom::augment(type.predict = "response")
glm4augment <- mutate(glm4augment, binprediction = round(.fitted, 0)) 
with(glm4augment, table(is_recid_sim, binprediction))
```

$\\$

Hmm, maybe we need to introduce an error term. 

```{r}
mean(linear_combination)
```

```{r}
sd(linear_combination)
```

$\\$

Maybe it's okay to not have class balance? Think about this some more.

# Data Generation Mechanism #5 (class balance, but with some error)

## Generating the Parent Data Set

```{r}
set.seed(93)
# define a linear combination of predictors as desired
error <- rnorm(n = 5000, mean = 100, sd = 100)
linear_combination = - 0.34 - 16 * compas_sim_balanced$age + ifelse(compas_sim_balanced$prior_offense == 1, 600, 0) + error

# pass through an inverse-logit function
probs = exp(linear_combination) / (1 + exp(linear_combination))  

# generate 5000 Bernoulli RVs for y
is_recid_sim = rbinom(5000, 1, probs) 

# join to original data frame
compas_sim_balanced_5 <- cbind(compas_sim_balanced, is_recid_sim)
```

$\\$

There are `r count(compas_sim_balanced_5$is_recid_sim == 0)` defendants who did not recidivate in this data set. 


$\\$

There are `r count(compas_sim_balanced_5$is_recid_sim == 1)` defendants who recidivated in this data set. 

$\\$

Next, let's induce balance into this data set. First, let's subset these 4 groups.

```{r}
compas_b_y <- compas_sim_balanced_5 %>%
  filter(race == "African-American" & is_recid_sim == 1)

compas_b_n <- compas_sim_balanced_5 %>%
  filter(race == "African-American" & is_recid_sim == 0)

compas_w_y <- compas_sim_balanced_5 %>%
  filter(race == "Caucasian" & is_recid_sim == 1)

compas_w_n <- compas_sim_balanced_5 %>%
  filter(race == "Caucasian" & is_recid_sim == 0)
```

$\\$

Next, let's randomly sample 1250 observations with replacement from each of the 4 groups in the new data set.

```{r}
set.seed(93)
compas_b_y_balanced <- compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE),]
compas_b_n_balanced <- compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE),]
compas_w_y_balanced <- compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE),]
compas_w_n_balanced <- compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE),]
```

$\\$

Finally, let's union all these together into a single data set.

```{r}
compas_sim_balanced_5 <- rbind(compas_b_y_balanced, 
                               compas_b_n_balanced, 
                               compas_w_y_balanced, 
                               compas_w_n_balanced)
```

$\\$

Let's also shuffle the data set row orderings to aid the machine learning algorithms later.

```{r}
set.seed(93)
compas_sim_balanced_5 <- compas_sim_balanced_5[sample(nrow(compas_sim_balanced_5), 
                                                      5000, 
                                                      replace = FALSE),]
```


$\\$

The data set is now ready. 

## Assessing Baseline Predictive Performance of the Parent Data Set 

We want to make sure that our data set also has good, but not perfect, predictive performance. We'll fit a logistic regression and assess baseline accuracy. 

```{r, warning = FALSE, message = FALSE}
glm5 <- glm(is_recid_sim ~ age + prior_offense,
            data = compas_sim_balanced_5,
            family = binomial(logit))

msummary(glm5)
```

```{r}
glm5augment <- glm5 %>% 
  broom::augment(type.predict = "response")
glm5augment <- mutate(glm5augment, binprediction = round(.fitted, 0)) 
with(glm5augment, table(is_recid_sim, binprediction))
```

```{r}
(2324 + 2370)/ 5000
```

This model has 93.88% accuracy.

```{r}
preds <- predict(glm5, newdata = compas_sim_balanced_5, type="response")

compas_sim_balanced_5 <- compas_sim_balanced_5 %>% 
  mutate(preds = preds,
         prediction = round(preds, 0),
         pred_risk = ifelse(prediction == 0, 'Low', 'High')) 
```

$\\$

The discrimination statistic is ~0.06.

```{r, warning = FALSE, message = FALSE}
compas_sim_balanced_5 %>%
  dplyr::select(race, pred_risk, is_recid) %>%
  rename("Risk" = pred_risk,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  rename("Black" = `African-American`,
         "White" = `Caucasian`) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(booktabs = TRUE)
```




# Data Generation Mechanism #6 (class balance, but with more error)

## Generating the Parent Data Set

```{r}
set.seed(93)
# define a linear combination of predictors as desired
error <- rnorm(n = 5000, mean = 50, sd = 75)
error2 <- rnorm(n = 5000, mean = 50, sd = 75)
linear_combination = - 0.34 - 16 * compas_sim_balanced$age + ifelse(compas_sim_balanced$prior_offense == 1, 600, 0) + error + error2

# pass through an inverse-logit function
probs = exp(linear_combination) / (1 + exp(linear_combination))  

# generate 5000 Bernoulli RVs for y
is_recid_sim = rbinom(5000, 1, probs) 

# join to original data frame
compas_sim_balanced_6 <- cbind(compas_sim_balanced, is_recid_sim)
```

$\\$

There are `r count(compas_sim_balanced_6$is_recid_sim == 0)` defendants who did not recidivate in this data set. 


$\\$

There are `r count(compas_sim_balanced_6$is_recid_sim == 1)` defendants who recidivated in this data set. 


$\\$

Next, let's induce balance into this data set. First, let's subset these 4 groups.

```{r}
compas_b_y <- compas_sim_balanced_6 %>%
  filter(race == "African-American" & is_recid_sim == 1)

compas_b_n <- compas_sim_balanced_6 %>%
  filter(race == "African-American" & is_recid_sim == 0)

compas_w_y <- compas_sim_balanced_6 %>%
  filter(race == "Caucasian" & is_recid_sim == 1)

compas_w_n <- compas_sim_balanced_6 %>%
  filter(race == "Caucasian" & is_recid_sim == 0)
```

$\\$

Next, let's randomly sample 1250 observations with replacement from each of the 4 groups in the new data set.

```{r}
set.seed(93)
compas_b_y_balanced <- compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE),]
compas_b_n_balanced <- compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE),]
compas_w_y_balanced <- compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE),]
compas_w_n_balanced <- compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE),]
```

$\\$

Finally, let's union all these together into a single data set.

```{r}
compas_sim_balanced_6 <- rbind(compas_b_y_balanced, 
                               compas_b_n_balanced, 
                               compas_w_y_balanced, 
                               compas_w_n_balanced)
```

$\\$

Let's also shuffle the data set row orderings to aid the machine learning algorithms later.

```{r}
set.seed(93)
compas_sim_balanced_6 <- compas_sim_balanced_6[sample(nrow(compas_sim_balanced_6), 
                                                      5000, 
                                                      replace = FALSE),]
```


$\\$

The data set is now ready. 

## Assessing Baseline Predictive Performance of the Parent Data Set 

We want to make sure that our data set also has good, but not perfect, predictive performance. We'll fit a logistic regression and assess baseline accuracy. 

```{r, warning = FALSE, message = FALSE}
glm6 <- glm(is_recid_sim ~ age + prior_offense,
            data = compas_sim_balanced_6,
            family = binomial(logit))

msummary(glm6)
```

```{r}
glm6augment <- glm6 %>% 
  broom::augment(type.predict = "response")
glm6augment <- mutate(glm6augment, binprediction = round(.fitted, 0)) 
with(glm6augment, table(is_recid_sim, binprediction))
```

```{r}
(2309 + 2373)/5000
```

This data set has 93.64% accuracy.

```{r}
preds <- predict(glm6, newdata=compas_sim_balanced_6, type="response")

compas_sim_balanced_6 <- compas_sim_balanced_6 %>% 
  mutate(preds = preds,
         prediction = round(preds, 0),
         pred_risk = ifelse(prediction == 0, 'Low', 'High')) 
```

$\\$

Some discrepancy is preserved, and in the same direction as previous analyses, although not of the same magnitude. The discrimination statistic is ~0.07.

```{r, warning = FALSE, message = FALSE}
compas_sim_balanced_6 %>%
  dplyr::select(race, pred_risk, is_recid) %>%
  rename("Risk" = pred_risk,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  rename("Black" = `African-American`,
         "White" = `Caucasian`) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(booktabs = TRUE)
```

# Data Generation Mechanism #7 (playing around with the coefficients to induce more error)

## Generating the Parent Data Set & Assessing Performance

```{r, eval = FALSE}
set.seed(93)

# positive number from 0 to 25, incluing decimals 0.1 apart
coeff_age = 1
  
coeff_po = 0.5
coeff_po_not = 0

while(TRUE) {
  # define a linear combination of predictors as desired
  linear_combination = - 0.34 - coeff_age * compas_sim_balanced$age + ifelse(compas_sim_balanced$prior_offense == 1, coeff_po, coeff_po_not) 
  
  # pass through an inverse-logit function
  probs = exp(linear_combination) / (1 + exp(linear_combination))  
  
  # generate 5000 Bernoulli RVs for y
  is_recid_sim = rbinom(5000, 1, probs) 
  
  if (coeff_po > 1000) {
    coeff_age <- coeff_age + 1
    coeff_po <- 0
    print(paste0("Coeff Age: ", coeff_age))
    next
  }
  
  if (count(is_recid_sim == 1) < 500 | count(is_recid_sim == 0) < 500) {
    coeff_po <- coeff_po + 5
    print(coeff_po)
    next
  }
  
  print("Data set produced")
  
  # join to original data frame
  compas_sim_balanced_7 <- cbind(compas_sim_balanced, is_recid_sim)
  
  # induce balance
  compas_b_y <- compas_sim_balanced_7 %>%
    filter(race == "African-American" & is_recid_sim == 1)
  
  compas_b_n <- compas_sim_balanced_7 %>%
    filter(race == "African-American" & is_recid_sim == 0)
  
  compas_w_y <- compas_sim_balanced_7 %>%
    filter(race == "Caucasian" & is_recid_sim == 1)
  
  compas_w_n <- compas_sim_balanced_7 %>%
    filter(race == "Caucasian" & is_recid_sim == 0)
  
  compas_b_y_balanced <- compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE),]
  compas_b_n_balanced <- compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE),]
  compas_w_y_balanced <- compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE),]
  compas_w_n_balanced <- compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE),]
  
  compas_sim_balanced_7 <- rbind(compas_b_y_balanced, 
                                 compas_b_n_balanced, 
                                 compas_w_y_balanced, 
                                 compas_w_n_balanced)
  
  compas_sim_balanced_7 <- compas_sim_balanced_7[sample(nrow(compas_sim_balanced_7), 
                                                        5000, 
                                                        replace = FALSE),]
  
  # fit GLM
  glm_final <- glm(is_recid_sim ~ age + prior_offense,
              data = compas_sim_balanced_7,
              family = binomial(logit))
  
  print("GLM converged")
  
  glm_finalaugment <- glm_final %>% 
    broom::augment(type.predict = "response")
  glm_finalaugment <- mutate(glm_finalaugment, binprediction = round(.fitted, 0)) 
  conf_matrix <- with(glm_finalaugment, table(is_recid_sim, binprediction))
  conf_matrix[3] + conf_matrix[2]
  
  if (conf_matrix[3] + conf_matrix[2] >= 1000) {
    break
  } else {
    coeff_po <- coeff_po + 5
  }
  
  print(paste0("Coeff Age: ", coeff_age))
  print(coeff_po)
}

coeff_age
coeff_po
conf_matrix
# I want: conf_matrix[3] + conf_matrix[2] >= 1000
```


```{r, eval = FALSE}
set.seed(93)

# positive number from 0 to 25, incluing decimals 0.1 apart
coeff_age = 1
  
coeff_po = 0.5
coeff_po_not = 0

while(TRUE) {
  # define a linear combination of predictors as desired
  linear_combination = - 0.34 - coeff_age * compas_sim_balanced$age + ifelse(compas_sim_balanced$prior_offense == 1, coeff_po, coeff_po_not) 
  
  # pass through an inverse-logit function
  probs = exp(linear_combination) / (1 + exp(linear_combination))  
  
  # generate 5000 Bernoulli RVs for y
  is_recid_sim = rbinom(5000, 1, probs) 
  
  if (coeff_po > 1000) {
    coeff_age <- coeff_age + 1
    coeff_po <- 0
    print(paste0("Coeff Age: ", coeff_age))
    next
  }
  
  if (count(is_recid_sim == 1) < 500 | count(is_recid_sim == 0) < 500) {
    coeff_po <- coeff_po + 5
    print(coeff_po)
    next
  }
  
  print("Data set produced")
  
  # join to original data frame
  compas_sim_balanced_7 <- cbind(compas_sim_balanced, is_recid_sim)
  
  # induce balance
  compas_b_y <- compas_sim_balanced_7 %>%
    filter(race == "African-American" & is_recid_sim == 1)
  
  compas_b_n <- compas_sim_balanced_7 %>%
    filter(race == "African-American" & is_recid_sim == 0)
  
  compas_w_y <- compas_sim_balanced_7 %>%
    filter(race == "Caucasian" & is_recid_sim == 1)
  
  compas_w_n <- compas_sim_balanced_7 %>%
    filter(race == "Caucasian" & is_recid_sim == 0)
  
  compas_b_y_balanced <- compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE),]
  compas_b_n_balanced <- compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE),]
  compas_w_y_balanced <- compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE),]
  compas_w_n_balanced <- compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE),]
  
  compas_sim_balanced_7 <- rbind(compas_b_y_balanced, 
                                 compas_b_n_balanced, 
                                 compas_w_y_balanced, 
                                 compas_w_n_balanced)
  
  compas_sim_balanced_7 <- compas_sim_balanced_7[sample(nrow(compas_sim_balanced_7), 
                                                        5000, 
                                                        replace = FALSE),]
  
  # fit GLM
  glm_final <- glm(is_recid_sim ~ age + prior_offense,
              data = compas_sim_balanced_7,
              family = binomial(logit))
  
  print("GLM converged")
  
  glm_finalaugment <- glm_final %>% 
    broom::augment(type.predict = "response")
  glm_finalaugment <- mutate(glm_finalaugment, binprediction = round(.fitted, 0)) 
  conf_matrix <- with(glm_finalaugment, table(is_recid_sim, binprediction))
  conf_matrix[3] + conf_matrix[2]
  
  if (conf_matrix[3] + conf_matrix[2] >= 1000) {
    break
  } else {
    coeff_po <- coeff_po + 5
  }
  
  print(paste0("Coeff Age: ", coeff_age))
  print(coeff_po)
}

coeff_age
coeff_po
conf_matrix
# I want: conf_matrix[3] + conf_matrix[2] >= 1000
```


# Final Data Generation Mechanism 

## Generating the Parent Data Set & Assessing Baseline Performance

```{r}
set.seed(123)

# define a linear combination of predictors as desired
linear_combination = 5  - 0.2 * compas_sim$age + ifelse(compas_sim$prior_offense == 1, 0.5, 0)

# pass through an inverse-logit function
probs = exp(linear_combination) / (1 + exp(linear_combination))

# generate 5000 Bernoulli RVs for y
is_recid_sim = rbinom(nrow(compas_sim), 1, probs)

# join to original data frame
compas_sim_balanced_final <- cbind(compas_sim, is_recid_sim)

# induce balance
compas_b_y <- compas_sim_balanced_final %>%
  filter(race == "African-American" & is_recid_sim == 1)

compas_b_n <- compas_sim_balanced_final %>%
  filter(race == "African-American" & is_recid_sim == 0)

compas_w_y <- compas_sim_balanced_final %>%
  filter(race == "Caucasian" & is_recid_sim == 1)

compas_w_n <- compas_sim_balanced_final %>%
  filter(race == "Caucasian" & is_recid_sim == 0)

compas_b_y_balanced <-
  compas_b_y[sample(nrow(compas_b_y), 1250, replace = TRUE), ]
compas_b_n_balanced <-
  compas_b_n[sample(nrow(compas_b_n), 1250, replace = TRUE), ]
compas_w_y_balanced <-
  compas_w_y[sample(nrow(compas_w_y), 1250, replace = TRUE), ]
compas_w_n_balanced <-
  compas_w_n[sample(nrow(compas_w_n), 1250, replace = TRUE), ]

compas_sim_balanced_final <- rbind(
  compas_b_y_balanced,
  compas_b_n_balanced,
  compas_w_y_balanced,
  compas_w_n_balanced
)

compas_sim_balanced_final <-
  compas_sim_balanced_final[sample(nrow(compas_sim_balanced_final),
                               5000,
                               replace = FALSE), ]

# fit GLM
glm_final <- glm(is_recid_sim ~ age + prior_offense,
            data = compas_sim_balanced_final,
            family = binomial(logit))

glm_finalaugment <- glm_final %>%
  broom::augment(type.predict = "response")
glm_finalaugment <-
  mutate(glm_finalaugment, binprediction = round(.fitted, 0))
conf_matrix <-
  with(glm_finalaugment, table(is_recid_sim, binprediction))
conf_matrix[3] + conf_matrix[2]
```
This model has 78% accuracy --> so better predictive performance and class imbalance is achieved! There is more room for the Seldonian algorithm's accuracy to vary.

```{r}
(5000 - 1102)/5000
```


## Examining Distributions of the Recidivism in the Parent Data Set

The bar plot below shows that we've achieved perfect class balance.


```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_final %>%
  ggplot(mapping = aes(x = as.factor(is_recid_sim))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Recidivism",
       title = "Recidivism Prevalence")
```


$\\$

The bar plot below reveals that the balance is preserved by race as well. 

```{r, warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, fig.align = 'center'}
compas_sim_balanced_final %>%
  ggplot(mapping = aes(x = as.factor(is_recid_sim), fill = race)) +
  geom_bar() +
  theme_minimal() +
  labs(x = "Recidivism",
       title = "Recidivism Prevalence by Race")
```

## Assessing Discrimination

There is a discrimination statistic of 0.2, still preserved in the original directions.

```{r, warning = FALSE, message = FALSE}
preds <- predict(glm_final, newdata = compas_sim_balanced_final, type="response")

compas_sim_balanced_final <- compas_sim_balanced_final %>% 
  mutate(preds = preds,
         prediction = round(preds, 0),
         pred_risk = ifelse(prediction == 0, 'Low', 'High')) 

compas_sim_balanced_final %>%
  dplyr::select(race, pred_risk, is_recid) %>%
  rename("Risk" = pred_risk,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  rename("Black" = `African-American`,
         "White" = `Caucasian`) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(booktabs = TRUE)
```


## Save the Final Simulation Data Set

Let's write the parent data set into a CSV file. 

```{r}
compas_sim_balanced_final <- compas_sim_balanced_final |>
  dplyr::select(c(race, prior_offense, age, is_recid_sim)) |>
  rename("is_recid" = is_recid_sim)
write.csv(compas_sim_balanced_final, file = "/home/dasienga24/Statistics-Senior-Honors-Thesis/Data Sets/COMPAS/compas_sim.csv")
```

$\\$

The parent data set is now ready. 

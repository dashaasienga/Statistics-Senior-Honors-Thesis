In an increasingly data-driven world, algorithmic bias is widespread and poses a complex challenge within machine learning (ML) and artificial intelligence (AI) systems. This raises critical questions about fairness and equity. Is it feasible to produce fairer models? In this thesis, we delve into this pressing issue, examining various mathematical definitions of fairness, highlighting conflicts in their simultaneous enforcement, and emphasizing the importance of human judgment in selecting relevant metrics. Our investigation then explores the Seldonian framework proposed by P. S. Thomas et al. (2019) to address the algorithmic bias pervasive in traditional ML applications. It is based on the concept that we can create algorithms that avoid mathematically defined unfair outcomes with high confidence by placing probabilistic fairness constraints on the algorithms' objective functions. Applying this framework to the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), a real-world dataset with racially disparate outcomes, we demonstrate its potential to achieve less discriminatory outcomes but discover significant tradeoffs with model performance. We further evaluate Seldonian algorithms in practical classification settings through simulation studies, shedding light on the challenges of obtaining Seldonian solutions and optimizing fairness, especially at its extreme, alongside predictive accuracy, particularly for smaller sample sizes. Our findings underscore the necessity of ongoing efforts to mitigate algorithmic bias, acknowledging the complexity of achieving fairness while balancing model performance and the need for more socio-technical collaboration.
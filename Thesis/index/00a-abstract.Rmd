In an increasingly data-driven world, algorithmic bias is widespread and poses complex challenges within machine learning (ML) and artificial intelligence (AI) systems. These challenges raise critical questions about fairness and equity. Is it possible to produce fairer models? In this thesis, we delve into this pressing issue, examining various mathematical definitions of fairness, highlighting conflicts in their simultaneous enforcement, and emphasizing the importance of human judgment in selecting relevant metrics. Our investigation then explores the Seldonian framework to address the algorithmic bias pervasive in traditional ML applications. It is based on the concept that we can create algorithms that avoid mathematically defined unfair outcomes with high confidence by placing probabilistic fairness constraints on the algorithms' objective functions. Applying this framework to a real-world dataset with racially disparate outcomes, the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) data, we demonstrate its potential to achieve less discriminatory outcomes but discover substantial tradeoffs with model performance. We further evaluate Seldonian algorithms in practical classification settings through simulation studies, shedding light on the challenges of obtaining Seldonian solutions and optimizing fairness alongside predictive accuracy, particularly for strict fairness constraints or smaller sample sizes. Our findings underscore the necessity of ongoing efforts to mitigate algorithmic bias, acknowledging the complexity of achieving fairness while balancing model performance and the need for more socio-technical collaboration.
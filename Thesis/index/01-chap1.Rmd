# Introduction {#intro}
<!--
Each chapter file must start with a chapter header (e.g., # Chapter name). Notice we have also included a label after the chapter name ({# intro}) so we can refer back to this chapter later in the text if want to (e.g., by using \@ref(intro)). This also helps you debug potential issues as you knit.

You can knit chapters all at once using the index file, or individually. When you knit a single chapter, every other chapter will have a one-sentence placeholder to make knitting easier.

You can re-name each chapter file (e.g., change this filename from "01-chap1.Rmd" to "01-introduction.Rmd"), just make sure you also update the filename in the list of rmd files in _bookdown.yml.
-->

  The public and private sector are increasingly turning to data-driven methods to automate and to guide simple and complex decision-making. However, this trend raises an important question of bias. There is a lot of misinterpretation when it comes to the collection of data in many application areas, and there is a major concern for data-driven methods to further introduce and perpetuate discriminatory practices, or to otherwise be unfair because of the social and historical processes that operate to the disadvantage of certain groups. 

For example, within healthcare, using mortality or readmission rates to measure hospital performance penalizes hospitals serving poor or non-White populations as those inherently have higher mortality and readmission rates due to confounding societal factors. Outside healthcare, credit-scoring algorithms predict outcomes based on income, which disadvantages low-income groups further perpetuating economic immobility. Policing algorithms result in increased scrutiny of Black neighborhoods because of the bias against Black people that is already present in the U.S. policing system, and hiring algorithms, which predict employment decisions, are affected by historical race and gender biases. 

Yet, these algorithms are often regarded as ground truth and free of human limitations because they are based on mathematics, statistics, and computer science – otherwise regarded as objective disciplines. In theory, this should lead to greater fairness. However, left unregulated, these mathematical models privilege majority groups and discriminate against minority groups because they often learn from inherently biased data. If the data used to train models contains bias, then the resulting algorithms will learn the bias and reflect it into their predictions. In many cases, this can be detrimental.  

While there are widely-accepted, though sometimes disputed, societal notions of fairness, one key question emerges: are there any established statistical notions of fairness and bias? Is it possible to mathematically and statistically define algorithmic bias and unfairness, thereby paving a way for addressing the challenges they pose? And if so, are there ways to leverage statistical tools to resolve such bias and unfairness? This thesis paper aims to explore and answer precisely these questions. 

## Algorithmic Bias 

There are multiple different types and sources of bias in the realm of statistics. In particular, algorithmic bias arises when an algorithm’s decisions are skewed towards a particular group of people, either positively or negatively [@mehrabi2021survey]. The danger with biased algorithmic outcomes is that they generate a feedback loop. Take, for example, a hiring algorithm that discriminates against female applicants for a specific job. In the long run, this algorithm can perpetuate, and even amplify, existing gender biases by further widening the gender-based class imbalance. 

One such key example of  algorithmic bias often cited in literature is regarding the broad use of the COMPAS – or the Correctional Offender Management Profiling for Alternative Sanctions – tool to predict a defendant’s risk of recidivism (committing another crime) within two years. COMPAS is more likely to have higher false positive rates for African-American offenders than Caucasian offenders [@mehrabi2021survey]. Across the country, scores of similar assessments are given to judges, which injects bias into courts [@angwin2016machine].   

COMPAS is based on data from people arrested in Broward County, Florida in 2013 and 2014 [@angwin2016machine]. The response variable, recidivism, was encoded based on who was charged with new crimes over the next two years. Analyses on the predictive efficacy of the COMPAS algorithm found that the algorithm was 61% accurate for a full range of crimes, including misdemeanors, and only 20% of people forecasted to commit violent crimes actually went on to do so. While the overall accuracy rate for the full range of crimes is better than a coin flip, there exists room for enhancing the predictive performance, especially for a decision as critical as whether or not to grant a defendant bail or parole.

The COMPAS algorithm is a color-blind model -- race was not included directly as a predictor during its development. However, a statistical analysis showed that even when the effects of race, age, and gender are controlled for through their inclusion as variables in a logistic regression model, Black defendants were still 77% more likely to be predicted at higher risk of committing a future violent crime and 45% more likely to be predicted of committing a future crime of any kind as compared to white defendants [@larson2016compas]. The table in Figure \@ref(fig:compas1) highlights the performance discrepancy across race. 

```{r compas1, fig.cap="Prediction Fails Differently for Black v White Defendants (Angwin et al., 2016)", out.width = '100%', echo=FALSE, fig.scap="COMPAS Prediction Fails Differently for Black v White Defendants", fig.align='center'}

include_graphics(path = "figures/compas1.png")
```

Although the tool has 61% accuracy, Black defendants are almost twice as likely to be labeled as higher risk without re-offending than White defendants as observed in Figure \@ref(fig:compas1). It makes the opposite mistake among White defendants. The reason for this is that classification models are trained to minimize average error, which fits majority populations [@chouldechova2018frontiers]. 

In truth, however, various societal factors contribute to distinct environmental and social realities for Black and White individuals. These factors, including an offender's personal and familial background, as well as their residential environment, are incorporated into the COMPAS tool to forecast recidivism. Consequently, it appears justifiable to adjust the calibration of the relationship between an offender's social context and their propensity for recidivism differently for Black and White offenders, acknowledging these inherent disparities. A group-blind classifier algorithm that fails to do so could unfairly disadvantage one group over the other -- COMPAS is just one such algorithm. For example, in the education sector, different factors lead to different SAT performance between students from majority versus minority populations. It would, thus, also seem fair that the relationship between SAT and college admissions be calibrated differently for each demographic group.

Therefore, the question becomes, can we modify these algorithms to be group-blind but also fair? In order to do so, fairness constraints that reduce, or even correct for, algorithmic bias during the modeling process must be set. However, one must first define fairness mathematically, statistically, and quantifiably.  

## Statistical Definitions of Fairness {#fairnessdefinitions}

Statistical notions of fairness can be defined at a group level or an individual level. *Group notions* fix a few demographic groups and assess the parity of some statistical measures across all the groups [@chouldechova2018frontiers]. Note that group measures, on their own, do not guarantee fairness to individuals or structured subgroups within protected demographic groups, but rather, give guarantees to "average" numbers of protected groups. These notions are the focus of this thesis paper.

*Individual notions*, on the other hand, are assessed on specific pairs of individuals rather than averaged across groups [@chouldechova2018frontiers]. In other words, similar individuals should be treated similarly along some defined similarity or inverse distance metrics. Counter-factual fairness, for example, relies on the intuition that a decision is fair towards an individual if it's the same in both the real world and a counter-factual world where the individual belongs to a different demographic group [@mehrabi2021survey]. This can be impractical, relies on strong assumptions about the data, and approaches the realm of causality [@chouldechova2018frontiers]. Moreover, there is a gap in literature with regard to individual notions of fairness. 

Ultimately, group notions and individual notions are not in conflict per se. Instead, they are on the same spectrum of how much dependence is allowed between predictions and the sensitive attribute [@castelnovo2022clarification]. Subgroup fairness is an alternative notion that intends to obtain the best properties of both, for example, by picking a group fairness constraint and assessing whether it holds over a large collection of subgroups [@mehrabi2021survey]. Group and individual fairness notions can be defined in both classification settings and regression settings, although most of the literature focuses on fairness within classification.

### Group Fairness in Regression Settings 

Fair regression is the quantitative notion of fairness of real-valued targets [@agarwal2019fair]. Consider a general prediction setting where the training set consists of $X$, a feature vector with all the predictor variables, $A$, the levels of the protected attribute/ demographic group, and $Y$, the real-valued continuous response variable. $F$ is a set of possible prediction models, and the goal is to find $f \in F$ that is a good predictive model of $Y$ given $X$ and some fairness constraints. The accuracy of a prediction $f(X)$ = $\hat{Y}$ on $Y$ is measured by the mean squared error (MSE) as the loss function $l(Y, f(X))$. The goal is to minimize $l(Y, f(X))$, hence, maximizing accuracy.

*Statistical parity* refers to minimizing the expected loss function (MSE) such that the probability that each predicted  $f(X)$ = $\hat{Y}$ is above a certain threshold $z$ for each sensitive attribute is the same as the probability over the entire data set, given some margin $\epsilon_a$ that is dependent on the protected attribute [@agarwal2019fair]: 

\begin{equation}
\label{ch1eq1}
\text{min}_{f \in F} \text{ } E[l(Y, f(X))] \text{ such that } \forall a \in A, z \in [0,1]:
\end{equation}

$$ |P[f(X) \geq z | A = a] - P[f(X) \geq z]| \leq \epsilon_a.$$

This is akin to the classification setting where it may be desirable to have the probability of being in the positive class be above some certain threshold for each group as well as across the entire data set. A similar notion, known as *bounded loss*, requires that the MSE for each group be below some pre-specified level $c_a$ that is dependent on the protected attribute [@agarwal2019fair]:

\begin{equation}
\label{ch1eq2}
\text{min}_{f \in F} \text{ } E[l(Y, f(X))] \text{ such that } \forall a \in A: 
\end{equation}

$$ E[l(Y, f(X)) | A = a] \leq c_a.$$

### Group Fairness in Classification Settings 

Group notions of fairness in classification, at the core, refer to treating different groups equally. They aim to remedy or prevent disparate impact, which is a setting where there is unintended disproportionate adverse impact on a particular group [@chouldechova2017fair]. There are three broad notions of observational group fairness: independence, separation, and sufficiency [@castelnovo2022clarification]. 

#### Independence

\newcommand{\indep}{\perp \!\!\! \perp}

This fairness definition requires predictions, $\hat{Y}$, to be independent of any sensitive attribute, $A$, that is, $\hat{Y} \indep A$ [@castelnovo2022clarification]. Thus, it relies only on the distribution of features and decisions, that is, $A$, $X$, and $\hat{Y}$, and focuses on the equality of the predictions themselves by satisfying the following equation:

\begin{equation}
\label{ch1eq3}
P (\hat{Y} = 1 | A = a) = P (\hat{Y} = 1 | A = b), \text{    } \forall a, b \in A,
\end{equation}

where $a$, $b$ are the two demographic groups in question.

This definition is also known as *demographic parity*, *statistical parity*, or generally, group fairness. It requires equal positive prediction ratios (PPR), where PPR is the ratio of the probability of a positive prediction $\frac{P(\hat{Y} = 1 | A = a)}{P(\hat{Y} = 1 | A = b)} \text{ } \forall \text{ } a, b \in A$, across all demographic group pairings [@castelnovo2022clarification]. In other words, the likelihood of a positive prediction should be the same regardless of the demographic group.

In the COMPAS data set, independence would be satisfied if the probability of predicted recidivism is the same for both Black and White defendants in the data set. That is, the probability that a Black defendant is predicted to recommit a crime within the next two years should be the same as the probability that a White defendant is predicted to recommit a crime. 

The visual example in Figure \@ref(fig:dp) illustrates a toy scenario where independence is met [@durahly2023fairness]. 

```{r dp, fig.cap="An Example of Demographic Parity (Durahly, 2023)", out.width = '100%', echo=FALSE, fig.scap="An Example of Demographic Parity"}
include_graphics(path = "figures/dp.png")
```

The dashed line represents the decision boundary. In both group A and group B, four out of the eight participants were predicted to repay a loan. The other half of the participants were predicted to default. Notice, however, that the class imbalance in this toy credit lending example results in a higher error rate within group B than group A.

A difference in demographic parity $|P(\hat{Y} = 1 | A = a) - P(\hat{Y} = 1 | A = b)|$ close to 0 or a ratio $\frac{P(\hat{Y} = 1 | A = a)}{P(\hat{Y} = 1 | A = b)}$ close to 1 by some defined margin is considered a fair solution [@castelnovo2022clarification]. To achieve demographic parity, the different demographic groups must be treated differently, which may seem contrary to societal pre-conceived notions of fairness. Therefore, demographic parity should be used when the primary objective is to enforce some form of equality between groups regardless of all other information and when the objectivity of the target variable, $Y$, is under question, perhaps because of historical biases. This, however, can unknowingly amplify biases if used in the wrong setting. For example, when imposing demographic parity on a hiring algorithm, if qualifications are different across a protected attribute, then less-qualified candidates may be hired. If these candidates end up being low-performers, then this can perpetuate stereotypes about their demographic group.  

In the above example of using a hiring algorithm with gender as the protected attribute, it may then seem fairer to require independence on gender only for men and women with the same rating or qualification, that is, $\hat{Y} \indep A | R$. This is known as *conditional demographic parity* and requires that the following equation is satisfied [@castelnovo2022clarification]: 

\begin{equation}
\label{ch1eq4}
P (\hat{Y} = 1 | A = a, R = r) = P (\hat{Y} = 1 | A = b, R = r), \text{    } \forall a, b \in A, \forall r.
\end{equation}


This idea can be generalized more to condition on all attributes, that is, $\hat{Y} \indep A |  X$. As this is more generalized, however, it begins to satisfy individual fairness [@castelnovo2022clarification]. This type of individual fairness is also referred to as "fairness through unawareness" (FTU), which requires that any protected attributes, or their covariates, are not explicitly used in the decision-making process [@mehrabi2021survey]. This definition of fairness requires that the following equation be satisfied [@castelnovo2022clarification]: 

\begin{equation}
\label{ch1eq5}
P (\hat{Y} = 1 | A = a, X = x) = P (\hat{Y} = 1 | A = b, X = x), \text{ } \forall a, b \in A, \forall x \in X.
\end{equation}


#### Separation

Independence does not make use of the true target $Y$ and simply requires equality of predictions. However, as observed in Figure \@ref(fig:dp), this can lead to different error rates between different groups. In other words, the model is more accurate for one group than it is for another group. Separation precisely focuses on equality of the error rates and is widely known as the *equality of odds* [@castelnovo2022clarification]. This definition requires the same type I and type II error rates, precisely, the same false positive rate (FPR) and false negative rate (FNR) across all demographic groups. FPR and FNR are defined by:

\begin{equation}
\label{ch1eq6}
FPR = P(\hat{Y} = 1| Y = 0) = \frac{FP}{FP + TN}
\end{equation}

\begin{equation}
\label{ch1eq7}
FNR = P(\hat{Y} = 0| Y = 1) = \frac{FN}{TP + FN}
\end{equation}

where FP refers to false positive predictions, TP refers to true positive predictions, FN refers to false negative predictions, and TN refers to true negative predictions. These metrics can be understood through a confusion matrix as in Figure \@ref(fig:confusionmatrix) [@mohajon2021confmatrix].

```{r confusionmatrix, fig.cap="A Confusion Matrix (Mohajon, 2021)", out.width = '75%', echo=FALSE, fig.scap="A Confusion Matrix"}
include_graphics(path = "figures/confusionmatrix.png")
```

In the COMPAS data set, separation would be satisfied if both Black defendants and White defendants had equal error rates. However, as observed in Figure \@ref(fig:compas1), Black defendants had an FPR of 45% while White defendants had an FPR of 24% -- the FPR in this context refers to the percentage of times the algorithm predicted the defendants had recidivated when they hadn't. Similarly, Black defendants had an FNR of 28% while White defendants had an FNR of 48% -- the FNR in this context refers to the percentage of times the algorithm predicted the defendants had not recommitted a crime when they had.

Separation requires independence of the predictions $\hat{Y}$ and the sensitive attribute $A$ conditioned on the true value of the target variable $Y$, that is, $\hat{Y} \indep A|Y$ [@castelnovo2022clarification]. In other terms, the following equation must be satisfied:

\begin{equation}
\label{ch1eq8}
P(\hat{Y} = 1 | A = a, Y = y) = P(\hat{Y} = 1 | A = b, Y = y), \text{ } \forall a,b \in A, \text{ } y \in \{ 0, 1 \},
\end{equation}

where 0 is a negative outcome and 1 is a positive outcome. This is a reasonable fairness metric, as long as the objectivity of the target variable is trusted, as it ensures that the model optimizes performance for all groups, not just majority groups. 

The visual example in Figure \@ref(fig:eoo) illustrates a toy scenario where separation is met [@castelnovo2022clarification]. The dashed line represents the decision boundary. Filled in circles represent positive predictions and empty circles represent negative predictions. The error rates are consistent between both men and women. Notice, however, that the different demographic groups were treated differently to achieve separation as observed in the different decision boundaries (dashed lines).

```{r eoo, fig.cap="An Example of Equality of Odds (Castelnovo et al., 2022)", out.width = '60%', echo=FALSE, fig.scap="An Example of Equality of Odds"}
include_graphics(path = "figures/eoo.png")
```

There are two relaxed versions of this measure depending on which outcome is most important to predict [@castelnovo2022clarification]:

i) *Predictive Equality*: equality of false positive rates (FPR) across groups:

\begin{equation}
\label{ch1eq9}
P (\hat{Y} = 1 | A = a, Y = 0) = P (\hat{Y} = 1 | A = b, Y = 0), \text{ } \forall a,b \in A.
\end{equation}


ii) *Equality of Opportunity*: equality of false negative rates (FNR) across groups:

\begin{equation}
\label{ch1eq10}
P (\hat{Y} = 0 | A = a, Y = 1) = P (\hat{Y} = 0 | A = b, Y = 1), \text{ } \forall a,b \in A.
\end{equation}


#### Sufficiency 

Finally, sufficiency takes the perspective of people that receive the same model prediction and requires parity among them regardless of sensitive features [@castelnovo2022clarification]. This is also knows as *predictive parity* and requires that the precision be the same across sensitive groups, that is, $Y \indep A | \hat{Y}$. In other words, the following equation must be satisfied:

\begin{equation}
\label{ch1eq11}
P (Y = y | A = a, \hat{Y} = y) = P (Y = y | A = b, \hat{Y} = y), \text{ } \forall a, b \in A, \text{ for } y \in \{0,1\}.
\end{equation}

Simply put, the probability of a positive outcome given a positive prediction, and that of a negative outcome given a negative prediction, should be equal across all sensitive groups.

$\\$

Consistent with this line of reasoning, many fairness metrics can be defined. This begs the fundamental question: can multiple definitions be simultaneously enforced?

## Fairness Conflicts

Because of the way different fairness definitions are defined, it can be impossible to simultaneously enforce multiple definitions, and unexpected behavior may result from a particular definition of fairness. This section highlights some conflicts that arise both in the regression and classification setting. 

### Fairness Conflicts in Regression

The UFRGS Entrance Exam and GPA Data contains entrance exam scores of students applying to the Federal University of Rio Grande do Sul in Brazil, along with the students' GPAs during their first three semesters at the university [@DVN/O35FW8_2019]. Each student's score in nine different entrance exams is used to predict their GPA during their first 3 semesters of study at the university. Gender and race are the protected attributes.  

Taking gender as the protected attribute in a gender-blind model, independence, in this setting, would require that the average predictions be the same for each gender. That is, 

\begin{equation}
\label{ch1eq12}
E[\hat{Y} | G = Male] = E[\hat{Y} | G = Female].
\end{equation}

Independence is violated if, on average, a model predicts a higher or lower GPA based on gender.

Separation, on the other hand, would require that the average error of predictions be the same for each gender. In defined notion, 

\begin{equation}
\label{ch1eq13}
E[\hat{Y} - Y | G = Male] = E[\hat{Y} - Y| G = Female].
\end{equation}

Separation is violated if, on average, the model over-predicts for one gender but under-predicts for another gender or the model either over-predicts or under-predicts more for one gender. 

However, a study found that because male and female applicants had different GPAs in the original data set, these two fairness definitions cannot be simultaneously satisfied [@thomas2020housetestimony]. A similar result in the Section \@ref(class-conflict) will explain this in a mathematically tractable way when working in a classification setting.  

### Fairness Conflicts in Classification {#class-conflict}

Define prevalence $p$ as the probability of a positive outcome given the demographic group [@chouldechova2017fair]. It directly relates to the class distribution of the outcome. $p \in (0,1)$ and can be denoted by:

\begin{equation}
\label{ch1eq14}
p_a = P(Y=1|A=a).
\end{equation}

Further define the positive predictive value (PPV) of a prediction as the probability of a positive outcome given a positive prediction [@chouldechova2017fair]:

\begin{equation}
\label{ch1eq15}
PPV(\hat{Y}|A = a) \equiv P (Y = 1| \hat{Y} = 1, A = a).
\end{equation}

Similarly, the negative predictive value (NPV) of a prediction is the probability of a negative outcome given a negative prediction and can be denoted as:

\begin{equation}
\label{ch1eq16}
NPV(\hat{Y}|A = a) \equiv P (Y = 0| \hat{Y} = 0, A = a).
\end{equation}

Sufficiency would require equal PPV and equal NPV across the different demographic groups. Note that NPV and PPV can be computed from a confusion matrix (Figure \@ref(fig:confusionmatrix)) [@saeed2015evidence]:

\begin{equation}
\label{ch1eq17}
PPV = \frac{TP}{TP + FP} \text{   ;   } NPV = \frac{TN}{TN + FN}.
\end{equation}

Now, given values of the $PPV \in (0,1)$ and $p \in (0,1)$, it can be shown that [@chouldechova2017fair]:

\begin{equation}
\label{ch1eq18}
FPR = \frac{p}{1-p} \frac{1-PPV}{PPV}(1 - FNR).
\end{equation}

Appendix \@ref(appendix-a) provides the details for the derivation of this equation. However, its direct implication is that if the prevalence differs between two groups, then it is impossible to satisfy sufficiency (equal PPV across all groups) and separation (equal FPR and FNR across all groups) simultaneously. For example, in the COMPAS data set, if recidivism rates differ between Black and White offenders, then an algorithm that guarantees predictive parity/ equal precision for both Black and White offenders cannot simultaneously satisfy equality of odds. Indeed, the recidivism rate for Black defendants in the data is 51%, compared to 39% for White defendants, and hence, the disparate impact of the COMPAS tool as observed in Figure \@ref(fig:compas1) [@chouldechova2017fair].

Figure \@ref(fig:dp) illustrates a similar conflict between independence and separation. Satisfying independence resulted in an imbalance of error rates between group A and group B because of the difference in the prevalence of loan repayment between both groups.

$\\$ 

Unfortunately, the distribution of the outcome of interest often differs for different demographic groups, posing the all-important question: how can fairness be achieved in the face of this conflict?

### On Fairness Conflicts

As observed, disparate impact can result from the use of a prediction tool that is perceived to be free from predictive bias. Just because an algorithm satisfies a particular definition of fairness doesn't infer that the algorithm is *fair* in every sense of that word. Balancing overall error rates alone is not enough as it does not produce models that are free from bias or that guarantee fairness at finer levels of granularity. This highlights the need for human value and domain expertise in defining fairness within the context of a particular problem before the fairness constraints can be set. Chapter \@ref(chap-2) introduces a framework for setting these constraints. 



```{r load_packages, include = FALSE}
library(mosaic)
library(dplyr)
library(tidyverse)
library(ggplot2)
```

# The Seldonian Algorithm {#chap-2}

Chapter \@ref(intro) introduced the problem of algorithmic bias, discussed existing statistical definitions of fairness both in regression and classification settings, and finally, highlighted  fairness conflicts that can arise in certain settings. Of important note is that there are a plethora of fairness definitions that have been developed in statistical machine learning, many of which have been shown to be incompatible in ways similar to the illustration in Appendix \@ref(appendix-a). In any effort to enforce fairness on machine learning models, a critical first step is to define what fairness means in the specific context [@thomas2020housetestimony]. This responsibility falls on domain experts, social scientists, and regulators. Once there is consensus on that, machine learning researchers can work to develop appropriate algorithms that enforce the chosen definition of fairness. The Seldonian framework, introduced in this chapter, offers one such way to place probabilistic fairness constraints on traditional algorithms. However, because Seldonian algorithms place constraints on traditional machine learning (ML) algorithms, an initial in-depth understanding of the standard approach is key. Section \@ref(standardml) discusses the typical ML approach before diving into the Seldonian framework later in the Chapter. 

## The Standard Machine Learning Approach {#standardml}

When designing a machine learning algorithm, the first step is to mathematically define what the algorithm should do, in other words, the goal of the algorithm [@thomas2019supplementary]. At an abstract level, this goal is identical for all machine learning problems: find a solution $\theta^*$, within some feasible set $\Theta$, that maximizes some objective function $f: \Theta \rightarrow R$, where $R$ is the set of real numbers. Precisely, the goal of the algorithm is to search for an optimal solution  

$$\theta^* \in \underset{\theta \in \Theta}{\text{ arg max }} f(\theta).$$

For example, let $X$ and $Y$ be dependent real-valued random variables in a regression setting with the goal of estimating $Y$ given $X$. In this setting, $\Theta$ is the set of feasible functions that model the relationship between $X$ and $Y$. Feasible functions are of the form $\theta(X) = \beta_0 + \beta_1X = \hat{Y}$. Each function $\theta \in \Theta$ takes a real number as input and produces a real number as output; therefore, $\theta : R \rightarrow R$. Note that the true value of $f(\theta)$ is unknown and can only be estimated from the data [@thomas2019preventing]. A reasonable objective function would then be the negative mean squared error (MSE):

$$f(\theta):=-E[(\theta(X) - Y)^2].$$

In this case, maximizing MSE is equivalent to minimizing -MSE, defining the goal of the regression algorithm as finding the solution with the least average error. For a sample with $n$ observations, that is, $(x_i, y_i) \text{ for } i = 1,2,...,n$, the objective function can be estimated by:

$$\hat{f(\theta)}= -\frac{1}{n} \sum_{i=1}^{n}(\theta(x_i) - y_i)^2.$$

### Limitations of the Standard Approach {#standardlimitations}

Consider a linear regression example to predict the qualifications of job applicants based on information on their resumes. Let $G$ encode the gender of each applicant, with $G=0$ if the applicant is female and $G=1$ if the applicant is male. Let $X$ encode a summary measure of an applicants qualification based on information on their resume -- a simple example would be a measure of how many job-relevant key words appear on their resume. Let $Y$ encode their actual qualification for the job as determined by their observed performance. 

If this linear regression estimator is designed to be used to filter which resumes submitted to a company will be forwarded for human review, it is worthwhile to ensure that the algorithm does not produce racist or sexist behavior. Drawing from definitions in Chapter \@ref(fairnessdefinitions), it might be less important to ensure that the algorithm, on average, has the same predictions for applicants of both genders because the distribution of qualifications may be different for both genders. However, of more concern is whether the algorithm, on average, predicts too high for one gender and too low for the other gender. 

Suppose that the data has the following distribution: $Y$ ~ $N(1,1)$ if $G = 0$ and $Y$ ~ $N(-1,1)$ if $G = 1$, that is, $Y$ is a normal variable $N(\mu, \sigma)$ with different means $\mu$ for different genders but with the same standard deviation $\sigma$ for both genders. Further define $X$ ~ $N(Y,1)$, that is, an applicant's resume quality is equal to their true quality plus some random noise. Figure \@ref(fig:fig1) displays a scatterplot of 1000 such data points, 500 from each gender. The black solid line is the least squares fit on this data, using a gender-blind model. 


```{r fig1, fig.align='center', fig.cap="Least Squares Fit on Synthetic Data Drawn from Different Distributions", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, echo = FALSE}
# set seed for reproducibility 
set.seed(123)

# generate synthetic data for both male and female applicants
female <- rnorm(500, mean = 1, sd = 1)
male <- rnorm(500, mean = -1, sd = 1)
gender <- c(rep(0,500), rep(1,500))

# create the data set
y <- union(female, male)
x <- rnorm(1000, mean = y, sd = 1)
data <- cbind(y, x, gender)
data <- data %>%
  as.data.frame(data)

# fit least squares line
ggplot(data = data, mapping = aes(x = x, y = y)) +
  geom_point(aes(color = as.factor(gender)), alpha = 0.8) +
  geom_smooth(
    method = "lm",
    aes(color = as.factor(gender)),
    se = F,
    size = 2
  ) +
  geom_smooth(
    method = "lm",
    formula = y ~ x,
    color = 'black',
    se = F
  ) +
  labs(color = "Gender (G)",
       title = "Least Squares Fit on Synthetic Data") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) 
```

```{r, echo = FALSE}
# set seed for reproducibility
set.seed(123)

# obtain the least squares linear model
model <- lm(y ~ x, data = data)

# obtain the model predictions 
preds <- predict(model, newdata = data)

# add the error into the data set
data <- data %>%
  mutate(error = preds - data$y)

# compute average error per gender 
diff <- data %>%
  group_by(gender) %>%
  summarise(average = mean(error)) 

# obtain the discrimination statistic
d <- diff$average[1] - diff$average[2]
```

The least squares fit on Figure \@ref(fig:fig1) is impartial to an observation's gender with an objective to make the most accurate predictions. While it may be expected that impartiality would produce fair results, observe that the linear model tends to over-predict if $G = 1$ and under-predict if $G=0$, on the contrary, producing discriminatory behavior. In fact, by defining a discrimination statistic, $d(\theta)$, as defined below in line with the specified fairness definition, the discrimination statistic for the synthetic data set in Figure \@ref(fig:fig1) can be shown to be `r d`, suggesting that the model predictions are in favor of $G = 1$.
 

$$ d(\theta) =  E[\hat{Y}-Y|G = 0 ] - E[\hat{Y}-Y|G = 1 ]. $$


In crucial applications such as hiring, this is concerning and highlights how linear regression algorithms designed using the standard approach can result in predictions of applicant performance that systematically discriminate against a demographic group.


### Potential Remedies

As illustrated in Section \@ref(standardlimitations) above, machine learning algorithms that use the standard approach may produce undesirable behavior. In an attempt to remedy this problem, a number of approaches can be taken. One potential remedy is to identify the root cause of the undesirable behavior such as class imbalance in the training data, bias in the data set, the choice of linear estimator, the model's blindness to the demographic group, or insufficient data, to name a few [@thomas2019supplementary]. However, even though it might be possible to determine and correct the root cause of the undesirable behavior, doing so can be difficult, error-prone, and require extensive data analysis, rendering the central goal of machine learning algorithms, which are designed to automate and make decision-making processes simpler, obsolete. For instance, in the example set up in Section \@ref(standardlimitations) and displayed in Figure \@ref(fig:fig1), the root cause of the discriminatory behavior when using ordinary least squares linear regression was the fact that the objective function was designed to minimize MSE, which was at odds with minimizing the discrimination statistic.  

Provided that detailed knowledge of the problem is available, hard constraints may be placed on the objective function, for example, requiring that MSE is minimized only on the set of solutions with a discrimination value $d(\theta)$ less than some value $\epsilon$ [@thomas2019supplementary]. Rather than placing hard constraints on the set of solutions, soft constraints that penalize undesirable behavior may also be placed on $f$, the objective function [@boyd2004convex]. Although such penalty functions can be effective, they require a careful choice of the value of the parameter $\lambda$ that places relative importance on the objective function and the constraint. For the linear regression example, the new objective function with a soft constraint would now be:

$$f(\theta) = - MSE (\theta) - \lambda d(\theta).$$
Observe that as $\lambda$ increases, MSE increases and the discrimination statistic decreases. Cross-validation techniques can be employed to find optimal values for $\lambda$. Other remedies include maximizing multiple objective functions or allowing constraints on the probability that a solution with undesirable behavior will be returned, both of which may require detailed knowledge of the application problem and underlying distribution of the data [@thomas2019supplementary]. 

In principle, there might be definitions of $\Theta$ of $f$ that prevent the algorithm from converging on solutions that exhibit undesirable behavior such as through setting soft constraints or hard constraints [@thomas2019preventing]. However, in practice, this might require extensive domain expertise and data analysis in order to properly balance the relative importance of the objective function and the constraints, which can be at odds with each other. These techniques may also require knowledge of the probability distribution from which the data is sampled, which is not always available and limits applications to parametric statistics.

The Seldonian algorithm, thus, addresses this problem precisely by allowing probablistic constraints on undesirable behavior to be placed more easily without detailed knowledge of the specific problem or the distribution of the data, shifting the burden from the domain experts who use these tools to the experts in ML and statistics [@thomas2019preventing]. It's named after Isaac Asimovâ€™s fictional character, Hari Seldon ^[In the fictional book, Hari Seldon was a resident of a fictional planet where he develops psychohistory, an algorithmic science that allows him to predict the future in probabilistic terms.] [@asimov1994forward]. 

## The Seldonian Framework 

## Toy Example: A Seldonian Regression Algorithm

## Random Notes (Delete When Done):

Need another source for standard ML approach?

Fix image.

Assign equations labels in Chapter 1 & 2 like below? 

\begin{equation}
\label{discstat}
d(\theta) =  E[\hat{Y}-Y|G = 0 ] - E[\hat{Y}-Y|G = 1 ].
\end{equation}
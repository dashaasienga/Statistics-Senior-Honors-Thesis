# Seldonian Algorithms for Classification {#chap-3}

```{r, include = FALSE}
library(mosaic)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(knitr)
library(kableExtra)
library(ggplot2)
library(GGally)
library(MASS)
library(gridExtra)
library(reshape2)
library(kableExtra)
library(pROC)
```

Chapter \@ref(chap-2) introduced the Seldonian framework, which offers probabilistic guarantees for satisfying defined behavioral constraints. Although impractical, the toy linear regression example demonstrated how Seldonian algorithms may be employed in a setting with a continuous real-valued response variable.

However, machine learning algorithms deal with classification problems in many practical applications. Applications range from risk assessment tools like COMPAS, discussed in Chapter \@ref(intro), to credit scoring and employment prediction algorithms, to name a few. However, deploying such algorithms raises significant ethical concerns, as discussed in Chapters \@ref(intro) and \@ref(chap-2), primarily regarding fairness and the potential reinforcement of discriminatory practices. Given the historical and social biases inherent in the data used to train these algorithms, there is a pressing need to assess their fairness and mitigate any potential harm they may cause disadvantaged groups.

To offer a path forward in addressing this issue, this chapter aims to apply the Seldonian framework to the COMPAS data set and assess its predictive outcomes compared to those from the COMPAS tool and those from logistic regression, a standard ML procedure. Specifically, the objective is to assess whether Seldonian approaches can produce fairer outcomes, particularly COMPAS outcomes, drawing on some of the statistical definitions of fairness defined in Chapter \@ref(fairnessdefinitions). This chapter will present a framework that can be emulated in other classification problems where fairness is a concern. 


## The COMPAS Data Set

```{r, echo = FALSE}
# read in the data
compas_path <- "/home/dasienga24/Statistics-Senior-Honors-Thesis/Data Sets/COMPAS/compas_data.csv"
compasdata <- read.csv(compas_path)

# clean the data
compasdata <- compasdata %>%
  filter(decile_score > 0 & is_recid != -1 & days_b_screening_arrest >= -30 &
           days_b_screening_arrest <= 30) %>%
  mutate(days_b_screening_arrest = abs(days_b_screening_arrest))

# remove duplicates
clean_compasdata <- compasdata[-which(duplicated(compasdata$id)), ]
```

The COMPAS data set, obtained from the ProPublica Data Store [@compasdata], records information on defendants from Broward County, Florida, that were evaluated for the risk of recidivism by the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool in 2013 and 2014. A database hosts this data, and a SQL query, displayed in Appendix \@ref(appendix-c), retrieved 12160 observations. There are 29 variables, 6 of which were later identified to be useful for this predictive analysis: age, age category, sex, marital status, whether the defendant had a history of juvenile offenses or not, and the total number of prior offenses committed by the defendant. The response variable records whether or not a defendant had recommitted a crime within two years. Finally, the protected attribute, race, has six levels: African-American, Caucasian, Hispanic, Asian, Native American, and Other. After cleaning the data to address anomalies, improve the data quality, and remove duplicate observations, the data set size was reduced to 9387 observations. Section \@ref(descriptivestats) examines these variables in more detail and their relationships with each other.

## Descriptive Statistics {#descriptivestats}

This section presents a holistic exploratory analysis of the COMPAS data set. 

### Univariate Analysis {#univariateanalysis}

The six predictive variables are age, age category, sex, marital status, whether the defendant had a history of juvenile offenses, and the total number of prior offenses committed by the defendant. Only two variables are continuous: the number of prior offenses and the defendant's age. The remaining variables are categorical.

#### Continuous Predictors

The defendants' ages range from 18 to 96, with a median of 32 years and a mean of 34.75 years. There is a right skew, as illustrated in Figure \@ref(fig:ch3fig1), and the middle 50% of the defendants are between the ages of 25 and 42. Similarly, the variable recording the number of prior offenses has a significant right skew with a median of 1 offense and a mean of 3.02 offenses. The middle 50% of the defendants have committed between 0 and 4 offenses, and the defendant with the most prior offenses has committed 38 crimes. Figure \@ref(fig:ch3fig1) visualizes this distribution.

```{r ch3fig1, fig.align='center', fig.cap="Distribution of the COMPAS Continuous Variables", warning = FALSE, message = FALSE, fig.width = 6, fig.height = 3, echo = FALSE}
g1 <- ggplot(data = clean_compasdata, mapping = aes(x = age)) +
  geom_histogram(fill = "lightblue", color = "black") +
  theme_minimal() +
  labs(title = "Age")

g2 <- ggplot(data = clean_compasdata, mapping = aes(x = priors_count)) +
  geom_histogram(fill = "lightblue", color = "black") +
  theme_minimal() +
  labs(x = "priors",
       title = "Prior Offenses")

grid.arrange(g1, g2, nrow = 1)
```


#### Categorical Predictors

Sex has two levels: male and female. There are 7457 males and 1930 females in the data set. The age category variable classifies participants as either 'Less than 25', '25 - 45', or 'Greater than 45' -- there are 2009, 5366, and 2012 observations in each group, respectively. There are seven different levels of marital status. In descending order, they are single (7202), married (1138), divorced (398), significant other (332), separated (219), unknown (58), and widowed (40). Finally, 8254 defendants had no record of juvenile offenses, with only 1133 defendants having such a record. Figure \@ref(fig:ch3fig2) visually displays the distribution of these variables.

```{r ch3fig2, fig.align='center', fig.cap="Distribution of the COMPAS Categorical Variables", warning = FALSE, message = FALSE, fig.width = 6, fig.height = 6, echo = FALSE}
g3 <- ggplot(data = clean_compasdata, mapping = aes(x = sex)) +
  geom_bar(fill = "lightblue", color = "black") +
  theme_minimal() +
  labs(title = "Sex")

order <- c("Less than 25", "25 - 45", "Greater than 45")

g4 <- ggplot(data = clean_compasdata, mapping = aes(x = age_cat)) +
  geom_bar(fill = "lightblue", color = "black") +
  theme_minimal() +
  scale_x_discrete(limits = order) +
  labs(x = "Age Category",
       title = "Age Categories")

g5 <- ggplot(data = clean_compasdata, mapping = aes(x = marital_status)) +
  geom_bar(fill = "lightblue", color = "black") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 25, vjust = 1.2, hjust=1)) +
  labs(x = "Marital Status",
       title = "Marital Status") 

clean_compasdata <- clean_compasdata %>%
  mutate(juv_offense_count = juv_fel_count + juv_misd_count + juv_other_count,
         juv_offense = ifelse(juv_offense_count == 0, 0, 1))

g6 <- clean_compasdata %>%
  mutate(juv_offense = ifelse(juv_offense == 0, "No", "Yes")) %>%
  ggplot(mapping = aes(x = juv_offense)) +
  geom_bar(fill = "lightblue", color = "black") +
  theme_minimal() +
  labs(x = "Committed a Juvenile Offense",
       title = "Juvenile Offense Record")

grid.arrange(g3, g4, g6, g5, ncol = 2, nrow = 2)

```


#### Response Variable

The response variable records whether or not a defendant recommitted any crime within two years. Figure \@ref(fig:ch3fig3) illustrates that about two-thirds (6199) of the defendants did not recommit a crime within two years, while one-third (3188) did. The different proportion of observations in each level indicates class imbalance, which often affects the performance of machine learning classification algorithms. Given the class imbalance, analyzing performance relative to the classes will be important when assessing model performance in the proceeding sections. 

```{r ch3fig3, fig.align='center', fig.cap="Distribution of the COMPAS Response Variable: Recidivism", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 2.5, echo = FALSE}
clean_compasdata %>%
mutate(is_recid = ifelse(is_recid == 0, "No", "Yes")) %>%
ggplot(mapping = aes(x = is_recid)) +
  geom_bar(fill = "lightblue", color = "black") +
  theme_minimal() +
  labs(title = "Recidivism",
       x = "Recommitted a Crime Within 2 Years")
```

#### Demographic Variable

Finally, the demographic variable in this analysis is race. Most of the defendants are African-American (4674) and Caucasian (3250), with only 818 Hispanic, 48 Asian, and 27 Native American defendants. The remaining 570 defendants identify as 'Other'. Figure \@ref(fig:ch3fig4) visualizes this.

```{r ch3fig4, fig.align='center', fig.cap="Distribution of the COMPAS Demographic Variable: Race", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 2.5, echo = FALSE}
ggplot(data = clean_compasdata, mapping = aes(x = race)) +
  geom_bar(fill = "lightblue", color = "black") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 25, vjust = 1.2, hjust=1)) +
  labs(title = "Race")
```

### Bivariate Analysis {#bivariateanalysis}

With a thorough understanding of the variables from Section \@ref(univariateanalysis), this section examines the relationship between the predictive variables and the response variable, recidivism.

#### Continuous Predictors

There is some difference in the mean and median ages for defendants who recommit crimes within two years versus those who do not. Those who recidivate have a mean and median age of 32 and 29, respectively, compared to 36 and 33 for those who do not. Defendants who recidivate tend to be younger than those who do not, as illustrated by the boxplot in Figure \@ref(fig:ch3fig5). There is also some distributional difference in non-juvenile prior offenses for defendants who recidivate versus those who do not, as is indicated by different means and medians. Those who recommit a crime within two years tend to have more prior offenses, with a mean of 4.7 and median of 3, compared to a mean of 2.2 and a median of 1 offense for defendants who do not. Figure \@ref(fig:ch3fig5) visualizes this distributional difference.

```{r ch3fig5, fig.align='center', fig.cap="Distribution of the COMPAS Continuous Variables by Recidivism", warning = FALSE, message = FALSE, fig.width = 6, fig.height = 3, echo = FALSE}
g7 <- clean_compasdata %>%
  mutate(is_recid = ifelse(is_recid == 0, "No", "Yes")) %>%
  ggplot(mapping = aes(x = is_recid, y = age)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Recidivism by Age",
       x = "Recidivism",
       y = "Age")

g8 <- clean_compasdata %>%
  mutate(is_recid = ifelse(is_recid == 0, "No", "Yes")) %>%
  ggplot(mapping = aes(x = is_recid, y = priors_count)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Recidivism by Prior Offenses",
       x = "Recidivism",
       y = "Prior Offenses")

grid.arrange(g7, g8, nrow = 1)
```


#### Categorical Predictors

There is not much evident relationship between sex and recidivism. Among defendants who do not recidivate, more are 45 years old compared to those under 25. However, among those who recidivated, more defendants are less than 25 compared to those greater than 45. There is not much relationship between recidivism and marital status. Finally, while most defendants do not have juvenile offenses, the proportion of those who do not to those who do is much smaller for defendants who re-offend in comparison to those who do not. Figure \@ref(fig:ch3fig6) displays the significant relationships.

```{r ch3fig6, fig.align='center', fig.cap="Distribution of the COMPAS Categorical Variables by Recidivism", warning = FALSE, message = FALSE, fig.width = 6, fig.height = 4.5, echo = FALSE}
g9 <- ggplot(data = clean_compasdata, mapping = aes(x = sex, fill = sex)) +
  geom_bar() +
  theme_minimal() +
  facet_wrap(~is_recid) +
  labs(title = "Recidivism by Sex") 

order <- c("Less than 25", "25 - 45", "Greater than 45")

g10 <- ggplot(data = clean_compasdata, 
       mapping = aes(x = age_cat, fill = age_cat)) +
  geom_bar() +
  theme_minimal() +
  facet_wrap(~is_recid) +
  labs(title = "Recidivism by Age Categories",
       x = "Age Category",
       fill = "Age Category") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1.2, hjust=1)) +
  scale_x_discrete(limits = order) 

g11 <- ggplot(data = clean_compasdata, 
       mapping = aes(x = marital_status, fill = marital_status)) +
  geom_bar() +
  theme_minimal() +
  facet_wrap(~is_recid) +
  labs(title = "Recidivism by Marital Status",
       x = "Marital Status",
       fill = "Marital Status") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1.2, hjust=1))

g12 <- clean_compasdata %>%
  mutate(juv_offense = ifelse(juv_offense == 0, "No", "Yes")) %>%
  ggplot(mapping = aes(x = juv_offense, fill = juv_offense)) +
  geom_bar() +
  theme_minimal() +
  facet_wrap(~is_recid) +
  labs(title = "Recidivism by Juvenile Offense",
       x = "Juvenile Offense",
       fill = "Juvenile Offense") 

grid.arrange(g10, g12, nrow = 2)
```

### Multivariate Analysis

```{r, echo = FALSE}
compas_final <- clean_compasdata %>%
  dplyr::select(c(race, sex, age, age_cat, marital_status, 
                  juv_offense, priors_count,
                  decile_score, is_recid))
```


As detailed in Section \@ref(bivariateanalysis), some predictive variables have a covariate relationship with the response variable, recidivism. A scatterplot matrix examining the relationship between the continuous variables, age and prior offenses, revealed weak correlations $(\rho = 0.12)$ and nonlinear relationships. There are no significant concerns for multicollinearity. Figure \@ref(fig:ch3fig7) illustrates this using Spearman's correlation. In addition, the correlation matrix incorporates the COMPAS tool decile scores to examine the predictive relationship between the continuous variables and the COMPAS tool results. The COMPAS tool maps raw scores into decile scores ranging from 1 to 10. The decile scores are directly associated with the risk of recidivism: scores between 1 and 4 are labeled as 'low' risk, between 5 and 7 as 'medium' risk, and between 8 and 10 as 'high risk'. Spearman's correlation revealed a moderate relationship of $\rho = -0.44$ and $\rho = 0.44$ for age and prior offenses, respectively.

```{r ch3fig7, fig.align='center', fig.cap="Spearman's Correlation Matrix for COMPAS Predictive Variables and the COMPAS Tool Decile Scores", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, echo = FALSE}
mycordata3 <- compas_final %>%
  dplyr::rename("Age" = age,
         "Priors" = priors_count,
         "Decile Scores" = decile_score) %>%
  dplyr::select("Age", "Priors", "Decile Scores")

mycors <- round(cor(mycordata3, method = "spearman"),2)
mycorplot <- melt(mycors)

ggplot(data = mycorplot, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  labs(x = "",
       y = "",
       title = "Spearman's Correlation Matrix") +
  scale_fill_gradient2(
    low = "blue",
    high = "red",
    mid = "white",
    midpoint = 0,
    limit = c(-1, 1),
    space = "Lab",
    name = "Spearman\nCorrelation"
  ) +
  geom_text(aes(Var2, Var1, label = value),
            color = "black",
            size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1))

```

With a thorough understanding of the variables, Section \@ref(fairnessanalysis) analyzes the data set and the COMPAS tool decile scores with a focus on fairness and an aim to examine the fairness concerns discussed in Chapters \@ref(intro) and \@ref(chap-2).

## Fairness and Demographic Analysis {#fairnessanalysis}

Chapter \@ref(algbias) introduced algorithmic bias as a situation when an algorithm’s decisions are skewed towards a particular group of people, either positively or negatively. With race as the protected attribute, this section aims to examine the COMPAS tool results, the underlying proxy relationships between the variables and the sensitive attribute, race, and create a table similar to the one in Figure \@ref(fig:compas1), which highlights the discrepancy in false positive and false negative rates of the COMPAS tool for Black and White defendants. 

### COMPAS Tool Analysis {#comptoolanalysis}

Recall that COMPAS decile scores of 1 to 4 are mapped as 'low' risk, 5 to 7 as 'medium' risk, and 8 to 10 as 'high' risk. The median decile score for the entire data set is four, and Figure \@ref(fig:ch3fig8) illustrates that the COMPAS tool classifies more than half of the defendants as low risk. In particular, the tool classifies 5370 as low risk and 1677 as high risk, with the remaining 2340 as medium risk. These predictions align with expectations since most defendants did not recommit a crime within the two-year time window, as observed in Figure \@ref(fig:ch3fig3). 

```{r ch3fig8, fig.align='center', fig.cap="COMPAS Tool Decile Scores", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 4, echo = FALSE}

order <- c("Low", "Medium", "High")

g14 <- ggplot(data = clean_compasdata, mapping = aes(x = score_text)) +
  geom_bar() +
  theme_minimal() +
  scale_x_discrete(limits = order) +
  labs(x = "COMPAS Risk Prediction",
       title = "COMPAS Risk Predictions in the Data Set")

g15 <- ggplot(data = clean_compasdata, mapping = aes(x = as.factor(decile_score))) +
  geom_bar() +
  theme_minimal() +
  labs(x = "COMPAS Decile Score",
       title = "Decile Scores in the COMPAS Data Set")

grid.arrange(g14, g15)
```

However, when these scores are broken down by whether the defendant reoffended within two years, it is revealed that the COMPAS tool performs poorly in classifying participants who recidivate. Such participants are classified almost equally into the three risk categories: low, medium, and high. Furthermore, while most defendants who do not reoffend within two years are classified as low risk, a significant number of them are classified as 'medium' and 'high' risk. The decile scores of defendants who recidivate and those who do not also range from 1 to 10, although the median decile score is 6 for the former group and 3 for the latter group. Generally, these results raise concerns about the COMPAS tool's predictive performance. 

```{r ch3fig9, fig.align='center', fig.cap="COMPAS Tool Decile Scores by Recidivism", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 2.5, echo = FALSE}
order <- c("Low", "Medium", "High")
  
clean_compasdata %>%
  mutate(is_recid = ifelse(is_recid == 0, "No", "Yes")) %>%
  ggplot(mapping = aes(x = score_text, fill = score_text)) +
  geom_bar() +
  theme_minimal() +
  facet_wrap(~is_recid) +
  labs(title = "Recidivism by COMPAS Risk Score",
       x = "COMPAS Risk Score",
       fill = "COMPAS Risk Score") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1.2, hjust=1)) +
  scale_x_discrete(limits = order) 
```


A breakdown by race, the protected attribute, will allow for further analysis of the performance of the COMPAS tool. Before that, converting this into a binary prediction problem will make analysis much more tractable. Decile scores of 1 to 5 will be mapped as 'lower' risk, while those ranging from 6 to 10 will be mapped as 'higher' risk. 

```{r, echo = FALSE}
compas <- compas_final %>%
  mutate(risk = ifelse(decile_score %in% c(1, 2, 3, 4, 5),'Lower','Higher'))
```


Table \@ref(tab:ch3table1) displays the confusion matrix of the COMPAS tool's results on this data set, which has `r nrow(compas)` total observations, with this binary definition of risk. The matrix reveals that the model has an overall accuracy of 66.61% on this data set. However, 66.04% of defendants do not recommit a crime within two years, as observed in Figure \@ref(fig:ch3fig3). Therefore, a blind non-informative model that classifies every defendant into the negative class would attain an accuracy of 66%, suggesting that the COMPAS tool's 66.61% accuracy is only a marginal improvement. 

Notice, furthermore, that the model struggles more in predicting whether defendants will reoffend than in predicting whether defendants will not reoffend. This imbalance in error rates is expected because of the class imbalance observed when performing exploratory data analysis in Section \@ref(descriptivestats) -- most defendants do not reoffend, so the model maximizes performance for those defendants. In terms of proportions, 49.25% of defendants who reoffended are incorrectly labeled as lower risk (almost equivalent to a flip of a coin), compared to 25.23% of defendants who did not reoffend but are incorrectly labeled as higher risk. This also raises the question of what type of prediction is more important: the risk of recidivism or the risk of non-recidivism. Is wrongly attributing a defendant as higher risk or wrongly attributing a defendant as lower risk worse for society?


```{r ch3table1, warning = FALSE, message = FALSE, echo = FALSE}
compas %>%
  dplyr::select(risk, is_recid) %>%
  rename("Risk" = risk) %>%
  group_by(Risk) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  kable(digits = 2,
        caption = "COMPAS Tool Confusion Matrix",
        booktabs = TRUE)
```


$\\$

While the confusion matrix in Table \@ref(tab:ch3table1) relays information on what types of errors the COMPAS tool tends to make and raises questions about the implications of that, Table \@ref(tab:ch3table2) breaks that down further by race for a more granular assessment. While the overall FPR is 25.3%, it is much higher for Black defendants (37.71%) and much lower for White defendants (16.34%). Similarly, while the overall FNR is 49.25%, it is much higher for White defendants (62.26%) and much lower for Black defendants (39.01%). The tool is more than twice as likely to classify a White defendant who reoffended as lower risk compared to a Black defendant. It makes the opposite mistake, where it is 1.6 times more likely to classify a Black defendant who did not reoffend as higher risk compared to a White defendant. This discrepancy aligns with ProPublica's findings in Figure \@ref(fig:compas1) and is alarming given that race was not included in the model. 

Including the other races reveals that Asian defendants who did not reoffend were the least likely to be labeled as higher risk – Black defendants were the most likely. Conversely, excluding the "Other" group, White defendants who reoffended were the most likely to be labeled as lower risk – Native Americans were the least likely. However, it is essential to consider the few observations in both the Asian and Native American groups, as visualized in Figure \@ref(fig:ch3fig4).

The analysis in this section provides evidence for disparities with favorable COMPAS outcomes for White and Asian defendants and unfavorable outcomes for Black and Native American defendants. To visualize these results, Figure \@ref(fig:ch3fig11) displays the distribution of the decile scores for the two most populous races in the data set. While there is a similar right-skewed distribution of recidivism for all races, Black defendants' decile scores are distributed almost uniformly among the ten decile scores. In contrast, the White defendants' decile scores have a significant right skew, with most observations in lower decile scores. This further emphasizes the racial disparity in employing these risk scores in judicial decisions and the present algorithmic bias.


```{r ch3table2, warning = FALSE, message = FALSE, echo = FALSE}
compas %>%
  dplyr::select(race, risk, is_recid) %>%
  rename("Risk" = risk,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  rename("Black" = `African-American`,
         "White" = `Caucasian`) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2),
         Hispanic = round(100 * Hispanic / Total, 2),
         Asian = round(100 * Asian / Total, 2),
         `Native American` = round(100 * `Native American` / Total, 2),
         Other = round(100 * Other / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE),
            Hispanic = max(Hispanic, na.rm = TRUE),
            Asian = max(Asian, na.rm = TRUE),
            `Native American` = max(`Native American`, na.rm = TRUE),
            Other = max(Other, na.rm = TRUE)) %>%
  filter((Risk == "Higher" & Recidivism == "Did Not Reoffend") |
           (Risk == "Lower" & Recidivism == "Reoffended")
  ) %>%
  kable(digits = 2,
        caption = "COMPAS Tool Error Rates by Race",
        booktabs = TRUE)
```


```{r ch3fig11, fig.align='center', fig.cap="COMPAS Tool Decile Scores for Race", warning = FALSE, message = FALSE, fig.width = 6, fig.height = 3, echo = FALSE}
compas %>%
  filter(race %in% c("African-American", "Caucasian")) %>%
  mutate(race = ifelse(race == "African-American", "Black", "White")) %>%
ggplot(mapping = aes(x = as.factor(decile_score))) +
  geom_bar() +
  theme_minimal() +
  facet_wrap(~race, ncol = 2) + labs(
    title = "Decile Scores by Race",
    x = "Decile Scores")
```


### Proxy Relationships 

Section \@ref(comptoolanalysis) revealed that the predictions of the COMPAS tool exhibit significant racial discrepancies. Yet, the COMPAS tool is a race-blind model. Race was not included as a variable, so how could a model result in such racially distinct outcomes? To answer this question, this section analyzes the relationship between race and the predictive variables to examine how much information regarding a defendant's race is incorporated into the model via other variables.

There are more male than female defendants for all races, and most defendants are single. However, African-American defendants tend to be, on average, the youngest compared to all the other races. Asian defendants, followed by Caucasian defendants, tend to be the oldest. Nevertheless, there is considerable overlap among all the races, and Figure \@ref(fig:ch3fig10) visualizes the relationships. In looking at the age categories by race, more African-American defendants are less than 25 than those who are greater than 45. The converse is true for Caucasians, with more defendants that are greater than 45 in comparison to those less than 25. These distributional differences illustrate that there is some relationship between age and race in this data set, particularly for Black versus White defendants. 

Additionally, African-American defendants have the most prior offenses, on average, followed by their Native-American counterparts. When compared with Caucasian defendants, African Americans have almost twice as many prior offenses, suggesting a strong proxy relationship between race and prior offenses. Asian defendants have the least prior offenses. This is an important result that illustrates how a system that predisposes certain races to prison can perpetuate that discriminatory trend by using those same variables to predict the risk of committing another crime. The boxplot in Figure \@ref(fig:ch3fig10) helps to visualize this relationship more clearly.

This section illustrates that the demographic variable, race ($A$), has proxy relationships with some predictor variables ($X$). Even a group-blind classifier will not be entirely blind to race because of the correlations present and the information it gains about race from the proxy variables.

Most defendants do not recommit another crime, even when broken down by race. Because the class imbalance is in the same direction, a 'fair' model should not misclassify defendants in different directions based on race. However, the nature of the relationship between race and the predictor variables results in discriminatory and unfair outcomes, as elucidated in Section \@ref(comptoolanalysis). 

Before employing the Seldonian framework on this data set, Section \@ref(logreg) fits a logistic regression to assess how similarly the COMPAS tool performs to a standard ML procedure. 

```{r ch3fig10, fig.align='center', fig.cap="COMPAS Tool Proxy Variables for Race", warning = FALSE, message = FALSE, fig.width = 6, fig.height = 8, echo = FALSE}
g18 <- ggplot(data = compas_final, 
       mapping = aes(x = as.factor(race), y = age)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Age by Race",
       x = "Race",
       y = "Age")

order <- c("Less than 25", "25 - 45", "Greater than 45")
  
g19 <- compas_final %>%
  filter(race %in% c("Caucasian", "African-American")) %>%
  ggplot(mapping = aes(x = age_cat, fill = age_cat)) +
  geom_bar() +
  theme_minimal() +
  facet_wrap(~race) +
  labs(title = "Age Categories by Race",
       x = "Age Category",
       fill = "Age Category") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1.2, hjust=1)) +
  scale_x_discrete(limits = order) 

g20 <- ggplot(data = compas_final, 
       mapping = aes(x = as.factor(race), y = priors_count)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Prior Offenses by Race",
       x = "Race",
       y = "Prior Offenses")

grid.arrange(g18, g19, g20, nrow = 3)
```



## Logistic Regression {#logreg}

Logistic regression is a statistical generalized linear model (GLM) specifically designed to predict dichotomous/ binary outcomes. In this case, logistic regression was used to model the probability of a defendant re-committing a crime within two years in Broward County, Florida, using the COMPAS data set. With a cutoff of 0.5, the resulting probabilities were divided into two bins: lower risk ($p < 0.5$) and higher risk ($p >= 0.5$). 

In particular, logistic regression follows the standard ML procedure described in Chapter \@ref(standardml). The objective function is to maximize the likelihood, that is, the product of the probabilities of observing the given outcomes given the parameters of the logistic regression model. Maximizing the likelihood is equivalent to minimizing the log loss, or cross-entropy loss, which measures the difference between the actual binary prediction and the predicted probabilities. It is denoted in Equation \@ref(ch3eq1) in a setting with $m$ observations. Given that logistic regression is one of the most widely used classification algorithms, this analysis will provide insights into how state-of-the-art traditional algorithms that do not consider fairness guarantees may perform on this data set.

\begin{equation}
\label{ch3eq1}
- \frac{1}{m} \sum_{i=1}^{m} [y_i \text{ } log(p_i) \text{ } + \text{ } 1-y_i \text{ } log(1 - p_i)]
\end{equation}

### Fitting the Logistic Regression Model

Recall that if every observation were classified in the majority class, an accuracy of 66% would be expected. This is a benchmark for the race-blind logistic regression model implemented in this section. The model will be trained on 70% of the data and evaluated on the remaining 30%. Only the six predictors (sex, age, age category, marital status, juvenile offense, and prior offenses) will be fit to predict recidivism, the R code for which is displayed in Appendix \@ref(appendix-d).

The accuracy was 70.2% on the training set, which is ~3% higher than the COMPAS tool's predictions and suggests that ~30% of defendants are misclassified. Overall accuracy was consistent at 70.1% when evaluated on the testing set, indicating that the model did not over-fit on the training set and had good generalization performance. However, the ROC curve on Figure \@ref(fig:ch3fig12) indicates concerns about the model's sensitivity $(1 - FNR)$ and specificity $(1 - FPR)$. The diagonal line shows how the model would perform with random predictions. Curves that budge closer to the top left are preferred because those types of curves maximize the area under the curve (AUC), and the sensitivity and specificity of the model are closer to 100%. However, the AUC of this model is 68.8%. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# set seed
set.seed(123)

# drop missing observations
compas <- tidyr::drop_na(compas) 

# train and test split
n <- nrow(compas)
train_index <- sample(1:n, 0.70 * n) 
test_index <- setdiff(1:n, train_index)
train <- compas[train_index, ]
test <- compas[test_index, ]

# fit the logistic regression model on identified predictors
glm2 <- glm(is_recid ~ sex + age + age_cat + marital_status +
              juv_offense + priors_count,
            data = train,
            family = binomial(logit))

# obtain the binary predictions on train set
glm2augment <- glm2 %>% 
  broom::augment(type.predict = "response")
glm2augment <- mutate(glm2augment, binprediction = round(.fitted, 0)) 

# obtain the binary predictions on test set 
preds <- predict(glm2, newdata=test, type="response")
test2 <- test %>% 
  mutate(preds = preds,
         prediction = round(preds, 0)) 

# obtain the ROC curve
roccurve1 <- with(test, roc(is_recid ~ preds))
```


```{r ch3fig12, fig.align='center', fig.cap="COMPAS Logistic Regression ROC Curve", warning = FALSE, message = FALSE, fig.width = 3, fig.height = 3, echo = FALSE}
plot(roccurve1)
```

### Evaluating the Equality of Odds  

Despite the improved accuracy, examining the direction of these error rates and performing a demographic analysis by race to assess the model’s ‘fairness’ along racial lines reveals the same discrepancies observed in the COMPAS tool results. 

Table \@ref(tab:ch3table3) displays the overall false positive and false negative rates. As expected, the model performs better in classifying defendants who do not reoffend than those who do since that is the majority class. When evaluated on the test set, 91% of defendants who did not reoffend are correctly classified as low-risk. Thus, only 9% of participants who do not reoffend are incorrectly classified as high-risk. This is a lower FPR than COMPAS. However, only 29% of defendants who reoffended are correctly classified as high-risk. Thus, 71% of defendants who reoffended are incorrectly classified as low-risk. This is a higher FNR than COMPAS.

Table \@ref(tab:ch3table4) further breaks these results down, focusing on Black and White defendants. While the overall FPR for the model is 8.95%, notice that the FPR for Black defendants is 13.93%, compared to only 5.09% for White defendants. Similar to the COMPAS tool, Black defendants who do not reoffend are incorrectly misclassified as high risk at twice the rate that White defendants are. On the flip side, while the overall FNR for the model is 70.98%, notice that the FNR for Black defendants is 60.82%, compared to 86.17% for White defendants. Similar to the COMPAS tool, the logistic regression makes the opposite mistake in predicting recidivism for Black v White defendants, with White defendants being more likely to be incorrectly classified as low risk than their Black counterparts.

These results serve to highlight the real danger with using standard ML procedures, even when the sensitive variable itself is not included. These models have potential to perpetuate, and even introduce, harmful and discriminatory practices as observed in this Chapter. Although the details of Northepointe's COMPAS algorithm are kept secret, it's clear that traditional algorithms like logistic regression lead to comparable outcomes. In fact, a probability cutoff of $p = 0.34$, the probability of being in the positive class in this data set, on the logistic regression yielded the same accuracy ($66.7$%) as the COMPAS tool. Additionally, comparing the COMPAS decile scores to the logistic regression (LR) results reveals that the median decile score is 8 in the high-risk LR prediction and 3 in the low-risk LR prediction, further highlighting the similarity of these results. The next section uses the logistic regression model as a starting point, but places fairness constraints to enforce equality of odds for Black and White defendants.

```{r ch3table3, warning = FALSE, message = FALSE, echo = FALSE}
test2 <- test2 %>%
  mutate(pred_risk = ifelse(prediction == 0, 'Low', 'High'))

test2 %>%
  dplyr::select(pred_risk, is_recid) %>%
  rename("Risk" = pred_risk) %>%
  group_by(is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  mutate(Reoffended = round(100 * Reoffended / Total, 2),
         `Did Not Reoffend` = round(100 * `Did Not Reoffend` / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk) %>%
  summarize(Reoffended = max(Reoffended, na.rm = TRUE),
            `Did Not Reoffend` = max(`Did Not Reoffend`, na.rm = TRUE)) %>%
  kable(digits = 2,
        booktabs = TRUE,
        caption = "COMPAS Logistic Regression Error Rates")
```



```{r ch3table4, warning = FALSE, message = FALSE, echo = FALSE}
test2 %>%
  filter(race %in% c("African-American", "Caucasian")) %>%
  dplyr::select(race, pred_risk, is_recid) %>%
  rename("Risk" = pred_risk,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  rename("Black" = `African-American`,
         "White" = `Caucasian`) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(digits = 2,
        caption = "COMPAS Logistic Regression Error Rates by Race",
        booktabs = TRUE)
```




## Seldonian Classification {#seldapp}

This section aims to conclude the chapter by illustrating how Seldonian algorithms can mitigate undesirable outcomes made by classifiers in practicality. Consistent with the notation in Chapter \@ref(chap-2), $\Theta$ would be a set of classifiers and $f$ a classifier performance measure, such as the function described in Equation \@ref(ch3eq1). For this application, the Seldonian algorithm will use the logistic loss as its primary objective function. The goal is to use the Seldonian framework to create a model that makes predictions about recidivism that are fair with respect to race, which will be simplified to just two levels: Black and White. Further research can extend this work to include more races.

### Formulating the Seldonian Problem

Without fairness constraints, the standard ML problem is finding a solution $\theta$ that minimizes logistic loss. However, such a solution, as illustrated in Section \@ref(logreg) when fitting a logistic regression model, results in unequal error rates between Black and White defendants when using the COMPAS data set. Defining the discrimination statistic $d(\theta)$ to measure the difference in error rates as in Equation \@ref(ch3eq2), $d(\theta_{LR}) = 0.34$ (or $34.18$%). Similarly, for the COMPAS tool, $d(\theta_{COMPAS}) = 0.45$ (or $44.7$%) using this data set. This illustrates that the logistic regression performed slightly better than the COMPAS tool, both in overall accuracy and error rate disparities. However, there is still a significant discrepancy between the error rates for both races.  

\begin{equation}
\label{ch3eq2}
d(\theta) = abs[(FPR | \text{Black} - FPR | \text{White}) + (FNR | \text{White} - FNR | \text{Black})]
\end{equation}

\noindent To minimize $d(\theta)$, $g(\theta)$ will be defined, for some $\epsilon$, as: 

\begin{equation}
\label{ch3eq3}
g(\theta) = abs[(FPR | \text{Black} - FPR | \text{White}) + (FNR | \text{White} - FNR | \text{Black})] - \epsilon.
\end{equation}

\noindent Recall that a Seldonian algorithm ensures that:

\begin{equation}
\label{ch3eq4}
P(g(\theta) \leq 0) \geq 1 - \delta.
\end{equation}

\noindent The problem can now be fully formulated as a Seldonian machine learning problem. That is, using gradient descent, an optimization algorithm for finding a local minimum of a differentiable function, the Seldonian objective is to minimize logistic loss subject to the constraint: 

\begin{equation}
\label{ch3eq3}
P\{abs[(FPR | \text{Black} - FPR | \text{White}) + (FNR | \text{White} - FNR | \text{Black})]  - \epsilon \leq 0 \} \geq 1 - \delta \textit{; } \delta = 0.05.
\end{equation}

$\delta$ will be set to 0.05 to attain 95% confidence that separation (equalized odds) as defined in Chapter \@ref(classfairdef) is satisfied, that is, $d(\theta) \leq \epsilon$. The data will be partitioned into a training set that will be passed into the candidate selection mechanism to compute $\theta_c$. The remaining partition will be used in the safety test to ensure probabilistic satisfaction of the constraint. $\epsilon$ will be set to four values: $0.2, 0.1, 0.05, \text{and } 0.01$. The code to obtain a solution is available as part of the `seldonian-engine` Python library and is displayed in Appendix \@ref(appendix-e), along with the data pre-processing. 

### Evaluating Performance and Fairness

```{r, echo = FALSE}
# read in the data
seldonian_results <- read.csv("/home/dasienga24/Statistics-Senior-Honors-Thesis/Data Sets/COMPAS/compas_seldonian_results_bw.csv")
```


A solution that passed the safety test was obtained for all four values of $\epsilon$, although as $\epsilon$ got smaller, the logistic loss increased, and the overall accuracy decreased. The accuracy was 68.2%, 65.59%, 64.7%, and 64.4% respectively for $\epsilon = 0.2, 0.1, 0.05, 0.01$. Additionally, the models got progressively worse in correctly classifying observations into the positive class, with the final model classifying every observation into the negative class. Each model had higher overall FNR and lower overall FPR than both the COMPAS tool and the logistic regression model. Tables \@ref(tab:ch3table5), \@ref(tab:ch3table6), \@ref(tab:ch3table7), and \@ref(tab:ch3table8) break up the FPR and FNR for Black and White defendants in each of these four cases. When $\epsilon = 0.2, 0.1, 0.05, \text{and } 0.01$, note that $d(\theta) = 0.22, 0.06, 0.007, \text{and } 0$ respectively.

```{r, echo = FALSE}
seldonian_results <- seldonian_results %>%
  mutate(race = ifelse(Black == 1, 'Black', 'White'),
         pred_risk_0.2 = ifelse(risk_0.2 == 0, 'Low', 'High'),
         pred_risk_0.1 = ifelse(risk_0.1 == 0, 'Low', 'High'),
         pred_risk_0.05 = ifelse(risk_0.05 == 0, 'Low', 'High'),
         pred_risk_0.01 = ifelse(risk_0.01 == 0, 'Low', 'High')) 
```


```{r ch3table5, warning = FALSE, message = FALSE, echo = FALSE}
seldonian_results %>%
  dplyr::select(race, pred_risk_0.2, is_recid) %>%
  rename("Risk" = pred_risk_0.2,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(digits = 2,
        booktabs = TRUE,
        caption = "Seldonian Error Rates for $\\epsilon$ = 0.2")
```

```{r ch3table6, warning = FALSE, message = FALSE, echo = FALSE}
seldonian_results %>%
  dplyr::select(race, pred_risk_0.1, is_recid) %>%
  rename("Risk" = pred_risk_0.1,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(digits = 2,
        booktabs = TRUE,
        caption = "Seldonian Error Rates for $\\epsilon$ = 0.1")
```


```{r ch3table7, warning = FALSE, message = FALSE, echo = FALSE}
seldonian_results %>%
  dplyr::select(race, pred_risk_0.05, is_recid) %>%
  rename("Risk" = pred_risk_0.05,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(digits = 2,
        booktabs = TRUE,
        caption = "Seldonian Error Rates for $\\epsilon$ = 0.05")
```

```{r ch3table8, warning = FALSE, message = FALSE, echo = FALSE}
seldonian_results %>%
  dplyr::select(race, pred_risk_0.01, is_recid) %>%
  rename("Risk" = pred_risk_0.01,
         "Race" = race) %>%
  group_by(Race, is_recid) %>%
  mutate(Total = n()) %>%
  group_by(Risk, Race, Total) %>%
  summarise("Reoffended" = count(is_recid == 1),
            "Did Not Reoffend" = count(is_recid == 0)) %>%
  pivot_longer(cols = c("Reoffended", "Did Not Reoffend"),
               names_to = "Recidivism") %>%
  pivot_wider(
    id_cols = c("Risk", "Recidivism", "Total"),
    names_from = "Race",
    values_from = value
  ) %>%
  mutate(Black = round(100 * Black / Total, 2),
         White = round(100 * White / Total, 2)) %>%
  dplyr::select(-Total) %>%
  group_by(Risk, Recidivism) %>%
  summarize(Black = max(Black, na.rm = TRUE),
            White = max(White, na.rm = TRUE)) %>%
  filter((Risk == "High" & Recidivism == "Did Not Reoffend") |
           (Risk == "Low" & Recidivism == "Reoffended")
  ) %>%
  kable(digits = 2,
        booktabs = TRUE,
        caption = "Seldonian Error Rates for $\\epsilon$ = 0.01")
```


$\\$

In conclusion, the Seldonian algorithms successfully met the fairness constraints defined, with the FPR and FNR gaining more parity along racial lines for Black and White defendants as $\epsilon$ became more conservative. However, there was a tradeoff with model informativeness and performance in enforcing greater fairness. The FNR got larger as the FPR got smaller until, ultimately, the model was non-informative despite perfectly satisfying the constraints (100% FNR and 0% FPR for both races). Accuracy also dropped as the value of $\epsilon$ decreased, although the original data set could have been more informative. Notably, considering all races, the lowest achievable accuracy would only be 4% lower than the highest accuracy attained by the logistic regression model. 

Looking ahead, there are several avenues for future research. One direction involves extending this study to data sets with less class imbalance and higher overall predictive performance. By doing so, the tradeoffs inherent in incorporating fairness constraints into the traditional objective function in a practical setting can be better elucidated. Chapter \@ref(chap-4) contributes to this understanding by conducting a simulation study to explore the theoretical and practical implications of the Seldonian framework in the classification setting.


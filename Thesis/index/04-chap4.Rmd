# Seldonian Algorithms for Classification: A Simulation Study {#chap-4}

```{r, include = FALSE}
library(mosaic)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(knitr)
library(kableExtra)
library(ggplot2)
library(GGally)
library(reshape2)
library(latex2exp)
```

Chapter \@ref(chap-2) introduced the Seldonian framework, which offers probabilistic guarantees for satisfying defined behavioral constraints. However, the toy linear regression example demonstrated some of the limitations of Seldonian algorithms, particularly in scenarios with limited sample sizes where convergence may pose a challenge. In practice, sample sizes vary. Furthermore, Chapter \@ref(chap-3) applied the Seldonian framework to a classification setting using the COMPAS data set. The goal was to produce recidivism predictions that elicit fairer outcomes along racial lines, with a specific focus on Black and White defendants. However, the application illustrated that while the behavioral constraint was satisfied in all four cases, there was a tradeoff in the model's accuracy. In other words, a non-informative model can be perfectly Seldonian, but what value does a theoretically fair non-informative model add?

To address this concern and further study these tradeoffs, this chapter investigates the efficacy and applicability of Seldonian algorithms in practical classification settings with class balance and better predictive performance than COMPAS. By conducting a simulation study, the aim is to evaluate whether Seldonian approaches can effectively produce fairer outcomes and mitigate discriminatory tendencies, drawing on the fairness notion of separation (or equality of odds) defined in Chapter \@ref(fairnessdefinitions) and set up in Chapter \@ref(seldapp). Specifically, the objective is to assess the feasibility of leveraging Seldonian algorithms to enhance the fairness and equity of predictive models across various practical classification tasks.

## Simulation Design {#sim-design}

Before conducting the simulation study and analyzing the results, this section provides detailed explanations of the simulation set-up and design. 

### Aims

This simulation study presents a proof of concept for using the Seldonian framework in classification problems with robust predictive performance. Seldonian algorithms $\textit{can}$ fail, especially with insufficient data, as elucidated in Chapter \@ref(toy). On the flip side, as elucidated in Chapter \@ref(seldapp), Seldonian algorithms can succeed in their objective for fairer outcomes but fail to produce a useful model. Solutions returned are also probabilistic and may not always satisfy the constraint despite passing the safety test. With these limitations in mind, this simulation study aims to empirically assess the predictive performance of Seldonian algorithms, compared to that of the standard ML approach, in practical classification settings. 

### Data-Generation Mechanism {#data-gen} 

```{r, echo = FALSE}
compas_sim_path <-
  "/home/dasienga24/Statistics-Senior-Honors-Thesis/Data Sets/COMPAS/compas_sim.csv"
compas_sim_parent <- read.csv(compas_sim_path)

lr <- glm(is_recid ~ age + prior_offense,
          data = compas_sim_parent,
          family = binomial(logit))

lr_accuracy <-
  100 * count(round(lr[["fitted.values"]]) == lr[["y"]]) / nrow(compas_sim_parent)
```

Given that this is a proof-of-concept simulation study, the data-generation mechanism will follow a realistic design. Two of the most informative variables from the COMPAS data set were selected for a simpler set-up: the defendant's age (continuous) and whether or not the defendant had committed any prior offenses (binary). The choice to use the COMPAS data set as a starting point was so that the complex relationships between the predictor variables ($X$) and the protected attribute ($A$) may be retained, especially given that the data was collected for a practical application and elucidates social relationships that may be expected in the real world. There are only two levels of the protected attribute, race, that were retained for the study: Black and White. 

To achieve better predictive performance, a linear combination of age and prior offenses was chosen after searching through a range of plausible values: $logit(p_i) = 5 - 0.2 \text{ }Age_i + 0.5 \text{ } \textit{PriorOffense}_i \text{ | } i \in \{1,2, \ldots, 9387\}$, where $p_i$ is the probability of defendant $\textit{i}$ recommitting a crime within two years. The response variable -- whether or not a defendant recommitted a crime within two years -- was then drawn from a Bernoulli distribution with probability of reoffense equal to $p_i$ for each defendant $\textit{i}$. Finally, class balance was induced on the data set by randomly sampling, with replacement, 1250 observations from each of the four intersections of race and recidivism status: Black and White defendants who recidivated and did not recidivate. Note that this scheme (Appendix \@ref(appendix-f)) assumes equal prevalence of the response variable, $Y$, for both levels of the protected attribute. 

The parent simulation data set, thus, contains 5000 observations, achieves perfect class balance, and has a baseline logistic regression accuracy of `r round(lr_accuracy)`%. This offers a realistic improvement from the accuracy of 70.2% obtained from the COMPAS logistic regression. However, because of the class balance, the lowest achievable accuracy is now 50% (a random, coin-flip model) rather than 64.4%. This will allow more room for the Seldonian algorithm model accuracies to vary. 

One thousand total data sets of size $n = 500$, $n = 1000$, $n = 2500$, and $n = 5000$ will be sampled, with replacement, from this parent data set for the simulation study, with 250 data sets in each of the four partitions. 

### Target

The target of this simulation study is prediction, that is, to evaluate Seldonian algorithms for predictive performance in classification settings. 

### Methods

Because the Seldonian framework was designed to place probabilistic fairness constraints on traditional algorithms, solutions produced by Seldonian algorithms will be compared to those produced through the standard ML framework, specifically logistic regression, to assess the predictive performance and feasibility of the Seldonian framework.

Drawing from the fairness definitions described in Chapter \@ref(fairnessdefinitions) with a particular focus on the COMPAS example in Chapter \@ref(chap-3), it may be less important that a model satisfies independence. That is, that the model, on average, has the same likelihood of a positive prediction for all levels of the protected attribute. Instead, it may be more realistic to expect that the model be equally wrong or equally correct in its predictions of $Y$ for each protected attribute. For this reason, one fairness constraint will be set on the Seldonian algorithms to satisfy separation, otherwise known as equality of the error rates or equality of odds, as defined in Equation \@ref(ch4eq1) within some margin $\epsilon$ and with $1 - \delta$ % confidence. Similar to Chapter \@ref(seldapp), $\epsilon$ will be set to four levels: $0.2, 0.1, 0.05, \text{and } 0.01$. $\delta$ will be set to $0.05$ to guarantee 95% confidence. 

\begin{equation}
\label{ch4eq1}
g(\theta): abs[(FPR | \text{A = a} - FPR | \text{A = b})] + abs[(FNR | \text{A = a} - FNR | \text{A = b})] - \epsilon.
\end{equation}

Simulated data sets will be generated independently before being fed into a logistic regression algorithm and the four Seldonian algorithms. The relevant performance measures, as detailed in Section \@ref(performancemeasures), will be recorded for each trial. Two hundred and fifty trials will be run for each sample size as described in Section \@ref(data-gen). The code will be run in Python using the Rstudio software interface and the Amherst College High-Performance Computing Cluster The `seldonian` package is readily available for installation in Python and is equipped with functions that allow for the implementation of Seldonian algorithms. Other Python packages, such as `pandas`, `numpy`, and `sklearn`, will help conduct the rest of the simulation study. 


### Performance Measures {#performancemeasures}

Compared to the logistic regression models, the predictive performance of the Seldonian models will be assessed along three dimensions: the probability of a Seldonian solution, the accuracy of the solutions, and the satisfaction or violation of the behavioral constraints set across all trials within a specific setting (sample size).

It is essential to account for the fact that the Seldonian algorithms will not always return a solution. Recording the probability of a solution being returned in each sample setting will be crucial for evaluating the practical feasibility of this framework. Additionally, for fair statistical comparisons, the accuracy and discrimination statistics of the logistic regression models will be compared only with Seldonian solutions that converge. However, the performance of the logistic regression models in trials where no solutions were found using the Seldonian framework will be analyzed to elucidate learnings about the nature of those trials.

For each simulation trial, the overall accuracy of both the convergent Seldonian models and the logistic regression model will be recorded and eventually averaged over the number of data sets in the specific sample size. Both the mean and the standard error will be reported in tabular format and visualized graphically to compare the models' predictive performances and, potentially, evaluate the trade-offs that may occur by employing the Seldonian framework and enforcing behavioral constraints.

Finally, the satisfaction or violation of the behavioral constraint identified in Equation \@ref(ch4eq1) will be assessed in two ways. First, for each sample size as described above, a count of the times both frameworks satisfied the behavioral constraint, by some margin $\epsilon$, will be reported in tabular format. Additionally, the discrimination statistics as defined in Equation \@ref(ch4eq2) will be recorded for each simulation trial. The mean and standard error will be reported and visualized for each sample setting to compare the magnitude and direction of the models' unfairness with regard to the separation fairness definition.

\begin{equation}
\label{ch4eq2}
d(\theta) = abs[(FPR | \text{A = a} - FPR | \text{A = b})] + abs[(FNR | \text{A = a} - FNR | \text{A = b})]
\end{equation}

## Simulation Results {#sim-results}

As described in Section \@ref(sim-design), the simulation results will be analyzed along three key performance measures: convergence, discrimination, and accuracy.

```{r, echo = FALSE}
# read in logistic results

lr_500 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_500.csv"
  )
lr_1000 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_1000.csv"
  )
lr_2500 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_2500.csv"
  )
lr_5000 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_5000.csv"
  )

lr_500 <- lr_500 |>
  mutate(sample_size = 500) |>
  dplyr::select(-X)

lr_1000 <- lr_1000 |>
  mutate(sample_size = 1000) |>
  dplyr::select(-X)

lr_2500 <- lr_2500 |>
  mutate(sample_size = 2500) |>
  dplyr::select(-X)

lr_5000 <- lr_5000 |>
  mutate(sample_size = 5000) |>
  dplyr::select(-X)

logistic_results <- rbind(lr_500, lr_1000, lr_2500, lr_5000)


# read in Seldonian results

seldonian_results <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/Python/COMPAS Simulation/SeldonianSimulation/results/seldonian_sim_results.csv"
  )


# combine both data sets for full results

sim_results <- inner_join(logistic_results,
                          seldonian_results,
                          by = c("sample_size", "dataset_id"))

reps <- nrow(sim_results) / 4
```


### Probability of a Seldonian Solution

All logistic regression trials are expected to return a solution, thus a 100% convergence rate. However, while all the Seldonian trials will propose a candidate solution predicted to pass the safety test with 95% confidence, not all candidate solutions will pass the safety test itself. Table \@ref(tab:ch4tab1) illustrates this. For $\epsilon = 0.2, 0.1, 0.05$, the probability of a Seldonian solution was $>\text{~}90$% for all sample sizes. In fact, among these, for sample sizes $n = 1000, 2500, 5000$, the probability of a Seldonian solution was $> 96$%, suggesting a higher convergence rate for looser fairness constraints and data set sizes greater than $n = 500$. This is consistent with the results in Figure 2.6 from Chapter \@ref(exp). Regardless, a small data set size did not matter for the loosest constraint of $\epsilon = 0.2$, which still returned a Seldonian solution $99.6$% of the time.

However, when $\epsilon = 0.01$, which was the tightest constraint, the probability of a solution dropped significantly, and there were no consistent trends across sample sizes, suggesting that the models had trouble passing the safety test in this case, regardless of sample size, likely because the constraint was too strict and not feasible for this data set. Surprisingly, the lowest observed probability of a solution ($24.8$%) is recorded when $\epsilon = 0.01, n = 5000$. 

Overall, across all sample sizes, the probability of a solution generally reduced as the constraint tightened. 

```{r, ch4tab1, echo = FALSE}
sim_results |>
  group_by(sample_size) |>
  summarise(
    LR = 100 * count(lr_convergence == 1) / reps,
    `SA (0.2)` = 100 * count(passed_safety_02 == "True") / reps,
    `SA (0.1)` = 100 * count(passed_safety_01 == "True") / reps,
    `SA (0.05)` = 100 * count(passed_safety_005 == "True") / reps,
    `SA (0.01)` = 100 * count(passed_safety_001 == "True") / reps
  ) |>
  rename("Sample Size" = sample_size) |>
  kable(caption = "Probability of Obtaining a Seldonian Solution",
        booktabs = TRUE)
```

$\\$

However, obtaining a solution that passes the safety test is not the end of the story. Table \@ref(tab:ch4tab2) records the proportion of solutions that satisfied the behavioral constraint among those that passed the safety test (Table \@ref(tab:ch4tab1)). When $\epsilon = 0.01$, although a low probability of solution was previously observed, $> 92$% of returned solutions satisfied the behavioral constraint. Additionally, the probability was largest in the $\epsilon = 0.01, n = 5000$ case, which previously had the lowest probability of returning a solution across the entire simulation study. Except for $n = 5000$, the probability of a Seldonian solution satisfying the behavioral constraint generally increased as the constraint got tighter, suggesting some trade-off between the probability of returning a solution (which decreased as the constraint tightened) and the probability of that solution satisfying the behavioral constraints. However, there were no consistent trends as sample size increased across all four levels of $\epsilon$, although the probability actually decreased for $\epsilon = 0.1, 0.05$. This is inconsistent with expectations, but perhaps the increased variability introduced by more observations makes it more difficult to constrain the models' unfairness. Additionally, $> \delta = 5$% of the solutions did not satisfy the constraint, raising some alarms regarding the high-confidence guarantee notion of Seldonian algorithms. Perhaps that probability would have been better captured with more data sets in the simulation study. Additionally, most research studies are more liberal, setting $\delta$ to $0.1$.

```{r, ch4tab2, echo = FALSE}
# filter discrimination data out for only seldonian solutions that passed the safety test
sim_results_converged_disc <- sim_results |>
  mutate(
    sa_02_disc_stat = ifelse(passed_safety_02 == "True", sa_02_disc_stat, NA),
    sa_01_disc_stat = ifelse(passed_safety_01 == "True", sa_01_disc_stat, NA),
    sa_005_disc_stat = ifelse(passed_safety_005 == "True", sa_005_disc_stat, NA),
    sa_001_disc_stat = ifelse(passed_safety_001 == "True", sa_001_disc_stat, NA)
  )

sim_results_converged_disc |>
  group_by(sample_size) |>
  summarise(
    `SA (0.2)` = round(
      100 * count(sa_02_disc_stat <= 0.2) / count(passed_safety_02 == "True"),
      2
    ),
    `SA (0.1)` = round(
      100 * count(sa_01_disc_stat <= 0.1) / count(passed_safety_01 == "True"),
      2
    ),
    `SA (0.05)` = round(
      100 * count(sa_005_disc_stat <= 0.05) / count(passed_safety_005 == "True"),
      2
    ),
    `SA (0.01)` = round(
      100 * count(sa_001_disc_stat <= 0.01) / count(passed_safety_001 == "True"),
      2
    )
  ) |>
  rename("Sample Size" = sample_size) |>
  kable(caption = "Satisfaction of the Behavioral Constraint by Seldonian Solutions that Passed the Safety Test",
        booktabs = TRUE)
```

$\\$

Finally, Figure \@ref(fig:ch4fig1) visualizes these results to elucidate the convergence of Seldonian algorithms better. Looking at the proportion of solutions that both pass the safety test and satisfy the defined behavioral constraint (dark blue) raises concerns about the efficacy of Seldonian algorithms, especially for the $n = 2500, 5000$ cases. 

```{r, ch4fig1, echo = FALSE, fig.width = 6.3, fig.height = 6, warning = FALSE, message = FALSE, fig.cap = "Probability of Returning a Solution and Satisfying the Constraint by Sample Size"}
# get long failure data for plotting
sim_failure_long <- sim_results |>
  group_by(sample_size) |>
  summarise(`SA (0.2)` = 100*count(passed_safety_02 == "False")/reps,
            `SA (0.1)` = 100*count(passed_safety_01 == "False")/reps,
            `SA (0.05)` = 100*count(passed_safety_005 == "False")/reps,
            `SA (0.01)` = 100*count(passed_safety_001 == "False")/reps) |>
  pivot_longer(cols = -c(sample_size),
               names_to = "model",
               values_to = "prop_failed_safety") 


# get long convergence data for plotting
sim_converged_long <- sim_results |>
  group_by(sample_size) |>
  summarise(`SA (0.2)` = 100*count(passed_safety_02 == "True")/reps,
            `SA (0.1)` = 100*count(passed_safety_01 == "True")/reps,
            `SA (0.05)` = 100*count(passed_safety_005 == "True")/reps,
            `SA (0.01)` = 100*count(passed_safety_001 == "True")/reps) |>
  pivot_longer(cols = -c(sample_size),
               names_to = "model",
               values_to = "prop_passed_safety")

# get long discrimination data for plotting
sim_satisfied_cstr_long <- sim_results |>
  group_by(sample_size) |>
  summarise(`SA (0.2)` = 100*count(passed_safety_02 == "True" & sa_02_disc_stat <= 0.2)/reps,
            `SA (0.1)` = 100*count(passed_safety_01 == "True" & sa_01_disc_stat <= 0.1)/reps,
            `SA (0.05)` = 100*count(passed_safety_005 == "True" & sa_005_disc_stat <= 0.05)/reps,
            `SA (0.01)` = 100*count(passed_safety_001 == "True" & sa_001_disc_stat <= 0.01)/reps) |>
  pivot_longer(cols = -c(sample_size),
               names_to = "model",
               values_to = "prop_passed_safety_satisfied_cstr")

# join both data sets
conv_disc <- inner_join(sim_converged_long, sim_satisfied_cstr_long,
                        by = c("sample_size", "model")) |>
  mutate(prop_passed_safety = prop_passed_safety - prop_passed_safety_satisfied_cstr) |>
  rename("prop_passed_safety_failed_cstr" = prop_passed_safety) |>
  inner_join(sim_failure_long,
             by = c("sample_size", "model")) |>
  rename("failed safety" = prop_failed_safety,
         "passed safety, didn't satisfy constraint" = prop_passed_safety_failed_cstr,
         "passed safety, satisfied constraint" = prop_passed_safety_satisfied_cstr) |>
  pivot_longer(cols = -c(sample_size, model),
               names_to = "statistic", 
               values_to = "value") |>
  mutate(sample_size = factor(case_when(sample_size == 500 ~ "n = 500",
                                 sample_size == 1000 ~ "n = 1000",
                                 sample_size == 2500 ~ "n = 2500",
                                 sample_size == 5000 ~ "n = 5000"
                                 ), 
                              levels = c("n = 500", "n = 1000", "n = 2500", "n = 5000"))) 

# plot
conv_disc |> 
  mutate(model = factor(model, 
                        levels = c("SA (0.2)", "SA (0.1)", "SA (0.05)", "SA (0.01)"))) |>
  group_by(model, statistic, sample_size) |>
  summarise(avg = mean(value, na.rm = TRUE)) |>
  ggplot(mapping = aes(x = model, y = avg, fill = statistic)) +
  geom_col() +
   theme_minimal() +
  theme(legend.position = 'bottom') +
  scale_fill_brewer(palette = "Blues") +
  facet_wrap(~sample_size) +
  labs(y = "Percent",
       x = "Algorithm",
       fill = " ")
```


### Discrimination 

Seldonian algorithms are a proposed method to set probabilistic fairness constraints on traditional ML algorithms, so it is of utmost importance that they produce fairer results. Table \@ref(tab:ch4tab3) displays the average discrimination statistic, defined in Equation \@ref(ch4eq2), for all combinations of $n$ and $\epsilon$. Observe that the mean of $d(\theta_{LR})$ was consistently at 0.24 for all sample sizes, which is a 10% improvement from the imbalanced case in Chapter \@ref(chap-3), suggesting that remedies for class balance may be one way to reduce algorithmic bias. 

The mean discrimination statistic was lower in all Seldonian algorithms. Better yet, the mean discrimination statistic was equal to or lower than the defined fairness constraint in all cases except the $\epsilon = 0.1, n = 5000$ case where the mean of $d(\theta) = 0.14 > 0.1$. Another important observation is that, in most cases, the mean discrimination statistic tended to stay constant or increase as the sample size increased, supporting the previous hypothesis that higher sample sizes may have a more difficult time constraining model fairness, particularly in the classification setting. Overall, the Seldonian algorithms met the defined fairness constraints on average. However, this was just an average, and the standard deviation values in Table \@ref(tab:ch4tab3), as well as the interquartile range (IQR) and outlying observations visualized in Figure \@ref(fig:ch4fig2), illustrate the previous findings that not all Seldonian solutions that passed the safety test satisfied the behavioral constraint.

```{r ch4tab3, echo = FALSE}
# table
sim_results_converged_disc |>
  group_by(sample_size) |>
  summarise(LR = round(mean(lr_discrimination, na.rm = TRUE),2),
            sd_lr = round(sd(lr_discrimination, na.rm = TRUE),2),
            `SA (0.2)` = round(mean(sa_02_disc_stat, na.rm = TRUE),2),
            sd_02 = round(sd(sa_02_disc_stat, na.rm = TRUE),2),
            `SA (0.1)` = round(mean(sa_01_disc_stat, na.rm = TRUE),2),
            sd_01 = round(sd(sa_01_disc_stat, na.rm = TRUE),2),
            `SA (0.05)` = round(mean(sa_005_disc_stat, na.rm = TRUE),2),
            sd_005 = round(sd(sa_005_disc_stat, na.rm = TRUE),2),
            `SA (0.01)` = round(mean(sa_001_disc_stat, na.rm = TRUE),2),
            sd_001 = round(sd(sa_001_disc_stat, na.rm = TRUE),2)) |>
  rename("Sample Size" = sample_size,
         "sd" = sd_lr,
         "sd " = sd_02,
         "sd  " = sd_01,
         "sd   " = sd_005,
         "sd     " = sd_001) |>
  kable(caption = "Mean Discrimination Statistic of Convergent Seldonian Solutions",
        booktabs = TRUE)
```



```{r ch4fig2, echo = FALSE, fig.width = 6.3, fig.height = 5, warning = FALSE, message = FALSE, fig.cap = "The Distribution of the Discrimination Statistic of Convergent Seldonian Solutions by Sample Size"}
# visualization
sim_results_converged_disc |>
  dplyr::select(c(lr_discrimination, sa_02_disc_stat, sa_01_disc_stat, sa_005_disc_stat, 
                  sa_001_disc_stat, sample_size)) |>
  mutate(sample_size = factor(case_when(sample_size == 500 ~ "n = 500",
                                 sample_size == 1000 ~ "n = 1000",
                                 sample_size == 2500 ~ "n = 2500",
                                 sample_size == 5000 ~ "n = 5000"
                                 ), 
                              levels = c("n = 500", "n = 1000", "n = 2500", "n = 5000"))) |>
  rename("Sample Size" = sample_size,
         "LR" = lr_discrimination,
         "SA (0.2)" = sa_02_disc_stat,
         "SA (0.1)" = sa_01_disc_stat,
         "SA (0.05)" = sa_005_disc_stat,
         "SA (0.01)" = sa_001_disc_stat) |>
  pivot_longer(cols = -c(`Sample Size`),
               names_to = "model",
               values_to = "discrimination") |>
  ggplot(mapping = aes(x = model, y = discrimination, color = model)) +
  geom_boxplot() +
  scale_x_discrete(limits = c("LR", "SA (0.2)", "SA (0.1)", "SA (0.05)", "SA (0.01)")) +
  theme_minimal() + 
  guides(color = "none") +
  facet_wrap(~`Sample Size`) +
  labs(x = "Algorithm",
       y = "Discrimination Statistic")
```



### Accuracy

The Seldonian algorithms primarily delivered on their promise for fairer outcomes, but a concern that became apparent in Chapter \@ref(chap-3) was regarding the predictive performance of these models, particularly overall accuracy. Notably, in the simulation study, there was also a decrease in overall accuracy as the fairness constraints got tighter across all sample sizes. This decrease was most significant for smaller sample sizes, as expected and illustrated in Figure \@ref(fig:ch4fig3). Conversely, larger sample sizes had a smaller drop in accuracy, suggesting that model predictive accuracy may have dominated the fairness constraint in the search for an optimal solution, particularly for the more liberal constraints ($\epsilon = 0.2, 0.1$). The following section will further analyze the accuracy-discrimination trade-off. Nevertheless, it is crucial to take note of the steady decrease in accuracy towards 50% (akin to a random, coin-flip model) as the constraint is tightened across all sample sizes

```{r ch4fig3, echo = FALSE, fig.width = 6.3, fig.height = 5, warning = FALSE, message = FALSE, fig.cap = "The Distribution of the Overall Accuracy of Convergent Seldonian Solutions by Sample Size"}
# filter accuracy data out for only seldonian solutions that converged
sim_results_converged_accuracy <- sim_results |>
  mutate(sa_02_accuracy = ifelse(passed_safety_02 == "True", sa_02_accuracy, NA),
         sa_01_accuracy = ifelse(passed_safety_01 == "True", sa_01_accuracy, NA),
         sa_005_accuracy = ifelse(passed_safety_005 == "True", sa_005_accuracy, NA),
         sa_001_accuracy = ifelse(passed_safety_001 == "True", sa_001_accuracy, NA))

# plot
sim_results_converged_accuracy |>
  dplyr::select(c(lr_accuracy, sa_02_accuracy, sa_01_accuracy, sa_005_accuracy, 
                  sa_001_accuracy, sample_size)) |>
  rename("Sample Size" = sample_size,
         "LR" = lr_accuracy,
         "SA (0.2)" = sa_02_accuracy,
         "SA (0.1)" = sa_01_accuracy,
         "SA (0.05)" = sa_005_accuracy,
         "SA (0.01)" = sa_001_accuracy) |>
  pivot_longer(cols = -c(`Sample Size`),
               names_to = "model",
               values_to = "accuracy") |>
  mutate(accuracy = 100*accuracy) |>
  ggplot(mapping = aes(x = model, y = accuracy, color = model)) +
  geom_boxplot() +
  scale_x_discrete(limits = c("LR", "SA (0.2)", "SA (0.1)", "SA (0.05)", "SA (0.01)")) +
  theme_minimal() + 
  facet_wrap(~`Sample Size`) +
  guides(color = "none") +
  labs(x = "Algorithm",
       y = "Overall Model Accuracy (%)")
```


### The Trade-Off Between Discrimination and Accuracy

```{r ch4fig4, echo = FALSE, fig.width = 6.3, fig.height = 5, warning = FALSE, message = FALSE, fig.cap = "The Accuracy-Discrimination Trade-Off of Seldonian Algorithms by Sample Size"}
# get long discrimination data set for plotting (1000 rows)
sim_results_converged_disc_long <- sim_results_converged_disc |>
  dplyr::select(c(lr_discrimination, sa_02_disc_stat, sa_01_disc_stat, sa_005_disc_stat, 
                  sa_001_disc_stat, sample_size, dataset_id)) |>
  mutate(sample_size = factor(case_when(sample_size == 500 ~ "n = 500",
                                 sample_size == 1000 ~ "n = 1000",
                                 sample_size == 2500 ~ "n = 2500",
                                 sample_size == 5000 ~ "n = 5000"
                                 ), 
                              levels = c("n = 500", "n = 1000", "n = 2500", "n = 5000"))) |>
  rename("Sample Size" = sample_size,
         "LR" = lr_discrimination,
         "SA (0.2)" = sa_02_disc_stat,
         "SA (0.1)" = sa_01_disc_stat,
         "SA (0.05)" = sa_005_disc_stat,
         "SA (0.01)" = sa_001_disc_stat) |>
  pivot_longer(cols = -c(`Sample Size`, dataset_id),
               names_to = "model",
               values_to = "discrimination")

# get long accuracy data set for plotting (1000 rows)
sim_results_converged_accuracy_long <- sim_results_converged_accuracy |>
  dplyr::select(c(lr_accuracy, sa_02_accuracy, sa_01_accuracy, sa_005_accuracy, 
                  sa_001_accuracy, sample_size, dataset_id)) |>
  mutate(sample_size = factor(case_when(sample_size == 500 ~ "n = 500",
                                 sample_size == 1000 ~ "n = 1000",
                                 sample_size == 2500 ~ "n = 2500",
                                 sample_size == 5000 ~ "n = 5000"
                                 ), 
                              levels = c("n = 500", "n = 1000", "n = 2500", "n = 5000"))) |>
  rename("Sample Size" = sample_size,
         "LR" = lr_accuracy,
         "SA (0.2)" = sa_02_accuracy,
         "SA (0.1)" = sa_01_accuracy,
         "SA (0.05)" = sa_005_accuracy,
         "SA (0.01)" = sa_001_accuracy) |>
  pivot_longer(cols = -c(`Sample Size`, dataset_id),
               names_to = "model",
               values_to = "accuracy")

# join both data sets
accuracy_disc <- inner_join(sim_results_converged_disc_long, sim_results_converged_accuracy_long,
                            by = c("dataset_id", "Sample Size", "model")) |>
  pivot_longer(cols = -c(`Sample Size`, dataset_id, model),
               names_to = "statistic", 
               values_to = "value")

# plot error-bars by sample size
color <- c("purple", "orange")

accuracy_disc |> 
  group_by(model, statistic, `Sample Size`) |>
  summarise(avg = mean(value, na.rm = TRUE),
            se = sd(value, na.rm = TRUE)) |>
  mutate(statistic = ifelse(statistic == "accuracy", "accuracy (higher is better)", "discrimination (lower is better)")) |>
  ggplot(mapping = aes(x = model, y = avg, color = statistic)) +
  geom_point() +
  geom_errorbar(mapping = aes(ymin = avg - se,
                              ymax = avg + se,
                              width = 0.2)) +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  scale_color_manual(values = c("accuracy (higher is better)" = color[1],
                                "discrimination (lower is better)" = color[2])) +
  scale_x_discrete(limits = c("LR", "SA (0.2)", "SA (0.1)", "SA (0.05)", "SA (0.01)")) +
  facet_wrap(~`Sample Size`) +
  labs(x = "Algorithm",
       y = "Performance Measure",
       color = " ")
```

Figure \@ref(fig:ch4fig4) visualizes the trade-off between accuracy and discrimination for Seldonian algorithms. As previously observed, the value of the discrimination statistic decreases as the constraint is tightened across all sample sizes. Similarly, the accuracy decreases as the constraint is tightened. Table \@ref(tab:ch4tab2) showed that the Seldonian solutions returned in the $\epsilon = 0.01$ setting had the highest probability of passing the safety constraint. Figure \@ref(fig:ch4fig4) similarly displays that these models, on average, have a discrimination statistic of 0. However, notice that these models also have very poor predictive performance, averaging 50% and rendering them non-informative. 

On the other hand, Seldonian algorithms with liberal fairness constraints such as $\epsilon = 0.2, 0.1$ and larger sample sizes perform comparably to logistic regression with regard to accuracy while offering some improvement in model fairness. 


### Non-Convergent Seldonian Models

Finally, this section analyzes the instances that did not pass the safety test. Evaluating the logistic regression performance by whether the Seldonian solution passed or failed the safety test revealed unexpected results. Figure \@ref(fig:ch4fig5) displays that the logistic regression accuracy tended to be slightly higher in the cases when the Seldonian candidate solution failed the safety test, compared to the cases where it passed. Attributing this observation to higher discrimination in these cases may be tempting, but Figure \@ref(fig:ch4fig6) suggests otherwise. On the contrary, $d(\theta_{LR})$ was lower or comparable in the cases when the Seldonian candidate solution failed the safety compared to the cases when it passed. 

Nonetheless, analyzing the candidate solutions themselves in both cases where the solution passed the safety test or not revealed results that are more expected. The accuracy of the candidate solutions tended to be lower in the cases when the solution failed the safety test. Similarly, the discrimination statistic tended to be higher in such cases. Although inconclusive, these results suggest that there may be a flaw in how Seldonian algorithms select candidate solutions from traditional ML solutions, such as logistic regression. It appears that in some cases, the optimization process may lead to worse candidate solutions, both in accuracy and discrimination, compared to the standard ML solution itself. This warrants future research and work to investigate. 


```{r ch4fig5, echo = FALSE, fig.width = 6.3, fig.height = 7, warning = FALSE, message = FALSE, fig.cap = "Evaluating Logistic Regression Accuracy on the Data Sets by Seldonian Convergence"}
sim_results |>
  dplyr::select(c(lr_accuracy, passed_safety_02, passed_safety_01, passed_safety_005, 
                  passed_safety_001, sample_size)) |>
  pivot_longer(cols = -c(sample_size, lr_accuracy),
               names_to = "epsilon",
               values_to = "passed_safety") |>
  mutate(epsilon = case_when(epsilon == "passed_safety_02" ~ 0.2,
                             epsilon == "passed_safety_01" ~ 0.1,
                             epsilon == "passed_safety_005" ~ 0.05,
                             epsilon == "passed_safety_001" ~ 0.01)) |>
  mutate(sample_size = factor(case_when(sample_size == 500 ~ "n = 500",
                                 sample_size == 1000 ~ "n = 1000",
                                 sample_size == 2500 ~ "n = 2500",
                                 sample_size == 5000 ~ "n = 5000"
                                 ), 
                              levels = c("n = 500", "n = 1000", "n = 2500", "n = 5000")),
         epsilon = factor(epsilon, levels = c(0.2, 0.1, 0.05, 0.01))) |>
  ggplot(mapping = aes(x = passed_safety, y = lr_accuracy, color = passed_safety)) +
  geom_boxplot() +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  facet_grid(rows = vars(sample_size), cols = vars(epsilon)) +
  guides(color = 'none') +
  labs(x = "Seldonian Algorithm Passed Safety Test",
       y = "Logistic Regression Accuracy",
       subtitle = latex2exp::TeX("              $\\epsilon$"))
```


```{r ch4fig6, echo = FALSE, fig.width = 6.3, fig.height = 7, warning = FALSE, message = FALSE, fig.cap = "Evaluating Logistic Regression Discrimination on the Data Sets by Seldonian Convergence"}
sim_results |>
  dplyr::select(c(lr_discrimination, passed_safety_02, passed_safety_01, passed_safety_005, 
                  passed_safety_001, sample_size)) |>
  pivot_longer(cols = -c(sample_size, lr_discrimination),
               names_to = "epsilon",
               values_to = "passed_safety") |>
  mutate(epsilon = case_when(epsilon == "passed_safety_02" ~ 0.2,
                             epsilon == "passed_safety_01" ~ 0.1,
                             epsilon == "passed_safety_005" ~ 0.05,
                             epsilon == "passed_safety_001" ~ 0.01)) |>
  mutate(sample_size = factor(case_when(sample_size == 500 ~ "n = 500",
                                 sample_size == 1000 ~ "n = 1000",
                                 sample_size == 2500 ~ "n = 2500",
                                 sample_size == 5000 ~ "n = 5000"
                                 ), 
                              levels = c("n = 500", "n = 1000", "n = 2500", "n = 5000")),
         epsilon = factor(epsilon, levels = c(0.2, 0.1, 0.05, 0.01))) |>
  ggplot(mapping = aes(x = passed_safety, y = lr_discrimination, color = passed_safety)) +
  geom_boxplot() +
  theme_minimal() +
  theme(legend.position = 'bottom') +
  guides(color = 'none') +
  facet_grid(rows = vars(sample_size), cols = vars(epsilon)) +
  labs(x = "Seldonian Algorithm Passed Safety Test",
       y = "Logistic Regression Discrimination",
       subtitle = latex2exp::TeX("              $\\epsilon$"))
```


\newpage

## On Seldonian Algorithms

In conclusion, Seldonian algorithms offer a way forward in fair machine learning. However, this simulation study made some concerns more apparent, particularly in the classification setting, which was the focus of this research. While the probability of a Seldonian solution that passed the safety test was high, a closer look reveals that many of these solutions did not meet the specified fairness constraint, especially in the larger sample sizes. Similarly, while Seldonian solutions, on average, produced fairer outcomes as the constraint was tightened, a deeper dive also reveals a negative trend in predictive performance, more so for tighter constraints and smaller sample sizes. 

The observed trade-off in accuracy and discrimination underscored the need to establish techniques for finding optimal thresholds that balance fairness and predictive accuracy as desired, specific to the application and domain. Additionally, previous research, most of which employ loose fairness constraints, suggested that the predictive performance of Seldonian algorithms gains parity with that of standard procedures as sample size increases. However, that was only observed for the loosest constraint ($\epsilon = 0.2$). Otherwise, there were no consistent trends by sample size, likely because the constraints set were tighter than is typical for other research studies employing and evaluating Seldonian algorithms. Indeed, when the fairness constraint was liberal and there was ample data, the Seldonian framework worked exactly as expected; however, practical satisfaction of the tightest constraints is precisely what would be most desirable.

Ultimately, these results underscore the importance of pushing such algorithms to their limits and using the findings of such research to improve upon or help build practical and robust methods that address the problem of algorithmic bias, as Seldonian algorithms do, but also produce solutions that retain standard predictive performance while also simultaneously offering a significant reduction in discriminatory outcomes.



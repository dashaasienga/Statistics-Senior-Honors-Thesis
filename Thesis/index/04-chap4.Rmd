# Seldonian Algorithms for Classification: A Simulation Study {#chap-4}

```{r, include = FALSE}
library(mosaic)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(knitr)
library(kableExtra)
library(ggplot2)
library(GGally)
library(reshape2)
```

Chapter \@ref(chap-2) introduced the Seldonian framework, which offers probabilistic guarantees for satisfying defined behavioral constraints. However, the toy linear regression example demonstrated some of the limitations of Seldonian algorithms, particularly in scenarios with limited sample sizes where convergence may pose a challenge. In practice, sample sizes vary. Furthermore, Chapter \@ref(chap-3) applied the Seldonian framework to a classification setting using the COMPAS data set. The goal was to produce recidivism predictions that elicit fairer outcomes along racial lines, with a specific focus on Black and White defendants. However, the application illustrated that while the behavioral constraint was satisfied in all four cases, there was a tradeoff in the model's accuracy. In other words, a non-informative model can be perfectly Seldonian, but what value does a theoretically fair non-informative model add?

To address this concern and further study these tradeoffs, this chapter investigates the efficacy and applicability of Seldonian algorithms in practical classification settings with class balance and better predictive performance than COMPAS. By conducting a simulation study, the aim is to evaluate whether Seldonian approaches can effectively produce fairer outcomes and mitigate discriminatory tendencies, drawing on the fairness notion of separation (or equality of odds) defined in Chapter \@ref(fairnessdefinitions) and set up in Chapter \@ref(seldapp). Specifically, the objective is to assess the feasibility of leveraging Seldonian algorithms to enhance the fairness and equity of predictive models across various practical classification tasks.

## Simulation Design {#sim-design}

Before conducting the simulation study and analyzing the results, this section provides detailed explanations of the simulation set-up and design. 

### Aims

This simulation study presents a proof of concept for using the Seldonian framework in classification problems with robust predictive performance. Seldonian algorithms $\textit{can}$ fail, especially with insufficient data, as elucidated in Chapter \@ref(toy). On the flip side, as elucidated in Chapter \@ref(seldapp), Seldonian algorithms can succeed in their objective for fairer outcomes but fail to produce a useful model. Solutions returned are also probabilistic and may not always satisfy the constraint despite passing the safety test. With these limitations in mind, this simulation study aims to empirically assess the predictive performance of Seldonian algorithms, compared to that of the standard ML approach, in practical classification settings. 

### Data-Generation Mechanism {#data-gen} 

```{r, echo = FALSE}
compas_sim_path <-
  "/home/dasienga24/Statistics-Senior-Honors-Thesis/Data Sets/COMPAS/compas_sim.csv"
compas_sim_parent <- read.csv(compas_sim_path)

lr <- glm(is_recid ~ age + prior_offense,
          data = compas_sim_parent,
          family = binomial(logit))

lr_accuracy <-
  100 * count(round(lr[["fitted.values"]]) == lr[["y"]]) / nrow(compas_sim_parent)
```

Given that this is a proof-of-concept simulation study, the data-generation mechanism will follow a realistic design. Two of the most informative variables from the COMPAS data set were selected for a simpler set-up: the defendant's age (continuous) and whether or not the defendant had committed any prior offenses (binary). The choice to use the COMPAS data set as a starting point was so that the complex relationships between the predictor variables ($X$) and the protected attribute ($A$) may be retained, especially given that the data was collected for a practical application and elucidates relationships that may be expected in the real world. There are only two levels of the protected attribute, race, that were retained for the study: Black and White. 

To achieve better predictive performance, a linear combination of age and prior offenses was chosen after searching through a range of plausible values: $logit(p_i) = 5 - 0.2 \text{ }Age_i + 0.5 \text{ } \textit{PriorOffense}_i \text{ | } i \in \{1,2, \ldots, 9387\}$, where $p_i$ is the probability of a defendant recommitting a crime within two years. The response variable -- whether or not a defendant recommitted a crime within two years -- was then drawn from a Bernoulli distribution with probability of reoffense equal to $p_i$ for each defendant. Finally, class balance was induced on the data set by randomly sampling, with replacement, 1250 observations from each of the four intersections of race and recidivism status: Black and White defendants who recidivated and did not recidivate. Note that this scheme (Appendix \@ref(appendix-f)) assumes equal prevalence of the response variable, $Y$, for both levels of the protected attribute. 

The parent simulation data set, thus, contains 5000 observations, achieves perfect class balance, and has a baseline logistic regression accuracy of `r lr_accuracy`%. This offers a realistic improvement from the accuracy of 70.2% obtained from the COMPAS logistic regression. However, because of the class balance, the lowest achievable accuracy is now 50% (a random/ coin-flip model) rather than 64.4%. This will allow more room for the Seldonian algorithm model accuracies to vary. 

One thousand total data sets of size $n = 500$, $n = 1000$, $n = 2500$, and $n = 5000$ will be sampled, with replacement, from this parent data set for the simulation study, with 250 data sets in each of the four partitions. 

### Target

The target of this simulation study is prediction, that is, to evaluate Seldonian algorithms for predictive performance in classification settings. 

### Methods

Because the Seldonian framework was designed to place probabilistic fairness constraints on traditional algorithms, solutions produced by Seldonian algorithms will be compared to those produced through the standard ML framework, specifically logistic regression, to assess the predictive performance and feasibility of the Seldonian framework.

Drawing from the fairness definitions described in Chapter \@ref(fairnessdefinitions) with a particular focus on the COMPAS example in Chapter \@ref(chap-3), it may be less important that a model satisfies independence. That is, that the model, on average, has the same likelihood of a positive prediction for all levels of the protected attribute. Instead, it may be more realistic to expect that the model be equally wrong or equally correct in its predictions of $Y$ for each protected attribute. For this reason, one fairness constraint will be set on the Seldonian algorithms to satisfy separation, otherwise known as equality of the error rates or equality of odds, as defined in Equation \@ref(ch4eq1) within some margin $\epsilon$ and with $1 - \delta$ % confidence. Similar to Chapter \@ref(seldapp), $\epsilon$ will be set to four levels: $0.2, 0.1, 0.05, \text{and } 0.01$. $\delta$ will be set to $0.05$ to guarantee 95% confidence. 

\begin{equation}
\label{ch4eq1}
g(\theta): abs[(FPR | \text{A = a} - FPR | \text{A = b}) + (FNR | \text{A = a} - FNR | \text{A = b})] - \epsilon.
\end{equation}

Simulated data sets will be generated independently before being fed into a logistic regression algorithm and the four Seldonian algorithms. The relevant performance measures, as detailed in Section \@ref(performancemeasures), will be recorded for each trial. Two hundred and fifty trials will be run for each sample size as described in Section \@ref(data-gen). The code will be run in Python using the Rstudio software interface and the Amherst College High-Performance Computing System. The `seldonian` package is readily available for installation in Python and is equipped with functions that allow for the implementation of Seldonian algorithms. Other Python packages, such as `pandas`, `numpy`, and `sklearn`, will help conduct the rest of the simulation study. 


### Performance Measures {#performancemeasures}

Compared to the logistic regression models, the predictive performance of the Seldonian models will be assessed along three dimensions: the probability of a Seldonian solution, the accuracy of the solutions, and the satisfaction or violation of the behavioral constraints set across all trials within a specific setting (sample size).

It is essential to account for the fact that the Seldonian algorithms will not always return a solution. Recording the probability of a solution being returned in each sample setting will be crucial for evaluating the practical feasibility of this framework. Additionally, for fair statistical comparisons, the first and second performance measures will be compared only with Seldonian solutions that converge. However, the performance of the logistic regression models in trials where no solutions were found using the Seldonian framework will be analyzed to elucidate learnings about the nature of those trials.

Additionally, for each simulation trial, the overall accuracy of both the convergent Seldonian models and the logistic regression model will be recorded and eventually averaged over the number of data sets in the specific sample size. Both the mean and the standard error will be reported in tabular format and visualized graphically to compare the models' predictive performances and, potentially, evaluate the trade-offs that may occur by employing the Seldonian framework and enforcing behavioral constraints.

Finally, the satisfaction or violation of the behavioral constraint identified in Equation \@ref(ch4eq1) will be assessed in two ways. First, for each sample size as described above, a count of the times both frameworks satisfied the behavioral constraint, by some margin $\epsilon$, will be reported in tabular format. Additionally, the discrimination statistics as defined in Equation \@ref(ch4eq2) will be recorded for each simulation trial. The mean and standard error will be reported and visualized for each sample setting to compare the magnitude and direction of the models' unfairness with regard to the separation fairness definition.

\begin{equation}
\label{ch4eq2}
d(\theta) = abs[(FPR | \text{A = a} - FPR | \text{A = b}) + (FNR | \text{A = a} - FNR | \text{A = b})]
\end{equation}

## Simulation Results {#sim-results}

As described in Section \@ref(sim-design), the simulation results will be analyzed along 3 key performance measures: convergence, discrimination, and accuracy.

```{r, echo = FALSE}
# read in logistic results

lr_500 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_500.csv"
  )
lr_1000 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_1000.csv"
  )
lr_2500 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_2500.csv"
  )
lr_5000 <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/R/Simulation/LogisticRegression/Results/lr_5000.csv"
  )

lr_500 <- lr_500 |>
  mutate(sample_size = 500) |>
  dplyr::select(-X)

lr_1000 <- lr_1000 |>
  mutate(sample_size = 1000) |>
  dplyr::select(-X)

lr_2500 <- lr_2500 |>
  mutate(sample_size = 2500) |>
  dplyr::select(-X)

lr_5000 <- lr_5000 |>
  mutate(sample_size = 5000) |>
  dplyr::select(-X)

logistic_results <- rbind(lr_500, lr_1000, lr_2500, lr_5000)


# read in Seldonian results

seldonian_results <-
  read.csv(
    "/home/dasienga24/Statistics-Senior-Honors-Thesis/Python/COMPAS Simulation/SeldonianSimulation/results/seldonian_sim_results.csv"
  )


# combine both data sets for full results

sim_results <- inner_join(logistic_results,
                          seldonian_results,
                          by = c("sample_size", "dataset_id"))

reps <- nrow(sim_results) / 4
```


### Probability of a Seldonian Solution

All logistic regression trials are expected to return a solution, thus a 100% convergence rate. However, while all the Seldonian trials will propose a candidate solution predicted to pass the safety test with 95% confidence, not all candidate solutions will pass the safety test itself. Table \@ref(tab:ch4tab1) illustrates this. For $\epsilon = 0.2, 0.1, 0.05$, the probability of a Seldonian solution was $> \text{~}90$% for all sample sizes. In fact, for sample sizes $n = 1000, 2500, 5000$, the probability of a Seldonian solution was $> 96$%, suggesting a higher convergence rate for looser fairness constraints and data set sizes greater than $n = 500$. This is consistent with the results in Figure 2.6 from Chapter \@ref(exp). Regardless, a small data set size did not matter for the loosest constraint of $\epsilon = 0.2$.

However, when $\epsilon = 0.01$, which was the tightest constraint, the probability of a solution dropped significantly, and there were no consistent trends across sample sizes, suggesting that the models had trouble passing the safety test in this case, regardless of sample size, likely because the constraint was too strict and not feasible for this data set. Surprisingly, the lowest observed probability of a solution ($24.8$%) is recorded when $\epsilon = 0.01, n = 5000$. 

Overall, across all sample sizes, the probability of a solution generally reduced as the constraint tightened. 

```{r, ch4tab1, echo = FALSE}
sim_results |>
  group_by(sample_size) |>
  summarise(
    LR = 100 * count(lr_convergence == 1) / reps,
    `SA (0.2)` = 100 * count(passed_safety_02 == "True") / reps,
    `SA (0.1)` = 100 * count(passed_safety_01 == "True") / reps,
    `SA (0.05)` = 100 * count(passed_safety_005 == "True") / reps,
    `SA (0.01)` = 100 * count(passed_safety_001 == "True") / reps
  ) |>
  rename("Sample Size" = sample_size) |>
  kable(caption = "Probability of Obtaining a Seldonian Solution",
        booktabs = TRUE)
```

$\\$

However, the probability of a solution that passes the safety test is not the end of the story. Table \@ref(tab:ch4tab2) records the proportion of solutions that satisfied the behavioral constraint among those that passed the safety test (Table \@ref(tab:ch4tab1)). When $\epsilon = 0.01$, although a low probability of solution was previously observed, $> 92$% of returned solutions satisfied the behavioral constraint. Additionally, the probability was largest in the $n = 5000$ case, which previously had the lowest probability of returning a solution across the entire simulation study. Except for $n = 5000$, the probability of a Seldonian solution satisfying the behavioral constraint generally increased as the constraint got tighter, suggesting some trade-off between the probability of returning a solution and the probability of that solution satisfying the behavioral constraints. However, there were no consistent trends as sample size increased across all four levels of $\epsilon$, although the probability actually decreased for $\epsilon = 0.1, 0.05$. This is inconsistent with expectations, but perhaps the increased variability introduced by more observations makes it more difficult to constrain the models' unfairness.  

```{r, ch4tab2, echo = FALSE}
# filter discrimination data out for only seldonian solutions that passed the safety test
sim_results_converged_disc <- sim_results |>
  mutate(
    sa_02_disc_stat = ifelse(passed_safety_02 == "True", sa_02_disc_stat, NA),
    sa_01_disc_stat = ifelse(passed_safety_01 == "True", sa_01_disc_stat, NA),
    sa_005_disc_stat = ifelse(passed_safety_005 == "True", sa_005_disc_stat, NA),
    sa_001_disc_stat = ifelse(passed_safety_001 == "True", sa_001_disc_stat, NA)
  )

sim_results_converged_disc |>
  group_by(sample_size) |>
  summarise(
    `SA (0.2)` = round(
      100 * count(sa_02_disc_stat <= 0.2) / count(passed_safety_02 == "True"),
      2
    ),
    `SA (0.1)` = round(
      100 * count(sa_01_disc_stat <= 0.1) / count(passed_safety_01 == "True"),
      2
    ),
    `SA (0.05)` = round(
      100 * count(sa_005_disc_stat <= 0.05) / count(passed_safety_005 == "True"),
      2
    ),
    `SA (0.01)` = round(
      100 * count(sa_001_disc_stat <= 0.01) / count(passed_safety_001 == "True"),
      2
    )
  ) |>
  rename("Sample Size" = sample_size) |>
  kable(caption = "Satisfaction of the Behavioral Constraint by Seldonian Solutions that Passed the Safety Test",
        booktabs = TRUE)
```

$\\$

Finally, Figure \@ref(fig:ch4fig1) visualizes these results to elucidate the convergence of Seldonian algorithms better. As discussed, the probability of a solution passing the safety test (light blue) generally decreases as the constraint gets tighter, regardless of sample size. Notably, the decrease is most drastic for the tightest constraint ($\epsilon = 0.01$), which has a better probability of a solution for smaller sample sizes than larger sample sizes. However, looking at the proportion of solutions that both pass the safety test and satisfy the defined behavioral constraint (dark blue) raises concerns about the efficacy of Seldonian algorithms, especially for the $n = 2500, 5000$ cases. The following three sections further assess the discrimination and accuracy of the Seldonian models that passed the safety test. 


```{r, ch4fig1, echo = FALSE, fig.width = 6.3, fig.height = 6, warning = FALSE, message = FALSE, fig.cap = "Probability of Returning a Solution and Satisfying the Constraint by Sample Size"}
# get long failure data for plotting
sim_failure_long <- sim_results |>
  group_by(sample_size) |>
  summarise(`SA (0.2)` = 100*count(passed_safety_02 == "False")/reps,
            `SA (0.1)` = 100*count(passed_safety_01 == "False")/reps,
            `SA (0.05)` = 100*count(passed_safety_005 == "False")/reps,
            `SA (0.01)` = 100*count(passed_safety_001 == "False")/reps) |>
  pivot_longer(cols = -c(sample_size),
               names_to = "model",
               values_to = "prop_failed_safety") 


# get long convergence data for plotting
sim_converged_long <- sim_results |>
  group_by(sample_size) |>
  summarise(`SA (0.2)` = 100*count(passed_safety_02 == "True")/reps,
            `SA (0.1)` = 100*count(passed_safety_01 == "True")/reps,
            `SA (0.05)` = 100*count(passed_safety_005 == "True")/reps,
            `SA (0.01)` = 100*count(passed_safety_001 == "True")/reps) |>
  pivot_longer(cols = -c(sample_size),
               names_to = "model",
               values_to = "prop_passed_safety")

# get long discrimination data for plotting
sim_satisfied_cstr_long <- sim_results |>
  group_by(sample_size) |>
  summarise(`SA (0.2)` = 100*count(passed_safety_02 == "True" & sa_02_disc_stat <= 0.2)/reps,
            `SA (0.1)` = 100*count(passed_safety_01 == "True" & sa_01_disc_stat <= 0.1)/reps,
            `SA (0.05)` = 100*count(passed_safety_005 == "True" & sa_005_disc_stat <= 0.05)/reps,
            `SA (0.01)` = 100*count(passed_safety_001 == "True" & sa_001_disc_stat <= 0.01)/reps) |>
  pivot_longer(cols = -c(sample_size),
               names_to = "model",
               values_to = "prop_passed_safety_satisfied_cstr")

# join both data sets
conv_disc <- inner_join(sim_converged_long, sim_satisfied_cstr_long,
                        by = c("sample_size", "model")) |>
  mutate(prop_passed_safety = prop_passed_safety - prop_passed_safety_satisfied_cstr) |>
  rename("prop_passed_safety_failed_cstr" = prop_passed_safety) |>
  inner_join(sim_failure_long,
             by = c("sample_size", "model")) |>
  rename("failed safety" = prop_failed_safety,
         "passed safety, didn't satisfy constraint" = prop_passed_safety_failed_cstr,
         "passed safety, satisfied constraint" = prop_passed_safety_satisfied_cstr) |>
  pivot_longer(cols = -c(sample_size, model),
               names_to = "statistic", 
               values_to = "value") |>
  mutate(sample_size = factor(case_when(sample_size == 500 ~ "n = 500",
                                 sample_size == 1000 ~ "n = 1000",
                                 sample_size == 2500 ~ "n = 2500",
                                 sample_size == 5000 ~ "n = 5000"
                                 ), 
                              levels = c("n = 500", "n = 1000", "n = 2500", "n = 5000"))) 

# plot
conv_disc |> 
  mutate(model = factor(model, 
                        levels = c("SA (0.2)", "SA (0.1)", "SA (0.05)", "SA (0.01)"))) |>
  group_by(model, statistic, sample_size) |>
  summarise(avg = mean(value, na.rm = TRUE)) |>
  ggplot(mapping = aes(x = model, y = avg, fill = statistic)) +
  geom_col() +
   theme_minimal() +
  theme(legend.position = 'bottom') +
  scale_fill_brewer(palette = "Blues") +
  facet_wrap(~sample_size) +
  labs(y = "Percent",
       x = "Algorithm",
       fill = " ")
```



### Discrimination 

CHECK blue figure size, adjust legend, axes etcx

### Accuracy

### The Trade-Off Between Discrimination and Accuracy

### Non-Convergent Seldonian Models

## Discussion {#sim-disc}




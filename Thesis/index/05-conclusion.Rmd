# Conclusion {#conclusion}

Using the standard ML procedure for real-life applications can result in algorithmic bias: a situation where an algorithmâ€™s predictions systematically discriminate against a demographic group, perpetuating or even amplifying human biases. Accordingly, this research paper set out to investigate whether it is feasible to produce fairer models within the machine learning and artificial intelligence landscape. This goal entailed two key questions. One question involved how fairness can be mathematically and statistically defined; that is, what does it mean for a computational model to be fair to begin with? The second question asked, if fairness definitions can be mathematically tractable, how can fairness constraints, thus, be enforced in model optimization to result in less discriminatory outcomes?

$\\$

Chapter 1 explores various definitions of fairness, both in continuous and categorical settings. Separation (equality of the error rates), a classification group fairness definition, was most important for this research. Conflicts in simultaneously enforcing different fairness definitions highlighted the importance of an interdisciplinary team of experts to pick which fairness metrics are most relevant for the specific domain and application. 

Having defined (un)fairness mathematically, Chapter 2 introduces the Seldonian framework, which aims to create algorithms capable of learning from data to prevent unwanted outcomes with high confidence. Seldonian algorithms were designed to improve upon other remedies for algorithmic bias by allowing probabilistic constraints on undesirable behavior to be placed more easily without detailed knowledge of the specific problem or the distribution of the data. On the contrary, this research study found that implementing the Seldonian algorithm requires knowledge of the specific problem to define the appropriate constraint. Furthermore, in practice, $\textit{quasi}$-Seldonian algorithms, which make some assumptions about the data, are more commonly employed, suggesting that the idea of a fully Seldonian algorithm is more theoretical than practical. 

Experimenting with a toy $\textit{quasi}$-Seldonian linear regression algorithm illustrated some desirable qualities, such as meeting defined constraints, and some limitations, such as performance loss and convergence issues for small sample sizes. As an extension of this, Chapter 3 delves into the properties of Seldonian algorithms within a group-blind classification setting by applying the Seldonian framework to the COMPAS recidivism data set, with stricter fairness constraints than typical, and assessing its predictive outcomes. While Seldonian algorithms successfully resulted in fairer outcomes compared to logistic regression and the COMPAS tool itself, the models lost predictive performance as the constraint tightened -- they got progressively worse in correctly classifying observations into the positive (minority) class, with the final model classifying every observation into the negative (majority) class. Note that $\textit{separation}$, as the fairness definition, also holds a strong assumption about the objectivity of the response variable, which should be treated with caution because of the existent racial biases in the criminal justice system.

Finally, Chapter 4 focuses on accuracy-fairness trade-offs in classification settings with class balance and improved performance over COMPAS. Despite a high probability of obtaining a Seldonian solution, many failed to meet the set constraints. Similarly, while tightening the fairness constraints led to fairer outcomes, deeper analysis revealed declining predictive performance to a mere 50%, akin to random chance. Overall, these results raise concerns about the efficacy of Seldonian algorithms in practical classification settings.

$\\$

In conclusion, although a step in the right direction toward fair ML, the Seldonian framework is far from the perfect solution. Seldonian algorithms may yield no solution, unfair results, or fair results with poor accuracy, as there is a practical limit to how much fairness can be enforced while still retaining reasonable model predictive performance. In line with concerns about this trade-off throughout the fair ML landscape, another proposed solution is Microsoft's Fairlearn, designed to help navigate trade-offs between fairness and model performance [@bird2020fairlearn]. Thus, continued work involves investigating similar ways to balance fairness and predictive performance within the Seldonian framework.

Furthermore, this study focused on one group fairness definition specifically in the classification setting. To further understand how Seldonian algorithms can be employed more practically, assessing performance in practical continuous settings and enforcing different group fairness definitions and even subgroup or individual notions of fairness will be necessary for a full-picture analysis. Finally, many ongoing research studies are investigating ways to build more equitable algorithms. Performing a holistic comparison of Seldonian outcomes with other state-of-the-art fair ML tools, such as Microsoft's Fairlearn and IBM's Fairness 360 AI, as well as other theoretical remedies for algorithmic bias, such as hard and soft constraints on the objective function, will be important in determining the advantages and drawbacks of all of these technologies, propelling researchers toward the best solution for the problem of algorithmic bias. 

However, underpinning all this work is the crucial understanding that fairness in ML and AI systems is more than just a data-driven solution. The best remedy is eliminating bias in the data, which involves intensive social work. On the back end, domain experts in every field also have a role to play in coming to a consensus on which definitions of fairness are most important. 

Ultimately, because there are many complex sources of unfairness, it is becoming increasingly clear that it is not possible to fully debias an algorithm or guarantee fairness in every sense of that word. Needless to say, efforts to mitigate harm as much as possible moving forward into the future are necessary and $\textit{will}$ require both social and technical input.



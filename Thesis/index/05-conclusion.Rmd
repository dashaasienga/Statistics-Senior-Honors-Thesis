# Conclusion {#conclusion}

Using the standard ML procedure for real-life applications can result in algorithmic bias: a situation where an algorithm’s predictions systematically discriminate against a demographic group, perpetuating or even amplifying human biases. Accordingly, this research paper set out to investigate whether it is feasible to produce fairer models within the machine learning and artificial intelligence landscape. This goal entailed two key questions. One is how fairness can be mathematically and statistically defined; that is, what does it mean for a computational model to be fair to begin with? And two, if fairness definitions can be mathematically tractable, how can fairness constraints, thus, be enforced in model optimization to result in less discriminatory outcomes?

$\\$

In Chapter 1, various definitions of fairness were explored in continuous and categorical settings. Within both settings, there is a spectrum of fairness definitions, from group definitions, which focus on achieving parity of a defined fairness metric across all fixed demographic groups but do not guarantee fairness to subgroups or individuals, to individual definitions, which are based on the notion that similar individuals should be treated similarly along some defined similarity or inverse distance metrics but can be impractical in application. Three key group fairness definitions in the classification setting were important for this research: independence, separation, and sufficiency. Conflicts in simultaneously enforcing these definitions highlighted the importance of human value in picking which fairness metrics are most relevant for the specific domain and application. 

Chapter 2 introduced the Seldonian framework, which is premised on the notion that if ‘unfair’ or ‘unsafe’ outcomes or behaviors can be defined mathematically -- and chosen accordingly --, then it should be possible to create algorithms that can learn from the data on how to avoid these unwanted results with high confidence. The standard ML approach was explored, along with its limitations and potential remedies. Many remedies require strong assumptions about the data and knowledge of the specific problem -- Seldonian algorithms were designed to address this problem by allowing probabilistic constraints on undesirable behavior (mathematically defined) to be placed more easily without detailed knowledge of the specific problem or the distribution of the data. Quasi-Seldonian algorithms, however, are an extension of this framework that allow for some assumptions when the exact values of parameters are unknown. Experimenting with a toy quasi-Seldonian linear regression algorithm illustrated some desirable qualities in satisfying the set constraints as well as some limitations, such as performance loss and convergence issues in settings with small sample sizes. 

Chapter 3 further delved into the properties of Seldonian algorithms within a classification setting by applying the Seldonian framework to the COMPAS recidivism data set and assessing its predictive outcomes compared to the COMPAS tool itself and logistic regression, a standard ML procedure. Analysis of the COMPAS data set elucidated, in a practical way, that even a group-blind classifier will not be entirely blind to the demographic group because of the correlations present and the information it gains about the group from the proxy variables. Accordingly, the COMPAS tool and the logistic regression model exhibited disparate outcomes along the separation fairness definition. However, Seldonian algorithms successfully met the fairness constraints defined, although there was a tradeoff with model informativeness and performance. 

Finally, Chapter 4 further studied these tradeoffs to investigate the efficacy and applicability of Seldonian algorithms in practical classification settings with class balance and better predictive performance than COMPAS. The simulation study, which set stricter constraints than is typical for other research studies employing Seldonian algorithms, made some concerns more apparent, particularly in the classification setting. While the probability of obtaining a Seldonian solution was high, many of these solutions did not meet the actual constraint. Similarly, while Seldonian solutions, on average, produced fairer outcomes as the constraint was tightened, a deeper dive also reveals a negative trend in predictive performance.

$\\$

In conclusion, although a step in the right direction toward fair ML, the Seldonian framework is far from the perfect solution. Seldonian algorithms may still yield unfair results, and even producing fairer outcomes, there is a practical limit to how much fairness can be enforced while retaining reasonable model predictive performance. In line with such concerns throughout the fair ML landscape, Microsoft's Fairlearn is designed to help navigate trade-offs between fairness and model performance [@bird2020fairlearn]. Thus, continued work involves investigating similar ways to optimize fairness and predictive performance within the Seldonian framework and find a Pareto-optimal solution, assessing Seldonian performance in practical continuous settings, exploring how subgroup or individual notions of fairness can fit into the Seldonian framework as well as their performance in comparison to the group notions that were the focus of this research, and finally, comparing Seldonian outcomes with other state-of-the-art fair ML tools such as Microsoft's Fairlean and IBM's Fairness 360. However, underpinning all this work is the crucial understanding that fairness in ML and AI systems is more than just a data-driven solution. Because there are many complex sources of unfairness, it is not feasible to fully debias an algorithm or guarantee fairness in every sense of that word. However, efforts to mitigate harm as much as possible moving forward into the future are necessary and will, therefore, require both social and technical input.



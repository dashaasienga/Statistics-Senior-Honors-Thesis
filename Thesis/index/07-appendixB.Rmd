# Quasi-Seldonian Linear Regression Python Code {#appendix-b}

The `reticulate` package is useful for setting up the correct Python environment within RStudio.

```{r, eval = FALSE}
library(reticulate)
use_python("/cm/shared/apps/amh-Rstudio/python-3.11.4/bin/python3", 
           required = TRUE)
#py_config()
#conda_list()
```

\noindent Python packages not already pre-installed need to be imported into the `reticulate` package first before being imported into the Python environment, as illustrated using `sklearn` below.

```{r, eval = FALSE}
sklearn <- import("sklearn")
```

```{python, eval = FALSE}
import sklearn
```

\noindent `math` provides access to the standard mathematical functions. `numpy` supports large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. `sys` provides functions and variables used to manipulate different parts of the Python run-time environment. `sklearn` features various classification, regression and clustering algorithms. `scipy.stats` contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more. `scipy.optimize` provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programing, constrained and nonlinear least-squares, root finding, and curve fitting.

```{python, eval = FALSE}
import math
import numpy as np
import sys
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from scipy.stats import t
from scipy.optimize import minimize
```

```{python, eval = FALSE}
# display 5 decimal places for all results
np.set_printoptions(precision=5, suppress=True)
```

\noindent The `tinv` function returns the inverse of `Student's t` CDF using the degrees of freedom in `nu` for the corresponding probabilities in `p`.

```{python, eval = FALSE}
def tinv(p, nu):
    return t.ppf(p, nu)
```

\noindent The `stddev` function computes the sample standard deviation of the vector $v$, with Bessel's correction. In statistics, Bessel's correction is the use of $n-1$ instead of $n$ in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance.

```{python, eval = FALSE}
def stddev(v):
    n = v.size
    variance = (np.var(v) * n) / (n-1) 
    return np.sqrt(variance) 
```

\noindent The `ttestUpperBound` function computes a ($1 - \delta$)-confidence upper bound on the expected value of a random variable using Student's t-test. It analyzes the data in $v$, which holds i.i.d. samples of the random variable. The upper confidence bound is given by $\text{sampleMean} + \frac{\text{sampleStandardDeviation}}{\sqrt(n)} * tinv(1-\delta, n-1)$, where $n$ is the number of observations in $v$.


```{python, eval = FALSE}
def ttestUpperBound(v, delta):
    n  = v.size
    res = v.mean() + stddev(v) / math.sqrt(n) * tinv(1.0 - delta, n - 1)
    return res
```


\noindent The `predictTTestUpperBound` function works similarly to `ttestUpperBound`, but returns a more conservative upper bound. This function uses data in the vector $v$ to compute all relevant statistics (mean and standard deviation) but assumes that the number of points being analyzed is $k$ instead of $|v|$.

This function is used to estimate what the output of `ttestUpperBound` would be if it were to be run on a new vector, $v$, containing values sampled from the same distribution as the points in $v$. The 2.0 factor in the calculation is used to double the width of the confidence interval when predicting the outcome of the safety test in order to make the algorithm less confident/ more conservative.

```{python, eval = FALSE}
def predictTTestUpperBound(v, delta, k):
    
    res = v.mean() + 2.0 * stddev(v) / math.sqrt(k) * tinv(1.0 - delta, k - 1)
    return res
```

\noindent The function `main()` below is set up to run a simple experiment.

```{python, eval = FALSE}
def main():
    np.random.seed(123)  
    numPoints = 5000   

    (X,Y)  = generateData(numPoints)  

    gHats  = [gHat1, gHat2] 
    deltas = [0.1, 0.1]

    (result, found) = QSA(X, Y, gHats, deltas) 
    
    if found:
        print("A solution was found: [%.10f, %.10f]" % (result[0], result[1]))
        print("fHat of solution (computed over all data, D):", 
        fHat(result, X, Y))
    else:
        print("No solution found")
```

\noindent The `generateData` function samples data as described in the problem description.

```{python, eval = FALSE}
def generateData(numPoints):
    X =     np.random.normal(0.0, 1.0, numPoints) 
    Y = X + np.random.normal(0.0, 1.0, numPoints) 
    return (X,Y)
```


\noindent The `predict` function takes in a solution $\theta$ and an input $X$, and produces as output the prediction of $Y$. In other words, this function will implement $\hat{y}(X, \theta)$.

\noindent Recall $\hat{y}(X, \theta) = \theta_1 X + \theta_2$.

```{python, eval = FALSE}
def predict(theta, x):
    return theta[0] + theta[1] * x
```

\noindent The `fHat` function specifies the primary objective: to minimize the sample mean squared error. since the attempt is to maximize $\hat{f}$, however, the negative sample mean squared error is returned, so that maximizing $\hat{f}$ corresponds to minimizing the mean squared error.

```{python, eval = FALSE}
def fHat(theta, X, Y):
    n = X.size          
    res = 0.0           
    for i in range(n):  
        prediction = predict(theta, X[i])                
        res += (prediction - Y[i]) * (prediction - Y[i]) 
    res /= n            
    return -res         
```

\noindent The `gHat1` and `gHat2` functions set up the behavioral constraints.  

```{python, eval = FALSE}
def gHat1(theta, X, Y):
    n = X.size          
    res = np.zeros(n)   
    for i in range(n):
        prediction = predict(theta, X[i])                   
        res[i] = (prediction - Y[i]) * (prediction - Y[i])  
    res = res - 2.0     
    return res

def gHat2(theta, X, Y):
    n = X.size          
    res = np.zeros(n)   
    for i in range(n):
        prediction = predict(theta, X[i])                   
        res[i] = (prediction - Y[i]) * (prediction - Y[i])  
    res = 1.25 - res   
    return res
```

\noindent The `leastSq` function implements least squares linear regression, which will be used as a starting point in the search for a candidate solution. 

```{python, eval = FALSE}
def leastSq(X, Y):
    X = np.expand_dims(X, axis=1) 
    Y = np.expand_dims(Y, axis=1) 
    reg = LinearRegression().fit(X, Y)
    theta0 = reg.intercept_[0]   
    theta1 = reg.coef_[0][0]     
    return np.array([theta0, theta1])
```

\noindent The `QSA` function is the shell code that partitions the data set, gets a candidate solution, and runs the safety test.

```{python, eval = FALSE}
def QSA(X, Y, gHats, deltas):

    candidateData_len = 0.40
    candidateData_X, safetyData_X, candidateData_Y, safetyData_Y = 
    train_test_split(X, Y, test_size=1-candidateData_len, shuffle=False)
  
    candidateSolution = getCandidateSolution(candidateData_X, candidateData_Y, 
    gHats, deltas, safetyData_X.size)

    passedSafety      = safetyTest(candidateSolution, safetyData_X, 
    safetyData_Y, gHats, deltas)

    return [candidateSolution, passedSafety]
```

\noindent The `safetyTest` function uses the previously defined functions to implement the safety test. 

```{python, eval = FALSE}
def safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas):

    for i in range(len(gHats)):  
        g         = gHats[i]  
        delta     = deltas[i] 

    
        g_samples = g(candidateSolution, safetyData_X, safetyData_Y) 

        upperBound = ttestUpperBound(g_samples, delta) 

        if upperBound > 0.0: 
            return False

    return True
```

\noindent Finally, the `candidateObjective` and `getCandidateSolution` functions use a black-box optimization algorithm to search for a candidate solution. The black box algorithm used to search for a candidate solution is called Powell, which is an algorithm designed for finding a local minimum of a function using a bi-directional linear search. Powell, however, is not a constrained algorithm. One way of addressing this limitation is by incorporating the constraint into the objective function as a barrier function. In constrained optimization, a field of mathematics, barrier functions are used to replace inequality constraints by a penalizing term in the objective function that is easier to handle. That is, an approximate solution to the following unconstrained problem:

$$
\theta_c \in arg \: \underset{\theta \in \mathbb{R}^2}{max} 
    \begin{cases} 
      \hat{f}(\theta, D_1)  \text{    if} \:\: \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + 2 \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}} t_{1-\delta_i, |D_2|-1} \leq 0 \forall i \in \{1,2,...,n\}\\
      -100,000 - \sum_{i=1}^n max(0, \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + 2 \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}} t_{1-\delta_i, |D_2|-1})) \text{  otherwise}
    \end{cases}
$$

\noindent In this case, solutions that are predicted not to pass the safety test will not be selected by the optimization algorithm because a large negative performance is assigned to them. This barrier functions encourages Powell to tend towards solutions that will pass the safety test. 



```{python, eval = FALSE}
def candidateObjective(thetaToEvaluate, candidateData_X, candidateData_Y, gHats, 
deltas, safetyDataSize): 

    result = fHat(thetaToEvaluate, candidateData_X, candidateData_Y)

    predictSafetyTest = True     
    
    for i in range(len(gHats)):  
        g         = gHats[i]       
        delta     = deltas[i]      

        g_samples = g(thetaToEvaluate, candidateData_X, candidateData_Y)

        upperBound = predictTTestUpperBound(g_samples, delta, safetyDataSize)

        if upperBound > 0.0:

            if predictSafetyTest:
                predictSafetyTest = False  

                result = -100000.0    

            result = result - upperBound

    return -result  
```

```{python, eval = FALSE}
def getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, 
safetyDataSize):
  
    minimizer_method = 'Powell'
    minimizer_options={'disp': False}

    initialSolution = leastSq(candidateData_X, candidateData_Y)

    res = minimize(candidateObjective, x0=initialSolution, 
    method=minimizer_method, options=minimizer_options, 
    args=(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize))

    return res.x
```

\noindent Calling `main()` returns either a solution or NSF. 

```{python, eval = FALSE}
main()
```

\noindent The following code chunks utilize the above functions to perform the experimentation in Chapter \@ref(exp). `timeit` allows timing of the execution of experiments. `numba` allows the use of a Just-in-Time (JIT) compiler to accelerate Python code.

```{python, eval = FALSE}
#import necessary packages
import timeit               
from numba import jit       
```

```{python, eval = FALSE}
#path where experiment results are saved
bin_path = 
'/home/dasienga24/Statistics-Senior-Honors-Thesis/Thesis/index/'
'experiment_results/chapter_2/'
```

```{python, eval = FALSE}
def run_experiments(worker_id, nWorkers, ms, numM, numTrials, mTest):
    
    # Results of the Seldonian algorithm runs
    ## The following code initializes an array filled with 0's. 
    ## The resulting array will have numTrials rows (each trial) 
    ## and numM columns (each data set size).
    ## Default is 0=False.
    
    seldonian_solutions_found = np.zeros((numTrials, numM)) 
    seldonian_failures_g1     = np.zeros((numTrials, numM)) 
    seldonian_failures_g2     = np.zeros((numTrials, numM)) 
    seldonian_fs              = np.zeros((numTrials, numM)) 
    
    # Results of the Least-Squares (LS) linear regression runs
    LS_solutions_found = np.ones((numTrials, numM))  
    LS_failures_g1     = np.zeros((numTrials, numM)) 
    LS_failures_g2     = np.zeros((numTrials, numM)) 
    LS_fs              = np.zeros((numTrials, numM)) 
    
    
    # Prepares file where experiment results will be saved
    experiment_number = worker_id
    outputFile = bin_path + 'results%d.npz' % experiment_number
    
    
    # Generate the data used to evaluate the primary objective and failure rates
    np.random.seed( (experiment_number+1) * 9999 )
    (testX, testY) = generateData(mTest) 
    
    
    for trial in range(numTrials): #numTrials trials for each value of m 
        for (mIndex, m) in enumerate(ms): 
          
            # Generate the training data, D
            base_seed         = (experiment_number * numTrials)+1
            np.random.seed(base_seed+trial) 
            (trainX, trainY)  = generateData(m)
            
            # Run the Quasi-Seldonian algorithm
            (result, passedSafetyTest) = QSA(trainX, trainY, gHats, deltas)
            
            if passedSafetyTest:
                seldonian_solutions_found[trial, mIndex] = 1                        
                trueMSE = -fHat(result, testX, testY)                               
                seldonian_failures_g1[trial, mIndex] = 1 if trueMSE > 2.0  else 0   
                seldonian_failures_g2[trial, mIndex] = 1 if trueMSE < 1.25 else 0   
                seldonian_fs[trial, mIndex] = -trueMSE                              
                
            else:
                seldonian_solutions_found[trial, mIndex] = 0             
                seldonian_failures_g1[trial, mIndex]     = 0             
                seldonian_failures_g2[trial, mIndex]     = 0            
                seldonian_fs[trial, mIndex]              = None          

            # Run the Least Squares algorithm
            theta = leastSq(trainX, trainY)                              
            trueMSE = -fHat(theta, testX, testY)                         
            LS_failures_g1[trial, mIndex] = 1 if trueMSE > 2.0  else 0   
            LS_failures_g2[trial, mIndex] = 1 if trueMSE < 1.25 else 0   
            LS_fs[trial, mIndex] = -trueMSE                             
        
        
        
    # Save the arrays in a compressed format
    np.savez(outputFile, 
             ms=ms, 
             seldonian_solutions_found=seldonian_solutions_found,
             seldonian_fs=seldonian_fs, 
             seldonian_failures_g1=seldonian_failures_g1, 
             seldonian_failures_g2=seldonian_failures_g2,
             LS_solutions_found=LS_solutions_found,
             LS_fs=LS_fs,
             LS_failures_g1=LS_failures_g1,
             LS_failures_g2=LS_failures_g2)
```

```{python, eval = FALSE, cache = TRUE, message = FALSE, warning = FALSE}
# Create the behavioral constraints
gHats  = [gHat1, gHat2]
deltas = [0.1, 0.1]

# Initialize one worker because we're not using parallelization
nWorkers = 1  

# sample sizes
ms   = [2**i for i in range(5, 17)]  
numM = len(ms)
    
# The number of trials
numTrials = 100  

mTest = ms[-1] * 100 # about 5,000,000 test samples

# Run experiments sequentially without parallelization
tic = timeit.default_timer()
for worker_id in range(1, nWorkers + 1):
    run_experiments(worker_id, nWorkers, ms, numM, numTrials, mTest)
toc = timeit.default_timer()
time_sequential = toc - tic # Elapsed time in seconds
```

\noindent Finally, the following code chunks compile the results from the experiments and presents them visually. `csv` implements classes to read and write tabular data in CSV format. `glob` finds all the path names matching a specified pattern according to the rules used by the Unix shell. This will be useful for referencing file paths and names. `re` provides regular expression matching operations similar to those found in Perl.

```{python, eval = FALSE}
#import necessary packages
import csv 
import glob 
import re 
import matplotlib.pyplot as plt 
```

```{python, eval = FALSE}
#specify path names
bin_path = 
'/home/dasienga24/Statistics-Senior-Honors-Thesis/Thesis/index/experiment_results/'
'chapter_2/'
csv_path = 
'/home/dasienga24/Statistics-Senior-Honors-Thesis/Thesis/index/experiment_results/'
'chapter_2/csv/'
```

```{python, eval = FALSE}
#parse through files to obtain the experiment numbers
def get_existing_experiment_numbers():
    result_files       = glob.glob(bin_path + 'results*.npz')
    experiment_numbers = [re.search('.*results([0-9]*).*', 
    fn, re.IGNORECASE) for fn in result_files]
    experiment_numbers = [int(i.group(1)) for i in experiment_numbers]
    experiment_numbers.sort()
    return experiment_numbers
```

```{python, eval = FALSE}
#generate the file names for the results
def genFilename(n):
    return bin_path + 'results%d.npz' % n
```

\noindent For the `addMoreResults` function, recall that: 


- `ms`: data set size
- `seldonian_solutions_found`: stores whether a solution was found (1=True,0=False)
- `seldonian_fs`: stores the primary objective values (fHat) if a solution was found
- `seldonian_failures_g1`: stores whether Seldonian solution was unsafe, (1=True,0=False), for the 1st constraint, g_1
- `seldonian_failures_g2`: stores whether Seldonian solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2
- `LS_solutions_found`: stores whether a solution was found. These will all be true (=1)
- `LS_fs`: stores the primary objective values (f) 
- `LS_failures_g1`: stores whether LS solution was unsafe, (1=True,0=False), for the 1st constraint, g_1
- `LS_failures_g2`: stores whether LS solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2

```{python, eval = FALSE}
def addMoreResults(newFileId, ms, seldonian_solutions_found, seldonian_fs,
seldonian_failures_g1, seldonian_failures_g2, LS_solutions_found, LS_fs, 
LS_failures_g1, LS_failures_g2):

    newFile = np.load(genFilename(newFileId))
    new_ms                        = newFile['ms']
    new_seldonian_solutions_found = newFile['seldonian_solutions_found']
    new_seldonian_fs              = newFile['seldonian_fs']
    new_seldonian_failures_g1     = newFile['seldonian_failures_g1']
    new_seldonian_failures_g2     = newFile['seldonian_failures_g2']
    new_LS_solutions_found        = newFile['LS_solutions_found']
    new_LS_fs                     = newFile['LS_fs']
    new_LS_failures_g1            = newFile['LS_failures_g1']
    new_LS_failures_g2            = newFile['LS_failures_g2']

    if type(ms)==type(None):
        return [new_ms, new_seldonian_solutions_found, new_seldonian_fs,
      new_seldonian_failures_g1, new_seldonian_failures_g2,
      new_LS_solutions_found, new_LS_fs, new_LS_failures_g1, 
      new_LS_failures_g2]
    else:
        seldonian_solutions_found  = 
        np.vstack([seldonian_solutions_found, new_seldonian_solutions_found])
        seldonian_fs               = 
        np.vstack([seldonian_fs,              new_seldonian_fs])
        seldonian_failures_g1      = 
        np.vstack([seldonian_failures_g1,     new_seldonian_failures_g1])
        seldonian_failures_g2      = 
        np.vstack([seldonian_failures_g2,     new_seldonian_failures_g2])
        LS_solutions_found         = 
        np.vstack([LS_solutions_found,        new_LS_solutions_found])
        LS_fs                      = 
        np.vstack([LS_fs,                     new_LS_fs])
        LS_failures_g1             = 
        np.vstack([LS_failures_g1,            new_LS_failures_g1])
        LS_failures_g2             = 
        np.vstack([LS_failures_g2,            new_LS_failures_g2])

        return [ms, seldonian_solutions_found, seldonian_fs, seldonian_failures_g1,
      seldonian_failures_g2, LS_solutions_found, LS_fs, LS_failures_g1, LS_failures_g2]

```

```{python, eval = FALSE}
def stderror(v):
    non_nan = np.count_nonzero(~np.isnan(v))        
    return np.nanstd(v, ddof=1) / np.sqrt(non_nan)
```

\noindent The output CSV file will have columns corresponding to:


1. `m` -- the size of the data set
2. QSA mean value
3. QSA standard error bar size
4. LS mean value
5. LS standard error bar size 

\noindent There will be one column per value of `m` (amount of training data). 

```{python, eval = FALSE}
def saveToCSV(ms, resultsQSA, resultsLS, filename):
    nCols = resultsQSA.shape[1]


    with open(filename, mode='w') as file:
        writer = csv.writer(file, delimiter=',')

        for col in range(nCols):

            cur_m          = ms[col]
            seldonian_data = resultsQSA[:,col]
            LS_data        = resultsLS[:,col]

            non_nan = np.count_nonzero(~np.isnan(seldonian_data))
            if non_nan > 0:
                seldonian_mean     = np.nanmean(seldonian_data)
                seldonian_stderror = stderror(seldonian_data)
            else:
                seldonian_mean     = 'NaN'
                seldonian_stderror = 'NaN'

            LS_mean     = np.mean(LS_data)
            LS_stderror = stderror(LS_data)

            writer.writerow([cur_m, seldonian_mean, seldonian_stderror, 
            LS_mean, LS_stderror])
```

```{python, eval = FALSE}
#gather the results and compile into CSV file
def gather_results():
    ms                        = None
    seldonian_solutions_found = None
    seldonian_fs              = None
    seldonian_failures_g1     = None
    seldonian_failures_g2     = None
    LS_solutions_found        = None
    LS_fs                     = None
    LS_failures_g1            = None
    LS_failures_g2            = None

    experiment_numbers = get_existing_experiment_numbers()

    for file_idx in experiment_numbers:
        res = addMoreResults(file_idx, 
            ms, 
            seldonian_solutions_found, 
            seldonian_fs, seldonian_failures_g1, seldonian_failures_g2, 
            LS_solutions_found, LS_fs, LS_failures_g1, LS_failures_g2)
        
        [ms, 
        seldonian_solutions_found, seldonian_fs, seldonian_failures_g1,
        seldonian_failures_g2, LS_solutions_found, LS_fs, LS_failures_g1, 
        LS_failures_g2] = res


    saveToCSV(ms,  
    -1*seldonian_fs,           
    -1*LS_fs,           
    csv_path+'fs.csv') # here, negative to return MSE rather than negative MSE
    
    saveToCSV(ms,  
    seldonian_solutions_found,  
    LS_solutions_found, 
    csv_path+'solutions_found.csv')
    
    saveToCSV(ms,  
    seldonian_failures_g1,      
    LS_failures_g1,     
    csv_path+'failures_g1.csv')
    
    saveToCSV(ms,  
    seldonian_failures_g2,      
    LS_failures_g2,     
    csv_path+'failures_g2.csv')

```


```{python, eval = FALSE}
csv_path = 
'/home/dasienga24/Statistics-Senior-Honors-Thesis/Thesis/index/'
'experiment_results/chapter_2/csv/'
img_path = 
'/home/dasienga24/Statistics-Senior-Honors-Thesis/Thesis/index/'
'experiment_results/chapter_2/images/'
```

\noindent: Finally, the results are plotted as shown below.

```{python, eval = FALSE}
def loadAndPlotResults(fileName, ylabel, output_file, is_yAxis_prob, legend_loc):
    file_ms, file_QSA, file_QSA_stderror, file_LS, 
    file_LS_stderror = np.loadtxt(fileName, delimiter=',', unpack=True)

    fig = plt.figure()

    plt.xlim(min(file_ms), max(file_ms))
    plt.xlabel("Amount of data (m)", fontsize=12)
    plt.xscale('log')
    plt.xticks(fontsize=12)
    plt.ylabel(ylabel, fontsize=12)

    if is_yAxis_prob:
        plt.ylim(-0.1, 1.1)
    else:
        plt.ylim(-0.2, 2.2)
        plt.plot([1, 100000], [1.25, 1.25], ':k');
        plt.plot([1, 100000], [2.1,  2.1],  ':k');		

    plt.plot(     file_ms,     file_QSA, 'b-', linewidth=3, label='QSA')
    plt.errorbar( file_ms,     file_QSA, yerr=file_QSA_stderror, fmt='.k');
    plt.plot(     file_ms,     file_LS,  'r-', linewidth=3, label='LS')
    plt.errorbar( file_ms,     file_LS,  yerr=file_LS_stderror, fmt='.k');
    plt.legend(loc=legend_loc, fontsize=12)
    plt.tight_layout()

    plt.savefig(output_file)
    plt.show(block=False)
```

```{python, eval = FALSE, message = FALSE, warning = FALSE}
gather_results()
```

```{python, fig.align='center', fig.cap="QSA Experiment: Performance Loss", warning = FALSE, message = FALSE, fig.width = 5.25, fig.height = 3.5, eval = FALSE}
loadAndPlotResults(csv_path+'fs.csv', 
'Mean Squared Error', 
img_path+'tutorial7MSE_py.png', 
False, 
'lower right')
```

```{python, fig.align='center', fig.cap="QSA Experiment: Probability of a Solution", warning = FALSE, message = FALSE, fig.width = 5.25, fig.height = 3.5, eval = FALSE}
loadAndPlotResults(csv_path+'solutions_found.csv', 
'Probability of Solution',   
img_path+'tutorial7PrSoln_py.png',  
True,  
'best')
```

```{python, fig.align='center', fig.cap="QSA Experiment: Satisfaction of 1st Behavioral Constraint", warning = FALSE, message = FALSE, fig.width = 5.25, fig.height = 3.5, eval = FALSE}
loadAndPlotResults(csv_path+'failures_g1.csv',     
r'Probability of $g_1(a(D))>0$', 
img_path+'tutorial7PrFail1_py.png', 
True,  
'best')
```

```{python, fig.align='center', fig.cap="QSA Experiment: Satisfaction of 2nd Behavioral Constraint", warning = FALSE, message = FALSE, fig.width = 5.25, fig.height = 3.5, eval = FALSE}
loadAndPlotResults(csv_path+'failures_g2.csv',     
r'Probability of $g_2(a(D))>0$', 
img_path+'tutorial7PrFail2_py.png', 
True,  
'best')
```



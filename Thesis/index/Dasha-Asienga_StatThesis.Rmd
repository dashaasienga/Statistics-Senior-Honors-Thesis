---
title: 'Algorithmic Bias, Statistical Notions of Fairness, and the Seldonian Algorithm'
author: 'Dasha Asienga'
date: 'April DD, 20YY'
institution: 'Amherst College'
advisor: 'Professor Katharine Correia'
# Change department to'Special Programs' if interdisciplinary thesis
department: 'Mathematics and Statistics'
degree: 'Bachelor of Arts'
knit: "bookdown::render_book"
output: 
  acthesis::thesis_pdf: default
# include table of contents, list of tables, list of figures
toc: true
lot: true # set to false if no tables present
lof: true # set to false if no figures present
fig_height: 3.5
fig_width: 5.5
# Update abstract and acknowledgments filenames below as needed
abstract: |
  `r if(knitr:::is_latex_output()) 
    paste(readLines("00a-abstract.Rmd"),
    collapse = '\n  ')`
acknowledgments: |
  `r if(knitr:::is_latex_output()) 
    paste(readLines("00b-acknowledgments.Rmd"), 
    collapse = '\n  ')`
# Update the .bib filename below as needed
bibliography: bib/thesis.bib
csl: csl/apa.csl
# Uncomment the following to include any additional LaTeX packages
# header-includes:
# - \usepackage{latexPackageName}
---

<!-----------------------------------------------------------------------------
IMPORTANT REMINDERS!!

(1) Before you knit the document, make sure to update your _bookdown.yml file with:
    - your knitted pdf name (book_filename: "FirstName-LastName_StatThesis")
    - your filenames in order (rmd_files: ["thisfilename.Rmd", "firstChapter.Rmd", ..., "99-references.Rmd"])

(2) You can rename any of the chapter files, including this index file, as long as you update the filenames in _bookdown.yml

(3) Knit this file to knit the entire thesis document, or knit within a single chapter file to only knit that chapter (all other chapters will be knit with just a placeholder sentence). You will likely run into issues when knitting a single chapter if that chapter depends on code from previous chapters.  

(4) No thesis code or text should go in this index file except that used to set up the remainder of the document.
------------------------------------------------------------------------------->


<!--
The acthesis package does not need to be loaded for your thesis (but DOES need to be installed on your machine). It's loaded here as a quick way to load the packages needed to knit this example document. 

You can delete the following code chunk from your final document if you wish.
-->
```{r include_acthesis, include = FALSE}
# acthesis package loads dpylr, ggplot2, knitr, bookdown, and remotes
require(acthesis)
```


<!--
R has some built-in options that may be useful. Some recommended values are set below. Feel free to play around with the options You may edit or delete this code chunk if you wish.

- width: max width of R output (default: based on console width)
- digits: number of significant digits to print (default: 7)
-->
```{r set_options, include = FALSE}
# Default width of R ouput
options(width = 65, digits = 3)
```


<!--
This is a clunky workaround to wrap super long R output. This code chunk defines a new code chunk option called linewidth that allows you to wrap R output that falls into the margins or off the page.

The default linewidth for R output in the thesis template is 65, set with options(width = 65). If the linewidth option is still needed for a specific code chunk despite this setting, we recommend using linewidth = 65 for consistency.
-->
```{r wrap-hook, include = FALSE}
# Define a new code chunk option 'linewidth' for wrapping unruly R output
hook_output = knitr::knit_hooks$get('output')
knitr::knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n, exdent = 3, indent = 3)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
# Note: set linewidth = 65 when needed.
```


<!--
The following code downloads the latest Amherst thesis cover sheet if it doesn't already exist in your folder. You may delete this code if you wish.

Amherst Thesis Guidelines are provided at https://www.amherst.edu/academiclife/registrar/for-students/thesis_guide
-->
```{r get_coversheet, include = FALSE}
# download latest thesis cover sheet
if(!file.exists("CoverSheet.pdf")){
  download.file("https://www.amherst.edu/system/files/media/Thesis%2520Copyright.pdf", "CoverSheet.pdf")
}
```



<!-- 
If knitting to HTML, the following code is required to number equations in HTML files. You may delete the following code from your final document if you wish, but it should not affect your work.
-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<!--chapter:end:index.Rmd-->

# Introduction {#intro}
<!--
Each chapter file must start with a chapter header (e.g., # Chapter name). Notice we have also included a label after the chapter name ({# intro}) so we can refer back to this chapter later in the text if want to (e.g., by using \@ref(intro)). This also helps you debug potential issues as you knit.

You can knit chapters all at once using the index file, or individually. When you knit a single chapter, every other chapter will have a one-sentence placeholder to make knitting easier.

You can re-name each chapter file (e.g., change this filename from "01-chap1.Rmd" to "01-introduction.Rmd"), just make sure you also update the filename in the list of rmd files in _bookdown.yml.
-->

  The public and private sector are increasingly turning to data-driven methods to automate and to guide simple and complex decision-making. However, this trend raises an important question of bias. There is a lot of misinterpretation when it comes to the collection of data in many application areas, and there is a major concern for data-driven methods to further introduce and perpetuate discriminatory practices, or to otherwise be unfair because of the social and historical processes that operate to the disadvantage of certain groups. 

For example, within healthcare, using mortality or readmission rates to measure hospital performance penalizes hospitals serving poor or non-White populations as those inherently have higher mortality and readmission rates due to confounding societal factors. Outside healthcare, credit-scoring algorithms predict outcomes based on income, which disadvantages low-income groups further perpetuating economic immobility. Policing algorithms result in increased scrutiny of Black neighborhoods because of the bias against Black people that is already present in the U.S. policing system, and hiring algorithms, which predict employment decisions, are affected by historical race and gender biases. 

Yet, these algorithms are often regarded as ground truth and free of human limitations because they are based on mathematics, statistics, and computer science – otherwise regarded as objective disciplines. In theory, this should lead to greater fairness. However, left unregulated, these mathematical models privilege majority groups and discriminate against minority groups because they often learn from inherently biased data. If the data used to train models contains bias, then the resulting algorithms will learn the bias and reflect it into their predictions. In many cases, this can be detrimental.  

While there are widely-accepted, though sometimes disputed, societal notions of fairness, one key question emerges: are there any established statistical notions of fairness and bias? Is it possible to mathematically and statistically define algorithmic bias and unfairness, thereby paving a way for addressing the challenges they pose? And if so, are there ways leverage statistical tools to resolve such bias and unfairness? This thesis paper aims to explore and answer precisely these questions. 

## Algorithmic Bias 

  There are multiple different types and sources of bias in the realm of statistics. In particular, algorithmic bias arises when an algorithm’s decisions are skewed towards a particular group of people, either positively or negatively [@mehrabi2021survey]. The danger with biased algorithmic outcomes is that they generate a feedback loop. Take, for example, a hiring algorithm that discriminates against female applicants for a specific job. In the long run, this can perpetuate, and even amplify, existing gender biases by further widening the gender-based class imbalance. 

One such key example of  algorithmic bias often cited in literature is regarding the broad use of the COMPAS – or the Correctional Offender Management Profiling for Alternative Sanctions – tool to predict a defendant’s risk of recidivism (committing another crime) within two years. COMPAS is more likely to have higher false positive rates for African-American offenders than Caucasian offenders [@mehrabi2021survey]. Across the country, scores of similar assessments are given to judges, which injects bias into courts [@angwin2016machine].   

COMPAS is based on data from 7000 people arrested in Broward County, Florida in 2013 and 2014 [@angwin2016machine]. The response variable, recidivism, was encoded based on who was charged with new crimes over the next two years. Analyses on the predictive efficacy of the COMPAS algorithm found that the algorithm was 61% accurate for a full range of crimes, including misdemeanors, and only 20% of people forecasted to commit violent crimes actually went on to do so. While the overall accuracy rate for the full range of crimes is better than a coin flip, there exists room for enhancing the predictive performance, especially for a decision as critical as whether or not to grant a defendant bail or parole.

What's more concerning, however, is that when the effects of race, age, and gender are isolated and not included in the model, a statistical analysis showed that Black defendants were still 77% more likely to be predicted at higher risk of committing a future violent crime and 45% more likely to be predicted of committing a future crime of any kind, highlighting the role that proxies of race play into the predictions [@angwin2016machine]. The table in Figure \@ref(fig:compas1) highlights the performance discrepancy across race. 

```{r compas1, fig.cap="Prediction Fails Differently for Black v White Defendants (Angwin et al., 2016)", out.width = '100%', echo=FALSE, fig.scap="COMPAS Prediction Fails Differently for Black v White Defendants", fig.align='center'}
include_graphics(path = "figures/compas1.png")
```

Although the tool has 61% accuracy, Black defendants are almost twice as likely to be labeled as higher risk without re-offending than White defendants. It makes the opposite mistake among White defendants. The reason for this is that classification models are trained to minimize average error, which fits majority populations [@chouldechova2018frontiers]. 

For example, different factors in society lead to different environmental and social experiences between Black people and White people. These factors, such as an offender's personal history, familial history, and residential neighborhood, are used in the COMPAS tool to predict recidivism. It would, thus, seem fair that the relationships between an offender's social experience and their likelihood of recidivism be calibrated differently for Black versus White offenders to account for this inherent difference. A classifier that does not take this into account will disproportionately affect one group negatively. 

COMPAS is just one such algorithm. In the education sector, different factors lead to different SAT scores between majority and minority populations. It would, thus, seem fair that the relationship between SAT and college admissions be calibrated differently for each demographic group. However, if a group-blind classifier is trained, it cannot simultaneously fit both groups optimally and will fit the majority population as it's more important to overall error.  

Can we modify these algorithms to be group-blind but also fair? In order to do so, fairness constraints that reduce, or even correct for, algorithmic bias during the modeling process must be set. However, one must first define fairness mathematically, statistically, and quantifiably.  

## Statistical Definitions of Fairness {#fairnessdefinitions}

Statistical notions of fairness can be defined at a group level or an individual level. *Group notions* fix a few demographic groups and assess the parity of some statistical measures across all the groups [@chouldechova2018frontiers]. Note that group measures, on their own, do not guarantee fairness to individuals or structured subgroups within protected demographic groups, but rather, give guarantees to "average" numbers of protected groups. These notions are the focus of this thesis paper.

*Individual notions*, on the other hand, are assessed on specific pairs of individuals rather than averaged across groups [@chouldechova2018frontiers]. In other words, similar individuals should be treated similarly along some defined similarity or inverse distance metrics. Counter-factual fairness, for example, relies on the intuition that a decision is fair towards an individual if it's the same in both the real world and a counter-factual world where the individual belongs to a different demographic group [@mehrabi2021survey]. This can be impractical, relies on strong assumptions about the data, and approaches the realm of causality [@chouldechova2018frontiers]. Moreover, there is a gap in literature with regard to individual notions of fairness. 

Ultimately, group notions and individual notions are not in conflict per se. Instead, they are on the same spectrum of how much dependence is allowed between predictions and the sensitive attribute [@castelnovo2022clarification]. Subgroup fairness is an alternative notion that intends to obtain the best properties of both, for example, by picking a group fairness constraint and assessing whether it holds over a large collection of subgroups [@mehrabi2021survey]. Group and individual fairness notions can be defined in both classification settings and regression settings, although most of the literature focuses on fairness within classification.

### Group Fairness in Regression Settings 

Fair regression is the quantitative notion of fairness of real-valued targets [@agarwal2019fair]. Consider a general prediction setting where the training set consists of $X$, a feature vector with all the predictor variables, $A$, the levels of the protected attribute/ demographic group, and $Y$, the real-valued continuous response variable. $F$ is a set of possible prediction models, and the goal is to find $f \in F$ that is a good predictive model of $Y$ given $X$ and some fairness constraints. The accuracy of a prediction $f(X)$ / $\hat{Y}$ on $Y$ is measured by the loss function $l(Y, f(X))$ or mean squared error (MSE). The goal is to minimize $l(Y, f(X))$, hence, maximizing accuracy.

*Statistical parity* refers to minimizing the expected loss function/ MSE such that the probability that each predicted  $f(X)$/ $\hat{Y}$ is above a certain threshold $z$ for each sensitive attribute is the same as the probability over the entire data set, given some margin $\epsilon_a$ that is dependent on the protected attribute [@agarwal2019fair]: 

\begin{equation}
\label{ch1eq1}
\text{min}_{f \in F} \text{ } E[l(Y, f(X))] \text{ such that } \forall a \in A, z \in [0,1]:
\end{equation}

$$ |P[f(X) \geq z | A = a] - P[f(X) \geq z]| \leq \epsilon_a.$$

This is akin to the classification setting where it may be desirable to have the probability of being in the positive class be above some certain threshold for each group as well as across the entire data set. A similar notion, known as *bounded loss*, requires that the MSE for each group is below some pre-specified level $c_a$ that is dependent on the protected attribute [@agarwal2019fair]:

\begin{equation}
\label{ch1eq2}
\text{min}_{f \in F} \text{ } E[l(Y, f(X))] \text{ such that } \forall a \in A: 
\end{equation}

$$ E[l(Y, f(X)) | A = a] \leq c_a.$$

### Group Fairness in Classification Settings 

Group notions of fairness in classification, at the core, refer to treating different groups equally. They aim to remedy or prevent disparate impact, which is a setting where there is unintended disproportionate adverse impact on a particular group [@chouldechova2017fair]. There are three broad notions of observational group fairness: independence, separation, and sufficiency [@castelnovo2022clarification]. 

#### Independence

\newcommand{\indep}{\perp \!\!\! \perp}

This fairness definition requires predictions, $\hat{Y}$, to be independent of any sensitive attribute, $A$, that is, $\hat{Y} \indep A$ [@castelnovo2022clarification]. Thus, it relies only on the distribution of features and decisions, that is, $A$, $X$, and $\hat{Y}$, and focuses on the equality of the predictions themselves by satisfying the following equation:

\begin{equation}
\label{ch1eq3}
P (\hat{Y} = 1 | A = a) = P (\hat{Y} = 1 | A = b), \text{    } \forall a, b \in A,
\end{equation}

where $a$, $b$ are the two demographic groups in question.

This definition is also known as *demographic parity*, *statistical parity*, or generally, group fairness, and requires that all levels of the demographic group have the same positive prediction ratio (PPR) where PPR is the ratio of positive outcomes [@castelnovo2022clarification]. In other words, the likelihood of a positive outcome should be the same regardless of the demographic group.

In the COMPAS data set, independence would be satisfied if the probability of recidivism is the same for both Black and White defendants in the data set. That is, the probability that a Black defendant is predicted to recommit a crime within the next two years should be the same as the probability that a White defendant is predicted to recommit a crime. 

The visual example in Figure \@ref(fig:dp) illustrates a toy scenario where independence is met [@durahly2023fairness]. 

```{r dp, fig.cap="An Example of Demographic Parity (Durahly, 2023)", out.width = '100%', echo=FALSE, fig.scap="An Example of Demographic Parity"}
include_graphics(path = "figures/dp.png")
```

The dashed line represents the decision boundary. In both group A and group B, four out of the eight participants were predicted to repay a loan. The other half of the participants were predicted to default. Notice, however, that the class imbalance in this toy credit lending example results in a higher error rate within group B than group A.

A difference in demographic parity close to 0 or a ratio close to 1 by some defined margin is considered a fair solution [@castelnovo2022clarification]. To achieve demographic parity, the different demographic groups must be treated differently, which may seem contrary to societal pre-conceived notions of fairness. Therefore, demographic parity should be used when the primary objective is to enforce some form of equality between groups regardless of all other information and when the objectivity of the target variable, $Y$, is under question, perhaps because of historical biases. This, however, can unknowingly amplify biases if used in the wrong setting. For example, when imposing demographic parity on a hiring algorithm, if qualifications are different across a protected attribute, then less-qualified candidates may be hired. If these candidates end up being low-performers, then this can perpetuate stereotypes about their demographic group.  

In the above example of using a hiring algorithm with gender as the protected attribute, it may then seem fairer to require independence on gender only for men and women with the same rating or qualification, that is, $\hat{Y} \indep A | R$. This is known as *conditional demographic parity* and requires that the following equation is satisfied [@castelnovo2022clarification]: 

\begin{equation}
\label{ch1eq4}
P (\hat{Y} = 1 | A = a, R = r) = P (\hat{Y} = 1 | A = b, R = r), \text{    } \forall a, b \in A, \forall r.
\end{equation}


This idea can be generalized more to condition on all attributes, that is, $\hat{Y} \indep A |  X$. As this is more generalized, however, it begins to satisfy individual fairness and can be achieved by a gender-blind model [@castelnovo2022clarification]. This type of individual fairness is also referred to as fairness through unawareness (FTU), which requires that any protected attributes, or their covariates, are not explicitly used in the decision-making process [@mehrabi2021survey]. This definition of fairness requires that the following equation be satisfied [@castelnovo2022clarification]: 

\begin{equation}
\label{ch1eq5}
P (\hat{Y} = 1 | A = a, X = x) = P (\hat{Y} = 1 | A = b, X = x), \text{ } \forall a, b \in A, \forall x \in X.
\end{equation}


#### Separation

Independence does not make use of the true target $Y$ and simply requires equality of predictions. However, as observed in Figure \@ref(fig:dp), this can lead to different error rates between different groups. In other words, the model is more accurate for one group than it is for another group. Separation precisely focuses on equality of the error rates and is widely known as the *equality of odds* [@castelnovo2022clarification]. This definition requires the same type I and type II error rates, precisely, the same false positive rate (FPR) and false negative rate (FNR) across all demographic groups. FPR and FNR are defined by:

\begin{equation}
\label{ch1eq6}
FPR = P(\hat{Y} = 1| Y = 0) = \frac{FP}{FP + TN}
\end{equation}

\begin{equation}
\label{ch1eq7}
FNR = P(\hat{Y} = 0| Y = 1) = \frac{FN}{TP + FN}
\end{equation}

where FP refers to false positive predictions, TP refers to true positive predictions, FN refers to false negative predictions, and TN refers to true negative predictions. These metrics can be understood through a confusion matrix as in Figure \@ref(fig:confusionmatrix) [@mohajon2021confmatrix].

```{r confusionmatrix, fig.cap="A Confusion Matrix (Mohajon, 2021)", out.width = '75%', echo=FALSE, fig.scap="A Confusion Matrix"}
include_graphics(path = "figures/confusionmatrix.png")
```

In the COMPAS data set, separation would be satisfied if both Black defendants and White defendants had equal error rates. However, as observed in Figure \@ref(fig:compas1), Black defendants had an FPR of 45% while White defendants had an FPR of 24% -- these refer to the percentage of times the algorithm predicted the defendants had recidivated when they hadn't. Similarly, Black defendants had an FNR of 28% while White defendants had an FNR of 48% -- these refer to the percentage of times the algorithm predicted the defendants had not recommitted a crime when they had.

Separation requires independence of the predictions $\hat{Y}$ and the sensitive attribute $A$ conditioned on the true value of the target variable $Y$, that is, $\hat{Y} \indep A|Y$ [@castelnovo2022clarification]. In other terms, the following equation must be satisfied:

\begin{equation}
\label{ch1eq8}
P(\hat{Y} = 1 | A = a, Y = y) = P(\hat{Y} = 1 | A = b, Y = y), \text{ } \forall a,b \in A, \text{ } y \in \{ 0, 1 \},
\end{equation}

where 0 is a negative outcome and 1 is a positive outcome. This is a reasonable fairness metric, as long as the objectivity of the target variable is trusted, as it ensures that the model optimizes performance for all groups, not just majority groups. 

The visual example in Figure \@ref(fig:eoo) illustrates a toy scenario where separation is met [@castelnovo2022clarification]. The dashed line represents the decision boundary. Filled in circles represent positive predictions and empty circles represent negative predictions. The error rates are consistent between both men and women.

```{r eoo, fig.cap="An Example of Equality of Odds (Castelnovo et al., 2022)", out.width = '60%', echo=FALSE, fig.scap="An Example of Equality of Odds"}
include_graphics(path = "figures/eoo.png")
```

There are two relaxed versions of this measure depending on which outcome is most important to predict [@castelnovo2022clarification]:

i) *Predictive equality*: equality of false positive rates (FPR) across groups:

\begin{equation}
\label{ch1eq9}
P (\hat{Y} = 1 | A = a, Y = 0) = P (\hat{Y} = 1 | A = b, Y = 0), \text{ } \forall a,b \in A.
\end{equation}


ii) *Equality of Opportunity*: equality of false negative rates (FNR) across groups:

\begin{equation}
\label{ch1eq10}
P (\hat{Y} = 0 | A = a, Y = 1) = P (\hat{Y} = 0 | A = b, Y = 1), \text{ } \forall a,b \in A.
\end{equation}


#### Sufficiency 

Finally, sufficiency takes the perspective of people that receive the same model prediction and requires parity among them regardless of sensitive features [@castelnovo2022clarification]. This is also knows as *predictive parity* and requires that the precision be the same across sensitive groups, that is, $Y \indep A | \hat{Y}$. In other words, the following equation must be satisfied:

\begin{equation}
\label{ch1eq11}
P (Y = y | A = a, \hat{Y} = y) = P (Y = y | A = b, \hat{Y} = y), \text{ } \forall a, b \in A, \text{ for } y \in \{0,1\}.
\end{equation}

Simply put, the probability of a positive outcome given a positive prediction, and that of a negative outcome given a negative prediction, should be equal across all sensitive groups.

$\\$

Consistent with this line of reasoning, many fairness metrics can be defined. This begs the fundamental question: can multiple definitions be simultaneously enforced?

## Fairness Conflicts

Because of the way different fairness definitions are defined, it can be impossible to simultaneously enforce multiple definitions and unexpected behavior may result from a particular definition of fairness. This section highlights some conflicts that arise both in the regression and classification setting. 

### Fairness Conflicts in Regression

The UFRGS Entrance Exam and GPA Data contains entrance exam scores of students applying to the Federal University of Rio Grande do Sul in Brazil, along with the students' GPAs during their first three semesters at the university [@DVN/O35FW8_2019]. Each student's score in nine different entrance exams is used to predict their GPA during their first 3 semesters of study at the university. Gender and race are the protected attributes.  

Taking gender as the protected attribute in a gender-blind model, independence, in this setting, would require that the average predictions be the same for each gender. That is, 

\begin{equation}
\label{ch1eq12}
E[\hat{Y} | G = Male] = E[\hat{Y} | G = Female].
\end{equation}

This is violated if, on average, a model predicts a higher or lower GPA based on gender.

Separation, on the other hand, would require that the average error of predictions be the same for each gender. In defined notion, 

\begin{equation}
\label{ch1eq13}
E[\hat{Y} - Y | G = Male] = E[\hat{Y} - Y| G = Female].
\end{equation}

This is violated if, on average, the model over-predicts for one gender but under-predicts for another gender or the model either over-predicts or under-predicts more or less for one gender. 

However, a study found that because male and female applicants had different GPAs in the original data set, these two fairness definitions cannot be simultaneously satisfied [@thomas2020housetestimony]. A result in the Section \@ref(class-conflict) will explain this in a mathematically tractable way.  

### Fairness Conflicts in Classification {#class-conflict}

Define prevalence $p$ as the probability of a positive outcome given the demographic group [@chouldechova2017fair]. It directly relates to the class distribution of the outcome. $p \in (0,1)$ and can be denoted by:

\begin{equation}
\label{ch1eq14}
p_a = P(Y=1|A=a).
\end{equation}

Further define the positive predictive value (PPV) of a prediction as the probability of a positive outcome given a positive prediction [@chouldechova2017fair]:

\begin{equation}
\label{ch1eq15}
PPV(\hat{Y}|A = a) \equiv P (Y = 1| \hat{Y} = 1, A = a).
\end{equation}

Similarly, the negative predictive value (NPV) of a prediction is the probability of a negative outcome given a negative prediction and can be denoted as:

\begin{equation}
\label{ch1eq16}
NPV(\hat{Y}|A = a) \equiv P (Y = 0| \hat{Y} = 0, A = a).
\end{equation}

Sufficiency would require equal PPV and equal NPV across the different demographic groups. Note that NPV and PPV can be computed from a confusion matrix (Figure \@ref(fig:confusionmatrix)) as shown below [@saeed2015evidence]:

\begin{equation}
\label{ch1eq17}
PPV = \frac{TP}{TP + FP} \text{   ;   } NPV = \frac{TN}{TN + FN}.
\end{equation}

Now, given values of the $PPV \in (0,1)$ and $p \in (0,1)$, it can be shown that [@chouldechova2017fair]:

$$ FPR = \frac{p}{1-p} \frac{1-PPV}{PPV}(1 - FNR).$$
Appendix \@ref(appendix-a) provides the details for the derivation of this equation. However, its direct implication is that if the prevalence differs between two groups, then it is impossible to satisfy sufficiency (equal PPV across all groups) and separation (equal FPR and FNR across all groups) simultaneously. For example, in the COMPAS data set, if recidivism rates differ between Black and White offenders, then an algorithm that guarantees predictive parity/ equal precision for both Black and White offenders cannot also guarantee equality of odds. Indeed, the recidivism rate for Black defendants in the data is 51%, compared to 39% for White defendants, and hence, the disparate impact of the COMPAS tool as observed in Figure \@ref(fig:compas1) [@chouldechova2017fair].

Figure \@ref(fig:dp) illustrates a similar conflict between independence and separation. Satisfying independence resulted in an imbalance of error rates between group A and group B because of the difference in the prevalence of loan repayment between both groups.

$\\$ 

Unfortunately, the distribution of the outcome of interest often differs for different demographic groups, posing the all-important question: how can fairness be achieved in the face of this conflict?

### On Fairness Conflicts

As observed, disparate impact can result from the use of a prediction tool that is perceived to be free from predictive bias. Just because an algorithm satisfies a particular definition of fairness doesn't infer that the algorithm is *fair* in every sense of that word. Balancing overall error rates alone is not enough as it does not produce models that are free from bias or that guarantee fairness at finer levels of granularity. This highlights the need for human value and domain expertise in defining fairness within the context of a particular problem before the fairness constraints can be set. Once that is done, Chapter \@ref(chap-2) introduces a framework for setting these constraints. 


<!--chapter:end:01-chap1.Rmd-->


```{r load_packages, include = FALSE}
library(mosaic)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
```

# Seldonian Algorithms {#chap-2}

Chapter \@ref(intro) introduced the problem of algorithmic bias, discussed existing statistical definitions of fairness both in regression and classification settings, and finally, highlighted  fairness conflicts that can arise in certain settings. Of important note is that there are a plethora of fairness definitions that have been developed in statistical machine learning, many of which have been shown to be incompatible in ways similar to the illustration in Appendix \@ref(appendix-a). In any effort to enforce fairness on machine learning models, a critical first step is to define what fairness means in the specific context [@thomas2020housetestimony]. This responsibility falls on domain experts, social scientists, and regulators. Once there is consensus on that, machine learning researchers can work to develop appropriate algorithms that enforce the chosen definition of fairness. The Seldonian framework, introduced in this chapter, offers one such way to place probabilistic fairness constraints on traditional algorithms. However, because Seldonian algorithms place constraints on traditional machine learning (ML) algorithms, an initial in-depth understanding of the standard approach is key. Section \@ref(standardml) discusses the typical ML approach before diving into the Seldonian framework later in Section \@ref(seldonian). 

## The Standard Machine Learning Approach {#standardml}

When designing a machine learning algorithm, the first step is to mathematically define what the algorithm should do, in other words, the goal of the algorithm [@thomas2019supplementary]. At an abstract level, this goal is identical for all machine learning problems: find a solution $\theta^*$, within some feasible set $\Theta$, that maximizes some objective function $f: \Theta \rightarrow \textbf{R}$, where $\textbf{R}$ is the set of real numbers. Precisely, the goal of the algorithm is to search for an optimal solution  

\begin{equation}
\label{ch2eq1}
\theta^* \in \underset{\theta \in \Theta}{\text{ arg max }} f(\theta).
\end{equation}

For example, let $X$ and $Y$ be dependent real-valued random variables in a regression setting with the goal of estimating $Y$ given $X$. In this setting, $\Theta$ is the set of feasible functions that model the relationship between $X$ and $Y$. Feasible functions are of the form $\theta(X) = \beta_0 + \beta_1X = \hat{Y}$. Each function $\theta \in \Theta$ takes a real number as input and produces a real number as output; therefore, $\theta : \textbf{R} \rightarrow \textbf{R}$. A reasonable objective function would then be the negative mean squared error (MSE):

\begin{equation}
\label{ch2eq2}
f(\theta):=-E[(\theta(X) - Y)^2].
\end{equation}

In this case, minimizing MSE is equivalent to maximizing -MSE, defining the goal of the regression algorithm as finding the solution with the least average error. Note that the true value of $f(\theta)$ is unknown and can only be estimated from the data [@thomas2019preventing]. For a sample with $n$ observations, that is, $(x_i, y_i) \text{ for } i = 1,2,...,n$, the objective function can be estimated by:

\begin{equation}
\label{ch2eq3}
\hat{f(\theta)}= -\frac{1}{n} \sum_{i=1}^{n}(\theta(x_i) - y_i)^2.
\end{equation}

However, defining objective functions in this way can sometimes lead to undesirable behavior as illustrated in Section \@ref(standardlimitations).

### Limitations of the Standard Approach {#standardlimitations}

Consider a linear regression example to predict the qualifications of job applicants based on information on their resumes. Let $G$ encode the gender of each applicant, with $G=0$ if the applicant is female and $G=1$ if the applicant is male. Let $X$ encode a summary measure of an applicant's qualification based on information on their resume -- a simple example would be a measure of how many job-relevant key words appear on their resume. Let $Y$ encode their actual qualification for the job as determined by their observed performance. 

If this linear regression estimator is designed to be used to filter which resumes submitted to a company will be forwarded for human review, it is worthwhile to ensure that the algorithm does not produce racist or sexist behavior. Drawing from definitions in Chapter \@ref(fairnessdefinitions), it might be less important to ensure that the algorithm, on average, has the same predictions for applicants of both genders because the distribution of qualifications may be different for both genders. However, of more concern is whether the algorithm, on average, predicts too high for one gender and too low for the other gender. 

Suppose that the data has the following distribution: $Y \sim N(1,1)$ if $G = 0$ and $Y \sim N(-1,1)$ if $G = 1$, that is, $Y$ is a normal variable $N(\mu, \sigma)$ with different means $\mu$ for different genders but with the same standard deviation $\sigma$ for both genders. Further define $X \sim N(Y,1)$, that is, an applicant's resume quality is equal to their true qualification plus some random noise. Figure \@ref(fig:fig1) displays a scatterplot of 1000 such data points, 500 from each gender. The black solid line is the least squares fit on this data using a gender-blind model. 


```{r fig1, fig.align='center', fig.cap="Least Squares Fit on Synthetic Data Drawn from Different Distributions", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, echo = FALSE}
# set seed for reproducibility 
set.seed(123)

# generate synthetic data for both male and female applicants
female <- rnorm(500, mean = 1, sd = 1)
male <- rnorm(500, mean = -1, sd = 1)
gender <- c(rep(0,500), rep(1,500))

# create the data set
y <- union(female, male)
x <- rnorm(1000, mean = y, sd = 1)
data <- cbind(y, x, gender)
data <- data %>%
  as.data.frame()

# create custom color scale 
myColors <- brewer.pal(2,"Set1")
names(myColors) <- levels(data$gender)

# fit least squares line
ggplot(data = data, mapping = aes(x = x, y = y)) +
  geom_point(aes(color = as.factor(gender)), alpha = 0.8) +
  geom_smooth(
    method = "lm",
    aes(color = as.factor(gender)),
    se = F,
    size = 2
  ) +
  geom_smooth(
    method = "lm",
    formula = y ~ x,
    color = 'black',
    se = F
  ) +
  scale_colour_manual(name = "Gender (G)",values = myColors) +
  labs(color = "Gender (G)",
       title = "Least Squares Fit on Synthetic Data") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) 
```

```{r, echo = FALSE}
# set seed for reproducibility
set.seed(123)

# obtain the least squares linear model
model <- lm(y ~ x, data = data)

# obtain the model predictions 
preds <- predict(model, newdata = data)

# add the error into the data set
data <- data %>%
  mutate(error = preds - data$y)

# compute average error per gender 
diff <- data %>%
  group_by(gender) %>%
  summarise(average = mean(error)) 

# obtain the discrimination statistic
d <- diff$average[1] - diff$average[2]
```

The least squares fit on Figure \@ref(fig:fig1) is impartial to an observation's gender with an objective to make the most accurate predictions. While it may be expected that impartiality would produce fair results, observe that the linear model tends to over-predict if $G = 1$ and under-predict if $G=0$, on the contrary, producing discriminatory behavior. In fact, by defining a discrimination statistic, $d(\theta)$, as defined below in line with the specified fairness definition, the discrimination statistic for the synthetic data set in Figure \@ref(fig:fig1) can be shown to be `r d`, suggesting that the model predictions are in favor of $G = 1$:
 
\begin{equation}
\label{ch2eq4}
d(\theta) =  E[\hat{Y}-Y|G = 0 ] - E[\hat{Y}-Y|G = 1 ]. 
\end{equation}


In crucial applications such as hiring, this is concerning and highlights how linear regression algorithms designed using the standard approach can result in predictions of applicant performance that systematically discriminate against a demographic group.


### Potential Remedies

As illustrated in Section \@ref(standardlimitations) above, machine learning algorithms that use the standard approach may produce undesirable behavior. In an attempt to remedy this problem, a number of approaches can be taken. One potential remedy is to identify the root cause of the undesirable behavior such as class imbalance in the training data, bias in the data set, the choice of linear estimator, the model's blindness to the demographic group, or insufficient data, to name a few [@thomas2019supplementary]. For instance, in the example set up in Section \@ref(standardlimitations) and displayed in Figure \@ref(fig:fig1), the root cause of the discriminatory behavior when using ordinary least squares linear regression was the fact that the objective function was designed to minimize MSE, which was at odds with minimizing the discrimination statistic. However, even though it might be possible to determine and correct the root cause of the undesirable behavior, doing so can be difficult, error-prone, and require extensive data analysis, rendering the central goal of machine learning algorithms, which are designed to automate and make decision-making processes simpler, obsolete.

Assuming that the problem is with the objective function and provided that detailed knowledge of the problem is available, hard constraints may be placed on the objective function, for example, requiring that MSE is minimized only on the set of solutions with a discrimination value $d(\theta)$ less than some value $\epsilon$ [@thomas2019supplementary]. Additionally, rather than placing hard constraints on the set of solutions, soft constraints that penalize undesirable behavior may also be placed on $f$, the objective function [@boyd2004convex]. Although such penalty functions can be effective, they require a careful choice of the value of the parameter $\lambda$ that places relative importance on the objective function and the constraint. For the linear regression example, the new objective function with a soft constraint would now be:

\begin{equation}
\label{ch2eq5}
f(\theta) = - MSE (\theta) - \lambda d(\theta).
\end{equation}

Observe that as $\lambda$ increases, MSE increases and the discrimination statistic decreases. Cross-validation techniques can be employed to find optimal values for $\lambda$. Other remedies include maximizing multiple objective functions or allowing constraints on the probability that a solution with undesirable behavior will be returned, both of which may require detailed knowledge of the application problem and underlying distribution of the data [@thomas2019supplementary]. 

In principle, there might be definitions of $\Theta$ or $f$ that prevent the algorithm from converging on solutions that exhibit undesirable behavior [@thomas2019preventing]. However, in practice and as explained, this might require extensive domain expertise and data analysis in order to properly balance the relative importance of the objective function and the constraints, which can be at odds with each other. These techniques may also require knowledge of the probability distribution from which the data is sampled, which is not always available and limits applications to parametric statistics.

A Seldonian algorithm, thus, addresses this problem precisely by allowing probabilistic constraints on undesirable behavior to be placed more easily without detailed knowledge of the specific problem or the distribution of the data, shifting the burden from the domain experts who use these tools to the experts in ML and statistics [@thomas2019preventing]. It's named after Isaac Asimov’s fictional character, Hari Seldon ^[In the fictional book, Hari Seldon was a resident of a fictional planet where he develops psycho-history, an algorithmic science that allows him to predict the future in probabilistic terms.] [@asimov1994forward]. 

## The Seldonian Framework {#seldonian}

The first step of the Seldonian framework is to define mathematically the goal of the algorithm design [@thomas2019supplementary]. Define $\textbf{D}$ as the set of all possible inputs (data sets) to the algorithm. $\Theta$, as previously defined, is the set of all possible outputs (solutions) of the algorithm. Each solution is referred to as $\theta \in \Theta$. $D$ is the data set (input) given to the algorithm and is the only random variable. Now, $a: \textbf{D} \rightarrow \Theta$ is a machine learning algorithm which takes in a data set $D \in \textbf{D}$ as an input and returns a solution $\theta \in \Theta$ as an output. $\textbf{A}$ is the set of all possible machine learning algorithms. Synthesizing this, $f: \textbf{A} \rightarrow \textbf{R}$ is the objective function of the algorithm design, where $f(a) \in \textbf{R}$ is a real-valued measure of the utility of the algorithm, such as the value of the objective function for the solution returned by this algorithm. This objective function is optimized -- either minimized or maximized -- to select a desired machine learning algorithm from the set $\textbf{A}$.

Contrary to the standard ML approach, however, $n$ behavioral constraints can then be specified [@thomas2019supplementary]. Specifically, $(g_i, \delta_i)_{i=1}^{n}$ can be defined as a set of $n$ constraints, each of which contains a constraint function $g_i: \Theta \rightarrow \textbf{R}$ and a desired confidence level $\delta_i$. The constraint function takes in a solution returned from the chosen machine learning algorithm as an input and returns a real value encoding the "fairness" of the algorithm according to the fairness definition defined by the function. $(g_i, \delta_i)_{i=1}^{n}$ is defined such that:

- The $i^{th}$ constraint function measures an undesirable behavior. Specifically, $\theta \in \Theta$ produces undesirable behavior if and only if $g_i(\theta) > 0$. This is to ensure that undesirable behavior is defined in a mathematically tractable way such as how the discrimination statistic $d(\theta)$ was defined in Section \@ref(standardlimitations).

- The $i^{th}$ confidence level specifies the maximum probability that an algorithm can return a solution $\theta$ where $g_i(\theta) > 0$. In other words, $1 - \delta_i$ specifies the minimum probability that desirable behavior ($g_i(\theta) \leq 0$) is met. Smaller values of $\delta_i$ are preferred. 

In summary, a Seldonian algorithm ensures that for all $i \in \{1,2,\ldots,n\}$:

\begin{equation}
\label{ch2eq6}
P(g_i(a(D)) \leq 0) \geq 1 - \delta_i.
\end{equation}

Section \@ref(sop) goes into further detail about the Seldonian framework and how these probabilistic behavioral constraints are guaranteed.

### The Seldonian Optimization Problem {#sop}

As detailed, the Seldonian framework is different from current potential remedies of undesirable behavior because it defines a search over a possible set of algorithms with constraints, rather than over a possible set of solutions. This means that the constraints require that the probability that a machine learning algorithm returns an unsafe solution be bounded by some desired level of confidence, rather than the probability that a solution itself is unsafe. In summary, a Seldonian optimization problem (SOP) can be written as [@thomas2019preventing]:


\begin{equation}
\label{ch2eq7}
\underset{a \in \textbf{A}}{\text{ arg max }} f(a)
\text{ s.t. } \forall \text{ } i \in \{1, 2, \ldots, n\} \text{, } P(g_i(a(D)) \leq 0) \geq 1 - \delta_i.
\end{equation}


A Seldonian algorithm $a$, thus, returns, with high probability, a solution that guarantees desirable behavior. If one were to apply machine algorithm $a$ to obtain a solution from a large number of different data sets $D$ drawn from the same distribution, then it would be expected that at most $100\delta_i \%$ solutions (models) would produce undesirable behavior. 

Taking the previous regression example and turning it into a Seldonian optimization problem using the discrimination statistic in Section \@ref(standardlimitations), $f$ would still be an objective function like the MSE, $\Theta$ would still be the set of all possible linear models, and $D$ would be the data set as described. There would be 1 behavioral constraint, $g_1(a(D)) = |d(a(D))| - \epsilon$, to guarantee with probability at least $1-\delta_1$, that the absolute value of the discrimination statistic would be at most $\epsilon$, where $\epsilon$ and $\delta_1$ are chosen based on the specific application. Note that the user of the machine learning algorithm need not perform data analysis to determine whether $g_1(\theta) \leq 0$ for a particular solution $\theta \in \Theta$ returned. The burden is shifted to the computation algorithm to determine this with some desired level of probability. 

```{r seldonian1, fig.cap="Overview of the Seldonian Framework (P. S. Thomas et al., 2019a)", out.width = '100%', echo=FALSE, fig.scap="Overview of the Seldonian Framework"}
include_graphics(path = "figures/seldonian1.png")
```

Figure \@ref(fig:seldonian1) illustrates how this is achieved at a high level. A Seldonian algorithm takes in $n$ behavioral constraints $(g_i,\delta_i)_{i=1}^n$ and a data set $D$ as the inputs and returns either a solution (model) $\theta$ or $NSF$, which means "No Solution Found". That is, no algorithm was found that returned a model which satisfied the behavioral constraints with the desired probability. First, the data $D$ is partitioned into 2 sets $D_1$ and $D_2$ that essentially serve as train and test sets, respectively. $D_1$ is then passed through the candidate selection mechanism, which performs a search over algorithms to settle on a candidate solution $\theta_c$. $\theta_c$ is selected not only so that it optimizes the primary objective function $f$, but also so that it is predicted to pass the subsequent safety test. $D_2$ is then passed through the safety test to check whether $\theta_c$ indeed satisfies the $n$ behavioral constraints with the desired confidence for each, that is $P(g_i(\theta_c) \leq 0) \geq 1 - \delta_i$ for each constraint $i \in \{ 1,2, \ldots, n \}$. If so, $\theta_c$ is returned as the desired solution, and otherwise, NSF [@thomas2019preventing]. 

Note that finding exact confidence bounds may be impractical and require large amounts of data. Quasi-Seldonian algorithms, thus, are an extension of this idea that rely on standard statistical tools to transform sample statistics computed from $D$ into approximate bounds on the probability of undesirable behavior [@thomas2019preventing]. Section \@ref(qsa) discusses the statistical framework employed to achieve this. 

### Quasi-Seldonian Algorithms {#qsa}

Recall that the Seldonian goal is to create an algorithm $a$ that maximizes an objective function $f$ while satisfying behavioral constraints with high probability. In this Chapter, the focus has been on linear regression, so the aim would be to create a Seldonian linear regression algorithm, rather than one that follows the standard ML process described in Section \@ref(standardml). A Seldonian linear regression algorithm would be an approximate solution to:

\begin{equation}
\label{ch2eq8}
\underset{a \in \textbf{A}}{\text{ arg max }} f(a)
\text{ s.t. } \forall \text{ } i \in \{1, 2, \ldots, n\} \text{, } P(g_i(a(D)) \leq 0) \geq 1 - \delta_i.
\end{equation}


Because the value of $f(a)$, the objective function, is unknown, it'll be estimated from the data provided such that $\hat{f}: \Theta \text{ x } \textbf{D} \rightarrow \textbf{R}$ is a measure of the utility of the algorithm that returns a solution $\theta$ when given input $D$, the only random variable [@thomas2019preventing]. In this case, the MSE for a data set of size $m$ can be estimated by

\begin{equation}
\label{ch2eq9}
\hat{f}(\theta, D) = -\frac{1}{m} \sum_{i=1}^{m}(\hat{y}(X_i, \theta) - Y_i)^2.
\end{equation}

However, the optimization of this objective function is constrained to solutions that have probabilistic guarantees of safe behavior. As observed in Figure \@ref(fig:seldonian1), the candidate selection and safety test mechanisms are crucial to achieve this. 

#### The Safety Test Mechanism {#safety}

Seldonian algorithms ensure that $P(g_i(\theta_c) \leq 0) \geq 1 - \delta_i$ for each constraint $i \in \{ 1,2, \ldots, n \}$ and the safety test mechanism is the component that verifies whether these behavioral constraints actually hold [@thomas2019preventing]. This is achieved by computing an upper bound for each $g_i(\theta)$ using the data and a confidence interval derived from the Student $t$-statistic. If the high confidence upper bound is less than or equal to 0, then the solution is safe to return, otherwise, no solution will be returned. 

Let $X = (X_1, \ldots, X_m)$ be $m$ independent and identically distributed ($i.i.d.$) random variables. Under the assumption that $\frac{1}{m} \sum_{i=1}^m X_i$ is normally distributed or if $m$ is sufficiently large -- by the Central Limit Theorem --, then the Student $t$-statistic can be used to compute an upper bound of the expected value of these random variables as follows:

\begin{equation}
\label{ch2eq10}
P(E[X_1] \leq \hat{\mu}(X) + \frac{\hat{\sigma}(X)}{\sqrt{m}}t_{1-\delta, m-1}) \geq 1 - \delta,
\end{equation}

where

- $\hat{\mu}(X) = \bar{X}$ and $\hat{\sigma}(X) = s$ are the sample mean and standard deviation, respectively, of a vector X. That is, $\hat{\mu}(X) = \frac{1}{m}\sum_{i=1}^m X_i = \bar{X}$ and $\hat{\sigma}(X) = \sqrt{\frac{\frac{1}{m}\sum_{i=1}^m (X_i - \hat{\mu}(X))^2}{m-1}} = s$. 

- $t_{1-\delta, m-1}$ is the $100(1-\delta)$ percentile of the Student $t$-distribution with $m-1$ degrees of freedom.

Before constructing the safety test mechanism, recall from Section \@ref(sop) that [@thomas2019supplementary]:

- The safety test will be applied to a single solution $\theta_c$ selected by the candidate selection mechanism. This process is explained in the following section.  

- The safety data, $D_2$, is used to verify that the behavioral constraints hold. 

- $\hat{g_i}(\theta_c, D_2) = (\hat{g_{i,1}}(\theta_c, D_2), \ldots,\hat{g_{i,m}}(\theta_c, D_2))$ contains $m$ $i.i.d$ values of $\hat{g_i}(\theta_c)$ for each of the $m$ observations in $D_2$. $|D_2|$ will be used to denote the number of observations in $D_2$ for consistency in notation. 

- $E[\hat{g_i}(\theta_c, D_2)] = g_i(\theta_c)$.

By substituting the respective pieces into the Student $t$ high confidence upper bound discussed above, then:

\begin{equation}
\label{ch2eq11}
P(g_i(\theta_c) \leq \hat{\mu}(\hat{g_i}(\theta_c, D_2)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_2))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1}) \geq 1 - \delta_i.
\end{equation}


Notice that $\hat{\mu}(\hat{g_i}(\theta_c, D_2)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_2))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1}$ is an upper bound of the confidence interval with confidence $1-\delta_i$. If this upper bound is less than or equal to zero, then the $i^{th}$ behavioral constraint $g_i(\theta_c)$ is less than or equal to zero with at least probability $1-\delta_i$. Therefore, $\theta_c$ is only returned if $\hat{\mu}(\hat{g_i}(\theta_c, D_2)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_2))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1} \leq 0$. Specifically, this holds only under the assumption that $\hat{\mu}(\hat{g_i}(\theta_c, D_2))$ is normally distributed or if the size of the safety data $D_2$ is sufficiently large, hence the name $\textit{quasi}$-Seldonian [@thomas2019preventing]. The next section now discusses precisely how $\theta_c$ is selected before being passed into the safety test mechanism. 

#### The Candidate Selection Mechanism {#candidate}

With the safety test in place, any algorithm will be Seldonian regardless of how $\theta_c$ is computed, as long as $\theta_c$ is computed using a different subset of the data, hence the partition into $D_1$ (candidate data) and $D_2$ (safety data) [@thomas2019supplementary]. However, if $\theta_c$ is computed using the standard ML approach, then it will likely be unsafe as was illustrated in Section \@ref(standardlimitations), resulting in an NSF output. Instead, $\theta_c$ will be computed as follows:

\begin{equation}
\label{ch2eq12}
\theta_c \in \underset{\theta \in \Theta}{\text{ arg max }} \hat{f}(\theta, D_1)
\text{ s.t. } \theta_c \text{ is predicted to pass the safety test}.
\end{equation}


Thus, only solutions likely to pass the safety test will be considered by predicting the result of the safety test using $D_1$ (the candidate data) instead of $D_2$. In formal notation,

\begin{equation}
\label{ch2eq13}
\theta_c \in \underset{\theta \in \Theta}{\text{ arg max }} \hat{f}(\theta, D_1)
\text{ s.t. } \forall \text{ } i \in \{1,2,\ldots,n\}, \text{  } \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1} \leq 0.
\end{equation}


Notice that while the sample mean $\hat{\mu}$ and the sample standard deviation $\hat{\sigma}$ are computed over $D_1$, the size of $D_2$ is still used to correct the standard deviation and compute the Student $t$ percentile, in order to ensure that the solution is properly predicted to pass the safety test. 

The process defined can work well when the objective function $f$ and the behavioral constraints are aligned. However, when they are in conflict, the candidate selection mechanism tends to be over-confident that $\theta_c$ will pass the safety test and a safe solution will be returned [@thomas2019supplementary]. Doubling the width of the confidence level is a proposed solution to produce more conservative predictions and better guarantees of $\theta_c$ passing the safety test. Therefore, a black-box optimization algorithm is used to compute

\begin{equation}
\label{ch2eq14}
\theta_c \in \underset{\theta \in \Theta}{\text{ arg max }} \hat{f}(\theta, D_1)
\text{ s.t. } \forall \text{ } i \in \{1,2,\ldots,n\}, \text{  } \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + 2\frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1} \leq 0.
\end{equation}


\

This culminates the discussion on the Seldonian theoretical framework at a high level. To conclude this Chapter, Section \@ref(toy) walks through how this is implemented computationally using a toy regression example. 

## Toy Example: A Seldonian Regression Algorithm {#toy}

The tutorial in this section follows the presentation by @aisafety on the AI Safety webpage focusing on the key computational aspects of the Seldonian Algorithm. Consistent with the linear regression set-up in this chapter, consider $X, Y \in \textbf{R}$ as two dependent random variables with the goal of estimating $Y$ given $X$ through a sample of $m$ observations. $X$ is drawn synthetically from a $N(0,1)$ distribution and $Y$ is dependent on $X$ with a $N(X,1)$ distribution. Figure \@ref(fig:fig2) displays 10000 such points. $\hat{y}(X, \theta) = \theta_1X+\theta_2$ and $MSE = E[(\hat{y}(X, \theta)-Y)^2]$ are computed as previously defined.

```{r fig2, fig.align='center', fig.cap="Synthetic Data for Seldonian Linear Regression Tutorial", warning = FALSE, message = FALSE, fig.width = 4.5, fig.height = 3, echo = FALSE}
# set seed for reproducibility 
set.seed(123)

# generate synthetic data 
x <- rnorm(10000, mean = 0, sd = 1)
y <- rnorm(10000, mean = x, sd = 1)

# create the data set
seldonian_data <- cbind(y, x)
seldonian_data <- seldonian_data %>%
  as.data.frame()

# plot the data points
ggplot(data = seldonian_data, mapping = aes(x = x, y = y)) +
  geom_point(color = "lightblue") +
  labs(title = "Synthetic Data for Seldonian Linear Regression",
       x = "X",
       y = "Y") +
  theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) 
```

In this example, the goal of the linear regression algorithm is two-fold: to minimize MSE (or equivalently, maximize -MSE) while ensuring, with probability at least 0.9, that $1.25 < MSE < 2$. Note that this behavioral constraint is not practical in an application setting, but it's deliberately designed to be at odds with the objective function in order to test behavior when such conflict arises. Additionally, it'll be simple to verify satisfaction of the behavioral constraint for demonstration purposes. The behavioral constraint needs to be mathematically represented in a way that defines $g(\theta) \leq 0$ as safe behavior. Therefore, $n$ will be set to $2$ such that:

- $g_1(\theta) = MSE(\theta) - 2.0 \text{; } \delta_1 = 0.1$.

- $g_2(\theta) = 1.25 - MSE(\theta) \text{; } \delta_2 = 0.1$.

Unbiased estimates of the MSE and each $g_i(\theta)$ will be computed from the data set as elucidated in Section \@ref(qsa).


```{r, echo = FALSE}
library(reticulate)
use_python("/cm/shared/apps/amh-Rstudio/python-3.11.4/bin/python3", required = TRUE)
#py_config()
#conda_list()
```

```{r, echo = FALSE}
sklearn <- import("sklearn")
#conda_list()
```

```{r, include = FALSE}
py_config()
```

```{python, echo = FALSE}
import sklearn
```

```{python, echo = FALSE}
import math
import numpy as np
import sys
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from scipy.stats import t
from scipy.optimize import minimize
```

```{python, echo = FALSE}
np.set_printoptions(precision=5, suppress=True)
```

```{python, echo = FALSE}
def tinv(p, nu):
    return t.ppf(p, nu)
```

```{python, include = FALSE}
tinv(0.95,50)
```

```{python, echo = FALSE}
def stddev(v):
    n = v.size
    variance = (np.var(v) * n) / (n-1) 
    return np.sqrt(variance) 
```

```{python, echo = FALSE}
def ttestUpperBound(v, delta):
    n  = v.size
    res = v.mean() + stddev(v) / math.sqrt(n) * tinv(1.0 - delta, n - 1)
    return res
```

```{python, echo = FALSE}
def predictTTestUpperBound(v, delta, k):
    
    res = v.mean() + 2.0 * stddev(v) / math.sqrt(k) * tinv(1.0 - delta, k - 1)
    return res
```

```{python, echo = FALSE}
def main():
    np.random.seed(0)  
    numPoints = 5000   

    (X,Y)  = generateData(numPoints)  

    gHats  = [gHat1, gHat2] 
    deltas = [0.1, 0.1]

    (result, found) = QSA(X, Y, gHats, deltas) 
    
    if found:
        print("A solution was found: [%.10f, %.10f]" % (result[0], result[1]))
        print("fHat of solution (computed over all data, D):", fHat(result, X, Y))
    else:
        print("No solution found")
```

```{python, echo = FALSE}
def generateData(numPoints):
    X =     np.random.normal(0.0, 1.0, numPoints) 
    Y = X + np.random.normal(0.0, 1.0, numPoints) 
    return (X,Y)
```

```{python, echo = FALSE}
def predict(theta, x):
    return theta[0] + theta[1] * x
```

```{python, echo = FALSE}
def fHat(theta, X, Y):
    n = X.size          
    res = 0.0           
    for i in range(n):  
        prediction = predict(theta, X[i])                
        res += (prediction - Y[i]) * (prediction - Y[i]) 
    res /= n            
    return -res         
```

```{python, echo = FALSE}
def gHat1(theta, X, Y):
    n = X.size          
    res = np.zeros(n)   
    for i in range(n):
        prediction = predict(theta, X[i])                   
        res[i] = (prediction - Y[i]) * (prediction - Y[i])  
    res = res - 2.0     
    return res

def gHat2(theta, X, Y):
    n = X.size          
    res = np.zeros(n)   
    for i in range(n):
        prediction = predict(theta, X[i])                   
        res[i] = (prediction - Y[i]) * (prediction - Y[i])  
    res = 1.25 - res   
    return res
```

```{python, echo = FALSE}
def leastSq(X, Y):
    X = np.expand_dims(X, axis=1) 
    Y = np.expand_dims(Y, axis=1) 
    reg = LinearRegression().fit(X, Y)
    theta0 = reg.intercept_[0]   
    theta1 = reg.coef_[0][0]     
    return np.array([theta0, theta1])
```

```{python, echo = FALSE}
def QSA(X, Y, gHats, deltas):

    candidateData_len = 0.40
    candidateData_X, safetyData_X, candidateData_Y, safetyData_Y = train_test_split(
      X, Y, test_size=1-candidateData_len, shuffle=False)
  
    candidateSolution = getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyData_X.size)

    passedSafety      = safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas)

    return [candidateSolution, passedSafety]
```

```{python, echo = FALSE}
def safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas):

    for i in range(len(gHats)):  
        g         = gHats[i]  
        delta     = deltas[i] 

    
        g_samples = g(candidateSolution, safetyData_X, safetyData_Y) 

        upperBound = ttestUpperBound(g_samples, delta) 

        if upperBound > 0.0: 
            return False

    return True
```

```{python, echo = FALSE}
def candidateObjective(thetaToEvaluate, candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize): 

    result = fHat(thetaToEvaluate, candidateData_X, candidateData_Y)

    predictSafetyTest = True     
    
    for i in range(len(gHats)):  
        g         = gHats[i]       
        delta     = deltas[i]      

        g_samples = g(thetaToEvaluate, candidateData_X, candidateData_Y)

        upperBound = predictTTestUpperBound(g_samples, delta, safetyDataSize)

        if upperBound > 0.0:

            if predictSafetyTest:
                predictSafetyTest = False  

                result = -100000.0    

            result = result - upperBound

    return -result  
```

```{python, echo = FALSE}
def getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize):
  
    minimizer_method = 'Powell'
    minimizer_options={'disp': False}

    initialSolution = leastSq(candidateData_X, candidateData_Y)

    res = minimize(candidateObjective, x0=initialSolution, method=minimizer_method, options=minimizer_options, 
    args=(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize))

    return res.x
```

```{python, echo = FALSE}
main()
```


---- 


\noindent What's left in this section is displaying the results from applying the Seldonian regression as well as the experiment results. I have all these already done in the Jupyter notebooks. I can transfer the code from the Jupyter notebooks to this file so that everything is reproducible (and hopefully that all runs without error). 

Once that is figured out, do you think I should also put key Python code in a technical appendix? Let me know what you think about this!


\




- I also need to review my notes to fill in any missing pieces for the complete version of the Chapter. 




<!--chapter:end:02-chap2.Rmd-->

# Seldonian Algorithms for Classification {#chap-3}




<!--chapter:end:03-chap3.Rmd-->

`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 
<!--
The first appendix must start with the above text. Do not remove!
-->

# Sufficiency v Separation Fairness Conflict Equation {#appendix-a}

Recall from Chapter \@ref(intro) that [@castelnovo2022clarification]:

$$ PPV = P(Y=1|\hat{Y} = 1),$$
$$ FPR = P(\hat{Y} = 1| Y = 0),$$
$$FNR = P(\hat{Y} = 0| Y = 1),$$
$$ p = P(Y=1).$$
Using Bayes' rule,

$$PPV = P(Y=1|\hat{Y} = 1) = \frac{P(\hat{Y} = 1|Y=1)P(Y=1)}{P(\hat{Y} = 1|Y=1)P(Y=1) + P(\hat{Y} = 1|Y=0)P(Y=0)} $$
$$\Rightarrow PPV = \frac{P(\hat{Y} = 1|Y=1)p}{P(\hat{Y} = 1|Y=1)p + P(\hat{Y} = 1|Y=0)(1-p)}$$

$$\Rightarrow PPV = \frac{(1-FNR)p}{(1-FNR)p + FPR(1-p)}$$

$$\Rightarrow (1-FNR)p + FPR(1-p) = \frac{(1-FNR)p}{PPV}$$
$$\Rightarrow FPR(1-p) = \frac{(1-FNR)p}{PPV} - (1-FNR)p$$
$$\Rightarrow FPR = \frac{(1-FNR)p}{PPV(1-p)} - \frac{(1-FNR)p}{(1-p)}$$
$$\Rightarrow FPR = \frac{p}{1-p} \left[  \frac{(1-FNR)}{PPV} - (1-FNR) \right]$$
$$\Rightarrow FPR = \frac{p}{1-p} \left[  \frac{(1-FNR) - PPV(1-FNR)}{PPV} \right]$$
$$\Rightarrow FPR = \frac{p}{1-p} \left[  \frac{(1-FNR) (1 - PPV)}{PPV} \right]$$
$$\Rightarrow FPR = \frac{p}{1-p} \frac{1 - PPV}{PPV} (1-FNR) \blacksquare.$$

A similar equation can be derived relating $NPV = P(Y=0|\hat{Y} = 0)$ and both FPR and FNR.

Additionally, in conventional statistics notation, the sensitivity of a prediction tool can be defined as $P(\hat{Y}=1|Y=1) = 1 - FNR$ and its specificity can be defined as $P(\hat{Y}=0|Y=0) = 1 - FPR$. Given a prevalence $p$, sensitivity $s_e$, and specificity $s_p$, then:

$$PPV = \frac{s_ep}{s_ep + (1-s_p)(1-p)}.$$

Similarly, it can shown that: 

$$NPV = \frac{s_p(1-p)}{(1-s_e)p + s_p(1-p)}.$$

The code chunk below fixes arbitrary sensitivity (1 - FNR) and specificity (1 - FPR) values to illustrate through the proceeding plots that as prevalence varies, then PPV/ NPV varies and cannot be equal as long as sensitivity and specificity are held constant, hence a conflict. 

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(gridExtra)
```

```{r, fig.align='center'}
ppv <- function(p, sens, spec){
  ppv <- (sens*p)/((sens*p) + ((1-spec)*(1-p)))
  return(ppv)
}

npv <- function(p, sens, spec){
  npv <- (spec*(1-p))/(((1-sens)*p) + (spec*(1-p)))
  return(npv)
}

dat_8080 <- data.frame(prevalence = seq(0.05,0.95,0.05)
                       , sens=0.80
                       , spec=0.80
                       , ppv = ppv(p=seq(0.05,0.95,0.05), 
                                   sens=0.80, 
                                   spec=0.80)
                       , npv = npv(p=seq(0.05,0.95,0.05), 
                                   sens=0.80, 
                                   spec=0.80))

dat_9090 <- data.frame(prevalence = seq(0.05,0.95,0.05)
                       , sens=0.90
                       , spec=0.90
                       , ppv = ppv(p=seq(0.05,0.95,0.05), 
                                   sens=0.90, 
                                   spec=0.90)
                       , npv = npv(p=seq(0.05,0.95,0.05), 
                                   sens=0.90, 
                                   spec=0.90))

dat_9070 <- data.frame(prevalence = seq(0.05,0.95,0.05)
                       , sens=0.90
                       , spec=0.70
                       , ppv = ppv(p=seq(0.05,0.95,0.05), 
                                   sens=0.90, 
                                   spec=0.70)
                       , npv = npv(p=seq(0.05,0.95,0.05), 
                                   sens=0.90, 
                                   spec=0.70))

dat_7090 <- data.frame(prevalence = seq(0.05,0.95,0.05)
                       , sens=0.70
                       , spec=0.90
                       , ppv = ppv(p=seq(0.05,0.95,0.05), 
                                   sens=0.70, 
                                   spec=0.90)
                       , npv = npv(p=seq(0.05,0.95,0.05), 
                                   sens=0.70, 
                                   spec=0.90))

dat_all <- bind_rows(dat_8080, dat_7090, dat_9070, dat_9090) |>
  mutate(sens_spec = paste0("Sensitivity: ", sens, 
                            "\n Specificity: ", spec)
         , fpr = 1 - spec
         , fnr = 1 - sens)

g1 <- ggplot(dat_all, aes(x=prevalence, y=ppv)) +
        geom_point() + 
        labs(x="prevalence", y="positive predictive value",
             title = "PPV-FPR-FNR Conflict") +
        facet_wrap(~sens_spec)

g2 <- ggplot(dat_all, aes(x=prevalence, y=npv)) +
        geom_point() + 
        labs(x="prevalence", y="negative predictive value",
             title = "NPV-FPR-FNR Conflict") +
        facet_wrap(~sens_spec)

grid.arrange(g1,g2, nrow=1, ncol=2)
```




<!--chapter:end:06-appendixA.Rmd-->

# Key Python Code {#appendix-b}

Explain the code in this section using the Jupyter notebook as a guide: https://jupyter.hpc.amherst.edu/user/dasienga24/lab?

```{r, eval = FALSE }
library(reticulate)
use_python("/cm/shared/apps/amh-Rstudio/python-3.11.4/bin/python3", required = TRUE)
#py_config()
#conda_list()
```

```{r, eval = FALSE }
sklearn <- import("sklearn")
#conda_list()
```

```{r, include = FALSE}
py_config()
```

```{python, eval = FALSE }
import sklearn
```

```{python, eval = FALSE }
import math
import numpy as np
import sys
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from scipy.stats import t
from scipy.optimize import minimize
```

```{python, eval = FALSE }
np.set_printoptions(precision=5, suppress=True)
```

```{python, eval = FALSE }
def tinv(p, nu):
    return t.ppf(p, nu)
```

```{python, include = FALSE}
tinv(0.95,50)
```

```{python, eval = FALSE }
def stddev(v):
    n = v.size
    variance = (np.var(v) * n) / (n-1) 
    return np.sqrt(variance) 
```

```{python, eval = FALSE }
def ttestUpperBound(v, delta):
    n  = v.size
    res = v.mean() + stddev(v) / math.sqrt(n) * tinv(1.0 - delta, n - 1)
    return res
```

```{python, eval = FALSE }
def predictTTestUpperBound(v, delta, k):
    
    res = v.mean() + 2.0 * stddev(v) / math.sqrt(k) * tinv(1.0 - delta, k - 1)
    return res
```

```{python, eval = FALSE }
def main():
    np.random.seed(0)  
    numPoints = 5000   

    (X,Y)  = generateData(numPoints)  

    gHats  = [gHat1, gHat2] 
    deltas = [0.1, 0.1]

    (result, found) = QSA(X, Y, gHats, deltas) 
    
    if found:
        print("A solution was found: [%.10f, %.10f]" % (result[0], result[1]))
        print("fHat of solution (computed over all data, D):", fHat(result, X, Y))
    else:
        print("No solution found")
```

```{python, eval = FALSE }
def generateData(numPoints):
    X =     np.random.normal(0.0, 1.0, numPoints) 
    Y = X + np.random.normal(0.0, 1.0, numPoints) 
    return (X,Y)
```

```{python, eval = FALSE }
def predict(theta, x):
    return theta[0] + theta[1] * x
```

```{python, eval = FALSE }
def fHat(theta, X, Y):
    n = X.size          
    res = 0.0           
    for i in range(n):  
        prediction = predict(theta, X[i])                
        res += (prediction - Y[i]) * (prediction - Y[i]) 
    res /= n            
    return -res         
```

```{python, eval = FALSE }
def gHat1(theta, X, Y):
    n = X.size          
    res = np.zeros(n)   
    for i in range(n):
        prediction = predict(theta, X[i])                   
        res[i] = (prediction - Y[i]) * (prediction - Y[i])  
    res = res - 2.0     
    return res

def gHat2(theta, X, Y):
    n = X.size          
    res = np.zeros(n)   
    for i in range(n):
        prediction = predict(theta, X[i])                   
        res[i] = (prediction - Y[i]) * (prediction - Y[i])  
    res = 1.25 - res   
    return res
```

```{python, eval = FALSE }
def leastSq(X, Y):
    X = np.expand_dims(X, axis=1) 
    Y = np.expand_dims(Y, axis=1) 
    reg = LinearRegression().fit(X, Y)
    theta0 = reg.intercept_[0]   
    theta1 = reg.coef_[0][0]     
    return np.array([theta0, theta1])
```

```{python, eval = FALSE }
def QSA(X, Y, gHats, deltas):

    candidateData_len = 0.40
    candidateData_X, safetyData_X, candidateData_Y, safetyData_Y = train_test_split(
      X, Y, test_size=1-candidateData_len, shuffle=False)
  
    candidateSolution = getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyData_X.size)

    passedSafety      = safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas)

    return [candidateSolution, passedSafety]
```

```{python, eval = FALSE }
def safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas):

    for i in range(len(gHats)):  
        g         = gHats[i]  
        delta     = deltas[i] 

    
        g_samples = g(candidateSolution, safetyData_X, safetyData_Y) 

        upperBound = ttestUpperBound(g_samples, delta) 

        if upperBound > 0.0: 
            return False

    return True
```

```{python, eval = FALSE }
def candidateObjective(thetaToEvaluate, candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize): 

    result = fHat(thetaToEvaluate, candidateData_X, candidateData_Y)

    predictSafetyTest = True     
    
    for i in range(len(gHats)):  
        g         = gHats[i]       
        delta     = deltas[i]      

        g_samples = g(thetaToEvaluate, candidateData_X, candidateData_Y)

        upperBound = predictTTestUpperBound(g_samples, delta, safetyDataSize)

        if upperBound > 0.0:

            if predictSafetyTest:
                predictSafetyTest = False  

                result = -100000.0    

            result = result - upperBound

    return -result  
```

```{python, eval = FALSE }
def getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize):
  
    minimizer_method = 'Powell'
    minimizer_options={'disp': False}

    initialSolution = leastSq(candidateData_X, candidateData_Y)

    res = minimize(candidateObjective, x0=initialSolution, method=minimizer_method, options=minimizer_options, 
    args=(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize))

    return res.x
```

```{python, eval = FALSE }
main()
```


<!--chapter:end:07-appendixB.Rmd-->

<!--
This content below must go last in the thesis document according to how R Markdown renders.  More info is at http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html
-->

<!--
\backmatter indicates that the next (and final) chapter is the references
-->
\backmatter

<!-- 
You can change the name of the References chapter, but make sure to leave the {-} option to keep the chapter un-numbered.
-->
# References {-}

<!--
\noindent removes the indentation of the first entry.
-->
\noindent

<!--
The \setlength lines below create a hanging indent and spacing between entries.  These three lines may need to be removed for styles that don't require the hanging indent.
-->
\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}


<!--
This is just for testing with more citations for the bibliography at the end.  Add other entries into the list here if you'd like them to appear in the bibliography even if they weren't explicitly cited in the document.
-->
---
nocite: | 
...

<!--chapter:end:99-references.Rmd-->


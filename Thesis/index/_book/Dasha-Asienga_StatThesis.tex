%===========================================================
% This is the thesis template for the Statistics major at
% Amherst College. Brittney E. Bailey (bebailey@amherst.edu)
% adapted this template from the Reed College LaTeX thesis
% template in January 2019 with major updates in April 2020.
% Please send any comments/suggestions: bebailey@amherst.edu

% Most of the work for the original document class was done
% by Sam Noble (SN), as well as this template. Later comments
% etc. by Ben Salzberg (BTS). Additional restructuring and
% APA support by Jess Youngberg (JY). Email: cus@reed.edu
%===========================================================

\documentclass[12pt, twoside]{amherstthesis}
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs} %setspace loaded in .cls
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
\usepackage{rotating}
\usepackage{fancyvrb}
% User-added packages:
	\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
% End user-added packages

%===========================================================
% BIBLIOGRAPHY FORMATTING

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the
% following two lines to use the new biblatex-chicago style,
% for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


%===========================================================
% HYPERLINK FORMATTING

% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

%===========================================================
% CAPTION FORMATTING

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

%===========================================================
% TITLE FORMATTING

\renewcommand{\contentsname}{Table of Contents}

\usepackage{titlesec}
%%%%%%%%
% How to use titlesec:
% \titleformat{⟨command⟩}[⟨shape⟩]{⟨format⟩}{⟨label⟩}{⟨sep⟩}
%  {⟨before-code⟩}[⟨after-code⟩]
%%%%%%%%

\titleformat{\chapter}[hang]
{\normalfont%
    \Large% %change this size to your needs for the first line
    \bfseries}{\chaptertitlename\ \thechapter}{1em}{%
      %change this size to your needs for the second line
    }[]

\titleformat{\section}[hang]
{\normalfont%
    \large % %change this size to your needs for the first line
    \bfseries}{\thesection}{1em}{%
     %change this size to your needs for the second line
    }[]

\titleformat{\subsection}[hang]
{\normalfont%
    \normalsize % %change this size to your needs for the first line
    \bfseries}{\thesubsection}{1em}{%
     %change this size to your needs for the second line
    }[]

% \titleformat{\section}[display]
% {\normalfont%
%     \large% %change this size to your needs for the first line
%     \bfseries}{\chaptertitlename\ \thechapter}{20pt}{%
%     \normalsize %change this size to your needs for the second line
%     }


%===========================================================
% DOCUMENT FONT

% \usepackage{times}
% other fonts available eg: times, bookman, charter, palatino


%===========================================================
% PASSING FORMATS FROM RMD --> LATEX

%%%%%%%%
% NOTE: Dollar signs pass parameters between YAML inputs
% in index.Rmd and LaTeX
%%%%%%%%

\Abstract{
The abstract should be a short summary of your thesis work. A paragraph is usually sufficient here.
}

\Acknowledgments{
Use this space to thank those who have helped you in the thesis process (professors, staff, friends, family, etc.). If you had special funding to conduct your thesis work, that should be acknowledged here as well.
}

\Dedication{

}

\Preface{

}

% Formatting R code display
% Syntax highlighting #22
  \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

% Formatting R code: set baselinestretch = 1.5 for double-spacing
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{
  baselinestretch = 1,
  commandchars=\\\{\}}

% Formatting R output display: set baselinestretch = 1.5 for double-spacing
\DefineVerbatimEnvironment{verbatim}{Verbatim}{
  baselinestretch = 1,
  % indent from left margin
  xleftmargin = 1mm,
  % vertical grey bar on left side of R output
  frame = leftline,
  framesep = 0pt,
  framerule = 1.5mm, rulecolor = \color{black!15}
  }

\title{Algorithmic Bias, Statistical Notions of Fairness, and the Seldonian Framework}
\author{Dasha Asienga}
\date{April 17, 2024}
\division{}
\advisor{Professor Katharine Correia}
% for second advisor
\institution{Amherst College}
\degree{Bachelor of Arts}
\department{Mathematics and Statistics}

% Fix from pandoc about cslreferences?
% https://github.com/mpark/wg21/issues/54
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newenvironment{CSLReferences}[2]%
  {}%
  {\par}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

% ===========================================
% DOCUMENT SPACING

\setlength{\parskip}{0pt}
% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


% ===========================================
% ===========================================
% ===========================================
\begin{document}

\doublespace
% Everything below added by CII
  \maketitle

\frontmatter % this stuff will be roman-numbered
\pagenumbering{roman}
\pagestyle{fancyplain}
%\pagestyle{fancy} % this removes page numbers from the frontmatter

  \begin{abstract}
    The abstract should be a short summary of your thesis work. A paragraph is usually sufficient here.
  \end{abstract}
  \begin{acknowledgments}
    Use this space to thank those who have helped you in the thesis process (professors, staff, friends, family, etc.). If you had special funding to conduct your thesis work, that should be acknowledged here as well.
  \end{acknowledgments}

  \hypersetup{linkcolor=black}
  \setcounter{tocdepth}{2}
  \tableofcontents

  \addcontentsline{toc}{chapter}{List of Tables}\listoftables

  \addcontentsline{toc}{chapter}{List of Figures}\listoffigures


\mainmatter % here the regular arabic numbering starts
\pagenumbering{arabic}
\pagestyle{fancyplain} % turns page numbering back on

\hypertarget{intro}{%
\chapter{An Introduction to Fairness in Machine Learning}\label{intro}}

In the current landscape, both the public and private sectors are increasingly relying on data-driven methods to automate, streamline, and guide simple and complex decision-making. However, this trend raises an important question of bias. There is a lot of misinterpretation when it comes to the collection of data in many application areas, and there is a major concern for data-driven methods to introduce and perpetuate discriminatory practices inadvertently or to otherwise be unfair because of the social and historical processes that operate to the disadvantage of certain groups.

For example, within healthcare, using mortality or readmission rates to measure hospital performance penalizes hospitals serving poor or non-White populations as those inherently have higher mortality and readmission rates due to confounding societal factors. Outside healthcare, credit-scoring algorithms predict outcomes based on income, which disadvantages low-income groups, further perpetuating economic immobility. Policing algorithms result in increased scrutiny of Black neighborhoods because of the bias against Black people that is already present in the U.S. policing system, and hiring algorithms, which predict employment decisions, are affected by historical race and gender biases.

Yet, these algorithms are often regarded as ground truth and free of human limitations because they are based on mathematics, statistics, and computer science -- otherwise regarded as objective disciplines. In theory, this should lead to greater fairness. However, left unregulated, these mathematical models privilege majority groups and discriminate against minority groups because they often learn from inherently biased data. If the data used to train models contains bias, the resulting algorithms will learn and reflect it into their predictions. In many cases, this can be detrimental.

While there are widely-accepted, though sometimes disputed, societal notions of fairness, a fundamental question arises: are there any established statistical notions of fairness and bias? Is it possible to mathematically and statistically define algorithmic bias and unfairness, thereby opening avenues to address the challenges they pose? And if so, are there ways to leverage statistical tools to resolve such bias and unfairness? This thesis paper is dedicated to exploring and answering precisely these questions.

\hypertarget{algbias}{%
\section{Algorithmic Bias}\label{algbias}}

There are multiple different types and sources of bias in statistics. In particular, algorithmic bias arises when an algorithm's decisions are skewed towards a particular group of people, either positively or negatively (Mehrabi, Morstatter, Saxena, Lerman, \& Galstyan, 2021). The danger with biased algorithmic outcomes is that they generate a feedback loop. Take, for example, a hiring algorithm that discriminates against female applicants for a specific job. In the long run, this algorithm can perpetuate and even amplify existing gender biases by further widening the gender-based class imbalance.

One such key example of algorithmic bias often cited in the literature regards the broad use of the COMPAS -- or the Correctional Offender Management Profiling for Alternative Sanctions -- tool to predict a defendant's risk of recidivism (committing another crime) within two years. COMPAS is more likely to have higher false positive rates for African-American offenders than Caucasian offenders (Mehrabi et al., 2021). Across the country, scores of similar assessments are given to judges, which injects bias into courts (Angwin, Larson, Mattu, \& Kirchner, 2016).

COMPAS is based on data from people arrested in Broward County, Florida, in 2013 and 2014 (Angwin et al., 2016). The response variable, recidivism, was encoded based on who was charged with new crimes over the next two years. Analyses on the predictive efficacy of the COMPAS algorithm found that the algorithm was 61\% accurate for a full range of crimes, including misdemeanors, and only 20\% of people forecasted to commit violent crimes actually went on to do so. While the overall accuracy rate for the full range of crimes is better than a coin flip, there exists room for enhancing the predictive performance, especially for a decision as critical as whether or not to grant a defendant bail or parole.

The COMPAS algorithm is a color-blind model -- race was not included directly as a predictor during its development. However, a statistical analysis showed that even when the effects of race, age, and gender are controlled for through their inclusion as variables in a logistic regression model, Black defendants were still 77\% more likely to be predicted at higher risk of committing a future violent crime and 45\% more likely to be predicted of committing a future crime of any kind as compared to white defendants (Larson, Mattu, Kirchner, \& Angwin, 2016). The table in Figure \ref{fig:compas1} highlights the performance discrepancy across race.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/compas1} 

}

\caption[COMPAS Prediction Fails Differently for Black v White Defendants]{Prediction Fails Differently for Black v White Defendants (Angwin et al., 2016)}\label{fig:compas1}
\end{figure}
Although the tool has 61\% accuracy, Black defendants are almost twice as likely to be labeled as higher risk without re-offending than White defendants, as observed in Figure \ref{fig:compas1}. It makes the opposite mistake among White defendants. This is because classification models are trained to minimize average error, which fits majority populations (Chouldechova \& Roth, 2018).

In truth, however, various societal factors contribute to distinct environmental and social realities for Black and White individuals. These factors, including an offender's personal and familial background and their residential environment, are incorporated into the COMPAS tool to forecast recidivism. Consequently, it appears justifiable to adjust the calibration of the relationship between an offender's social context and their propensity for recidivism differently for Black and White offenders, acknowledging these inherent disparities. A group-blind classifier algorithm that fails to do so could unfairly disadvantage one group over the other -- COMPAS is just one such algorithm. For example, in the education sector, different factors lead to different SAT performances between students from majority versus minority populations. It would, thus, also seem fair that the relationship between SAT and college admissions be calibrated differently for each demographic group. However, making distinctions based on race is generally not acceptable in society.

Therefore, the question becomes, can we modify these algorithms to be group-blind but also fair? In order to do so, fairness constraints that reduce, or even correct for, algorithmic bias during the modeling process must be set. However, one must first define fairness mathematically, statistically, and quantifiably.

\hypertarget{fairnessdefinitions}{%
\section{Statistical Definitions of Fairness}\label{fairnessdefinitions}}

Understanding statistical notions of fairness at both group and individual levels is crucial. \emph{Group notions} focus on a few demographic groups and assess the parity of statistical measures across all these groups (Chouldechova \& Roth, 2018). It is important to note that while providing guarantees to `average' numbers of protected groups, group measures do not necessarily ensure fairness to individuals or structured subgroups within protected demographic groups. These notions are the focus of this paper.

\emph{Individual notions}, on the other hand, are assessed on specific pairs of individuals rather than averaged across groups (Chouldechova \& Roth, 2018). The principle is that similar individuals should be treated similarly along some defined similarity or inverse distance metrics. Counter-factual fairness, for example, relies on the intuition that a decision is fair towards an individual if it's the same in both the real world and a counter-factual world where the individual belongs to a different demographic group (Mehrabi et al., 2021). This can be impractical, relies on strong assumptions about the data, and approaches the realm of causality, further complicating its application (Chouldechova \& Roth, 2018). This highlights the challenges and limitations of individual notions, for which a gap in literature exists.

Ultimately, group notions and individual notions are not in conflict per se. Instead, they exist on the same spectrum of how much dependence is allowed between predictions and the sensitive attribute (Castelnovo et al., 2022). Subgroup fairness is an alternative notion that intends to combine the best properties of both, for example, by picking a group fairness constraint and assessing whether it holds over a large collection of subgroups (Mehrabi et al., 2021). Group and individual fairness notions can be defined in both classification settings and regression settings, although the majority of the literature focuses on fairness within classification.

\hypertarget{group-fairness-in-regression-settings}{%
\subsection{Group Fairness in Regression Settings}\label{group-fairness-in-regression-settings}}

Fair regression is the quantitative notion of fairness of real-valued targets (Agarwal, Dudík, \& Wu, 2019). Consider a general prediction setting where the training set consists of \(X\), a feature vector with all the predictor variables, \(A\), the levels of the protected attribute/ demographic group, and \(Y\), the real-valued continuous response variable. \(F\) is a set of possible prediction models, and the goal is to find \(f \in F\) that is a good predictive model of \(Y\) given \(X\) and some fairness constraints. The accuracy of a prediction \(f(X)\) = \(\hat{Y}\) on \(Y\) is measured by the mean squared error (MSE) as the loss function \(l(Y, f(X))\). The goal is to minimize \(l(Y, f(X))\), hence maximizing accuracy.

\emph{Statistical parity} refers to minimizing the expected loss function (MSE) such that the probability that each predicted \(f(X)\) = \(\hat{Y}\) is above a certain threshold \(z\) for each sensitive attribute is the same as the probability over the entire data set, given some margin \(\epsilon_a\) that is dependent on the protected attribute (Agarwal et al., 2019):
\begin{equation}
\label{ch1eq1}
\text{min}_{f \in F} \text{ } E[l(Y, f(X))] \text{ such that } \forall a \in A, z \in [0,1]:
\end{equation}
\[ |P[f(X) \geq z | A = a] - P[f(X) \geq z]| \leq \epsilon_a.\]

This is akin to the classification setting, where it may be desirable to have the probability of being in the positive class above some certain threshold for each group as well as across the entire data set. A similar notion, known as \emph{bounded loss}, requires that the MSE for each group be below some pre-specified level \(c_a\) that is dependent on the protected attribute (Agarwal et al., 2019):
\begin{equation}
\label{ch1eq2}
\text{min}_{f \in F} \text{ } E[l(Y, f(X))] \text{ such that } \forall a \in A: 
\end{equation}
\[ E[l(Y, f(X)) | A = a] \leq c_a.\]

\hypertarget{classfairdef}{%
\subsection{Group Fairness in Classification Settings}\label{classfairdef}}

At the core, group notions of fairness in classification refer to treating different groups equally. They aim to remedy or prevent disparate impact, a setting where there is unintended disproportionate adverse impact on a particular group (Chouldechova, 2017). There are three broad notions of observational group fairness: independence, separation, and sufficiency (Castelnovo et al., 2022).

\hypertarget{independence}{%
\subsubsection{Independence}\label{independence}}

\newcommand{\indep}{\perp \!\!\! \perp}

This fairness definition requires predictions, \(\hat{Y}\), to be independent of any sensitive attribute, \(A\), that is, \(\hat{Y} \perp \!\!\! \perp A\) (Castelnovo et al., 2022). Thus, it relies only on the distribution of features and decisions, that is, \(A\), \(X\), and \(\hat{Y}\), and focuses on the equality of the predictions themselves by satisfying the following equation:
\begin{equation}
\label{ch1eq3}
P (\hat{Y} = 1 | A = a) = P (\hat{Y} = 1 | A = b), \text{    } \forall a, b \in A,
\end{equation}
where \(a\), \(b\) are the two demographic groups in question.

This definition is also known as \emph{demographic parity}, \emph{statistical parity}, or generally, group fairness. It requires equal positive prediction ratios (PPR), where PPR is the ratio of the probability of a positive prediction \(\frac{P(\hat{Y} = 1 | A = a)}{P(\hat{Y} = 1 | A = b)} \text{ } \forall \text{ } a, b \in A\), across all demographic group pairings (Castelnovo et al., 2022). In other words, the likelihood of a positive prediction should be the same regardless of the demographic group.

In the COMPAS data set, independence would be satisfied if the probability of predicted recidivism is the same for both Black and White defendants in the data set. That is, the probability that a Black defendant is predicted to recommit a crime within the next two years should be the same as the probability that a White defendant is predicted to recommit a crime.

The visual example in Figure \ref{fig:dp} illustrates a toy scenario where independence is met (Durahly, 2023).
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/dp} 

}

\caption[An Example of Demographic Parity]{An Example of Demographic Parity (Durahly, 2023)}\label{fig:dp}
\end{figure}
The dashed line represents the decision boundary. In both group A and group B, four out of the eight participants were predicted to repay a loan. The other half of the participants were predicted to default. Notice, however, that the class imbalance in this toy credit lending example results in a higher error rate within group B than group A.

A difference in demographic parity \(|P(\hat{Y} = 1 | A = a) - P(\hat{Y} = 1 | A = b)|\) close to 0 or a ratio \(\frac{P(\hat{Y} = 1 | A = a)}{P(\hat{Y} = 1 | A = b)}\) close to 1 by some defined margin is considered a fair solution (Castelnovo et al., 2022). To achieve demographic parity, the different demographic groups must be treated differently, which may seem contrary to societal pre-conceived notions of fairness. Therefore, demographic parity should be used when the primary objective is to enforce some form of equality between groups regardless of all other information and when the objectivity of the target variable, \(Y\), is under question, perhaps because of historical biases. This, however, can unknowingly amplify biases if used in the wrong setting. For example, when imposing demographic parity on a hiring algorithm, if qualifications are different across a protected attribute, then less-qualified candidates may be hired. If these candidates end up being low-performers, then this can perpetuate stereotypes about their demographic group.

In the above example of using a hiring algorithm with gender as the protected attribute, it may then seem fairer to require independence on gender only for men and women with the same rating or qualification, that is, \(\hat{Y} \perp \!\!\! \perp A | R\). This is known as \emph{conditional demographic parity} and requires that the following equation is satisfied (Castelnovo et al., 2022):
\begin{equation}
\label{ch1eq4}
P (\hat{Y} = 1 | A = a, R = r) = P (\hat{Y} = 1 | A = b, R = r), \text{    } \forall a, b \in A, \forall r.
\end{equation}
This idea can be generalized more to condition on all attributes, that is, \(\hat{Y} \perp \!\!\! \perp A | X\). As this is more generalized, however, it begins to satisfy individual fairness (Castelnovo et al., 2022). This type of individual fairness is also referred to as ``fairness through unawareness'' (FTU), which requires that any protected attributes, or their covariates, are not explicitly used in the decision-making process (Mehrabi et al., 2021). This definition of fairness requires that the following equation be satisfied (Castelnovo et al., 2022):
\begin{equation}
\label{ch1eq5}
P (\hat{Y} = 1 | A = a, X = x) = P (\hat{Y} = 1 | A = b, X = x), \text{ } \forall a, b \in A, \forall x \in X.
\end{equation}
\hypertarget{separation}{%
\subsubsection{Separation}\label{separation}}

Independence does not make use of the true target \(Y\) and simply requires equality of predictions. However, as observed in Figure \ref{fig:dp}, this can lead to different error rates between different groups. In other words, the model is more accurate for one group than it is for another group. Separation precisely focuses on equality of the error rates and is widely known as the \emph{equality of odds} (Castelnovo et al., 2022). This definition requires the same type I and type II error rates, precisely, the same false positive rate (FPR) and false negative rate (FNR) across all demographic groups. FPR and FNR are defined by:
\begin{equation}
\label{ch1eq6}
FPR = P(\hat{Y} = 1| Y = 0) = \frac{FP}{FP + TN}
\end{equation}
\begin{equation}
\label{ch1eq7}
FNR = P(\hat{Y} = 0| Y = 1) = \frac{FN}{TP + FN}
\end{equation}
where FP refers to false positive predictions, TP refers to true positive predictions, FN refers to false negative predictions, and TN refers to true negative predictions. These metrics can be understood through a confusion matrix as in Figure \ref{fig:confusionmatrix} (Mohajon, 2021).
\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{figures/confusionmatrix} 

}

\caption[A Confusion Matrix]{A Confusion Matrix (Mohajon, 2021)}\label{fig:confusionmatrix}
\end{figure}
In the COMPAS data set, separation would be satisfied if both Black defendants and White defendants had equal error rates. However, as observed in Figure \ref{fig:compas1}, Black defendants had an FPR of 45\% while White defendants had an FPR of 24\% -- the FPR in this context refers to the percentage of times the algorithm predicted the defendants had recidivated when they hadn't. Similarly, Black defendants had an FNR of 28\% while White defendants had an FNR of 48\% -- the FNR in this context refers to the percentage of times the algorithm predicted the defendants had not recommitted a crime when they had.

Separation requires independence of the predictions \(\hat{Y}\) and the sensitive attribute \(A\) conditioned on the true value of the target variable \(Y\), that is, \(\hat{Y} \perp \!\!\! \perp A|Y\) (Castelnovo et al., 2022). In other terms, the following equation must be satisfied:
\begin{equation}
\label{ch1eq8}
P(\hat{Y} = 1 | A = a, Y = y) = P(\hat{Y} = 1 | A = b, Y = y), \text{ } \forall a,b \in A, \text{ } y \in \{ 0, 1 \},
\end{equation}
where 0 is a negative outcome and 1 is a positive outcome. This is a reasonable fairness metric, as long as the objectivity of the target variable is trusted, as it ensures that the model optimizes performance for all groups, not just majority groups.

The visual example in Figure \ref{fig:eoo} illustrates a toy scenario where separation is met (Castelnovo et al., 2022). The dashed line represents the decision boundary. Filled in circles represent positive predictions and empty circles represent negative predictions. The error rates are consistent between both men and women. Notice, however, that the different demographic groups were treated differently to achieve separation as observed in the different decision boundaries (dashed lines).
\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{figures/eoo} 

}

\caption[An Example of Equality of Odds]{An Example of Equality of Odds (Castelnovo et al., 2022)}\label{fig:eoo}
\end{figure}
There are two relaxed versions of this measure depending on which outcome is most important to predict (Castelnovo et al., 2022):
\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  \emph{Predictive Equality}: equality of false positive rates (FPR) across groups:
\end{enumerate}
\begin{equation}
\label{ch1eq9}
P (\hat{Y} = 1 | A = a, Y = 0) = P (\hat{Y} = 1 | A = b, Y = 0), \text{ } \forall a,b \in A.
\end{equation}
\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  \emph{Equality of Opportunity}: equality of false negative rates (FNR) across groups:
\end{enumerate}
\begin{equation}
\label{ch1eq10}
P (\hat{Y} = 0 | A = a, Y = 1) = P (\hat{Y} = 0 | A = b, Y = 1), \text{ } \forall a,b \in A.
\end{equation}
\hypertarget{sufficiency}{%
\subsubsection{Sufficiency}\label{sufficiency}}

Finally, sufficiency takes the perspective of people that receive the same model prediction and requires parity among them regardless of sensitive features (Castelnovo et al., 2022). This is also knows as \emph{predictive parity} and requires that the precision be the same across sensitive groups, that is, \(Y \perp \!\!\! \perp A | \hat{Y}\). In other words, the following equation must be satisfied:
\begin{equation}
\label{ch1eq11}
P (Y = y | A = a, \hat{Y} = y) = P (Y = y | A = b, \hat{Y} = y), \text{ } \forall a, b \in A, \text{ for } y \in \{0,1\}.
\end{equation}
Simply put, the probability of a positive outcome given a positive prediction, and that of a negative outcome given a negative prediction, should be equal across all sensitive groups.

\(\\\)

Consistent with this line of reasoning, many fairness metrics can be defined. This begs the fundamental question: can multiple definitions be simultaneously enforced?

\hypertarget{fairness-conflicts}{%
\section{Fairness Conflicts}\label{fairness-conflicts}}

Because of the way different fairness definitions are defined, it can be impossible to simultaneously enforce multiple definitions, and unexpected behavior may result from a particular definition of fairness. This section highlights some conflicts that arise both in the regression and classification setting.

\hypertarget{fairness-conflicts-in-regression}{%
\subsection{Fairness Conflicts in Regression}\label{fairness-conflicts-in-regression}}

The UFRGS Entrance Exam and GPA Data contains entrance exam scores of students applying to the Federal University of Rio Grande do Sul in Brazil, along with the students' GPAs during their first three semesters at the university (Silva, 2019). Each student's score in nine different entrance exams is used to predict their GPA during their first 3 semesters of study at the university. Gender and race are the protected attributes.

Taking gender as the protected attribute in a gender-blind model, independence, in this setting, would require that the average predictions be the same for each gender. That is,
\begin{equation}
\label{ch1eq12}
E[\hat{Y} | G = Male] = E[\hat{Y} | G = Female].
\end{equation}
Independence is violated if, on average, a model predicts a higher or lower GPA based on gender.

Separation, on the other hand, would require that the average error of predictions be the same for each gender. In defined notion,
\begin{equation}
\label{ch1eq13}
E[\hat{Y} - Y | G = Male] = E[\hat{Y} - Y| G = Female].
\end{equation}
Separation is violated if, on average, the model over-predicts for one gender but under-predicts for another gender or the model either over-predicts or under-predicts more for one gender.

However, a study found that because male and female applicants had different GPAs in the original data set, these two fairness definitions cannot be simultaneously satisfied (P. Thomas, 2020). A similar result in the Section \ref{class-conflict} will explain this in a mathematically tractable way when working in a classification setting.

\hypertarget{class-conflict}{%
\subsection{Fairness Conflicts in Classification}\label{class-conflict}}

Define prevalence \(p\) as the probability of a positive outcome given the demographic group (Chouldechova, 2017). It directly relates to the class distribution of the outcome. \(p \in (0,1)\) and can be denoted by:
\begin{equation}
\label{ch1eq14}
p_a = P(Y=1|A=a).
\end{equation}
Further define the positive predictive value (PPV) of a prediction as the probability of a positive outcome given a positive prediction (Chouldechova, 2017):
\begin{equation}
\label{ch1eq15}
PPV(\hat{Y}|A = a) \equiv P (Y = 1| \hat{Y} = 1, A = a).
\end{equation}
Similarly, the negative predictive value (NPV) of a prediction is the probability of a negative outcome given a negative prediction and can be denoted as:
\begin{equation}
\label{ch1eq16}
NPV(\hat{Y}|A = a) \equiv P (Y = 0| \hat{Y} = 0, A = a).
\end{equation}
Sufficiency would require equal PPV and equal NPV across the different demographic groups. Note that NPV and PPV can be computed from a confusion matrix (Figure \ref{fig:confusionmatrix}) (Saeed, Alireza, Mohamed, \& Ahmed, 2015):
\begin{equation}
\label{ch1eq17}
PPV = \frac{TP}{TP + FP} \text{   ;   } NPV = \frac{TN}{TN + FN}.
\end{equation}
Now, given values of the \(PPV \in (0,1)\) and \(p \in (0,1)\), it can be shown that (Chouldechova, 2017):
\begin{equation}
\label{ch1eq18}
FPR = \frac{p}{1-p} \frac{1-PPV}{PPV}(1 - FNR).
\end{equation}
Appendix \ref{appendix-a} provides the details for the derivation of this equation. However, its direct implication is that if the prevalence differs between two groups, then it is impossible to satisfy sufficiency (equal PPV across all groups) and separation (equal FPR and FNR across all groups) simultaneously. For example, in the COMPAS data set, if recidivism rates differ between Black and White offenders, then an algorithm that guarantees predictive parity/ equal precision for both Black and White offenders cannot simultaneously satisfy equality of odds. Indeed, the recidivism rate for Black defendants in the data is 51\%, compared to 39\% for White defendants, and hence, the disparate impact of the COMPAS tool as observed in Figure \ref{fig:compas1} (Chouldechova, 2017).

Figure \ref{fig:dp} illustrates a similar conflict between independence and separation. Satisfying independence resulted in an imbalance of error rates between group A and group B because of the difference in the prevalence of loan repayment between both groups.

\(\\\)

Unfortunately, the distribution of the outcome of interest often differs for different demographic groups, posing the all-important question: how can fairness be achieved in the face of this conflict?

\hypertarget{on-fairness-conflicts}{%
\subsection{On Fairness Conflicts}\label{on-fairness-conflicts}}

As observed, disparate impact can result from using a prediction tool that is perceived as free from predictive bias. Just because an algorithm satisfies a particular definition of fairness does not infer that the algorithm is \emph{fair} in every sense of that word. Balancing overall error rates alone is insufficient as it does not produce models that are free from bias or that guarantee fairness at finer levels of granularity. This highlights the need for human value and domain expertise in defining fairness within the context of a particular problem before the fairness constraints can be set. Chapter \ref{chap-2} introduces a framework for setting these constraints.

\hypertarget{chap-2}{%
\chapter{Seldonian Algorithms}\label{chap-2}}

Chapter \ref{intro} introduced the problem of algorithmic bias, discussed existing statistical definitions of fairness both in regression and classification settings, and finally, highlighted fairness conflicts that can arise in certain settings. Of important note is that there are a plethora of fairness definitions that have been developed in statistical machine learning, many of which have been shown to be incompatible in ways similar to the illustration in Appendix \ref{appendix-a}. In any effort to enforce fairness on machine learning models, a critical first step is to define what fairness means in the specific context (P. Thomas, 2020). This responsibility falls on domain experts, social scientists, and regulators. Once there is consensus on that, machine learning researchers can work to develop appropriate algorithms that enforce the chosen definition of fairness. The Seldonian framework, introduced in this chapter, offers one such way to place probabilistic fairness constraints on traditional algorithms. However, because Seldonian algorithms place constraints on traditional machine learning (ML) algorithms, an initial in-depth understanding of the standard approach is key. Section \ref{standardml} discusses the typical ML approach before diving into the Seldonian framework in Section \ref{seldonian}.

\hypertarget{standardml}{%
\section{The Standard Machine Learning Approach}\label{standardml}}

When designing a machine learning algorithm, the first step is to mathematically define what the algorithm should do, in other words, the goal of the algorithm (P. S. Thomas et al., 2019b). At an abstract level, this goal is identical for all machine learning problems: find a solution \(\theta^*\), within some feasible set \(\Theta\), that maximizes some objective function \(f: \Theta \rightarrow \textbf{R}\), where \(\textbf{R}\) is the set of real numbers. Precisely, the goal of the algorithm is to search for an optimal solution
\begin{equation}
\label{ch2eq1}
\theta^* \in \underset{\theta \in \Theta}{\text{ arg max }} f(\theta).
\end{equation}
For example, let \(X\) and \(Y\) be dependent real-valued random variables in a regression setting with the goal of estimating \(Y\) given \(X\). In this setting, \(\Theta\) is the set of feasible functions that model the relationship between \(X\) and \(Y\). Feasible functions are of the form \(\theta(X) = \beta_0 + \beta_1X = \hat{Y}\). Each function \(\theta \in \Theta\) takes a real number as input and produces a real number as output; therefore, \(\theta : \textbf{R} \rightarrow \textbf{R}\). A reasonable objective function would then be the negative mean squared error (MSE):
\begin{equation}
\label{ch2eq2}
f(\theta):=-E[(\theta(X) - Y)^2].
\end{equation}
In this case, minimizing MSE is equivalent to maximizing -MSE, defining the goal of the regression algorithm as finding the solution with the least average error. Note that the true value of \(f(\theta)\) is unknown and can only be estimated from the data (P. S. Thomas et al., 2019a). For a sample with \(n\) observations, that is, \((x_i, y_i) \text{ for } i = 1,2,...,n\), the objective function can be estimated by:
\begin{equation}
\label{ch2eq3}
\hat{f(\theta)}= -\frac{1}{n} \sum_{i=1}^{n}(\theta(x_i) - y_i)^2.
\end{equation}
However, defining objective functions in this way can sometimes lead to undesirable behavior as illustrated in Section \ref{standardlimitations}.

\hypertarget{standardlimitations}{%
\subsection{Limitations of the Standard Approach}\label{standardlimitations}}

Consider a linear regression example to predict the qualifications of job applicants based on information on their resumes. Let \(G\) encode the gender of each applicant, with \(G=0\) if the applicant is female and \(G=1\) if the applicant is male. Let \(X\) encode a summary measure of an applicant's qualification based on information on their resume -- a simple example would be a measure of how many job-relevant key words appear on their resume. Let \(Y\) encode their actual qualification for the job as determined by their observed performance.

If this linear regression estimator is designed to be used to filter which resumes submitted to a company will be forwarded for human review, it is worthwhile to ensure that the algorithm does not produce racist or sexist behavior. Drawing from definitions in Chapter \ref{fairnessdefinitions}, it might be less important to ensure that the algorithm, on average, has the same predictions for applicants of both genders because the distribution of qualifications may be different for both genders. However, of more concern is whether the algorithm, on average, predicts too high for one gender and too low for the other gender.

Suppose that the data has the following distribution: \(Y \sim N(1,1)\) if \(G = 0\) and \(Y \sim N(-1,1)\) if \(G = 1\), that is, \(Y\) is a normal variable \(N(\mu, \sigma)\) with different means \(\mu\) for different genders but with the same standard deviation \(\sigma\) for both genders. Further define \(X \sim N(Y,1)\), that is, an applicant's resume quality is equal to their true qualification plus some random noise. Figure \ref{fig:fig1} displays a scatterplot of 1000 such data points, 500 from each gender. The black solid line is the least squares fit on this data using a gender-blind model.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/fig1-1} 

}

\caption{Least Squares Fit on Synthetic Data Drawn from Different Distributions}\label{fig:fig1}
\end{figure}
The least squares fit on Figure \ref{fig:fig1} is impartial to an observation's gender with an objective to make the most accurate predictions. While it may be expected that impartiality would produce fair results, observe that the linear model tends to over-predict if \(G = 1\) and under-predict if \(G=0\), producing discriminatory behavior. In fact, by defining a discrimination statistic, \(d(\theta)\), that measures whether the model satisfies separation (equal error rates), the discrimination statistic for the synthetic data set in Figure \ref{fig:fig1} can be shown to be -0.719, suggesting that the model predictions are in favor of \(G = 1\):
\begin{equation}
\label{ch2eq4}
d(\theta) =  E[\hat{Y}-Y|G = 0 ] - E[\hat{Y}-Y|G = 1 ]. 
\end{equation}
In crucial applications such as hiring, this is concerning and highlights how group-blind linear regression algorithms designed using the standard approach and following statistical best practices can result in predictions that systematically discriminate against a demographic group.

\hypertarget{potential-remedies}{%
\subsection{Potential Remedies}\label{potential-remedies}}

In an attempt to remedy this undesirable behavior, a number of approaches can be taken. One potential remedy is to identify the root cause of the undesirable behavior such as class imbalance in the training data, bias in the data set, the choice of linear estimator, the model's blindness to the demographic group, or insufficient data, to name a few (P. S. Thomas et al., 2019b). For instance, in the example set up in Section \ref{standardlimitations} and displayed in Figure \ref{fig:fig1}, the root cause of the discriminatory behavior when using ordinary least squares linear regression was the fact that the objective function was designed to minimize MSE, which was at odds with minimizing the discrimination statistic. However, even though it might be possible to determine and correct the root cause of the undesirable behavior, doing so can be difficult, error-prone, and require extensive data analysis, rendering the central goal of machine learning algorithms, which are designed to automate and make decision-making processes simpler, obsolete.

Assuming that the problem is with the objective function and provided that detailed knowledge of the problem is available, hard constraints may be placed on the objective function, for example, requiring that MSE is minimized only on the set of solutions with a discrimination value \(d(\theta)\) less than some value \(\epsilon\) (P. S. Thomas et al., 2019b). Additionally, rather than placing hard constraints on the set of solutions, soft constraints that penalize undesirable behavior may also be placed on \(f\), the objective function (Boyd \& Vandenberghe, 2004). Although such penalty functions can be effective, they require a careful choice of the value of the parameter \(\lambda\) that places relative importance on the objective function and the constraint. For the linear regression example, the new objective function with a soft constraint would now be:
\begin{equation}
\label{ch2eq5}
f(\theta) = - MSE (\theta) - \lambda d(\theta).
\end{equation}
Observe that as \(\lambda\) increases, MSE increases and the discrimination statistic decreases. Cross-validation techniques can be employed to find optimal values for \(\lambda\). Other remedies include maximizing multiple objective functions or allowing constraints on the probability that a solution with undesirable behavior will be returned, both of which may require detailed knowledge of the application problem and underlying distribution of the data (P. S. Thomas et al., 2019b).

In principle, there might be definitions of \(\Theta\) or \(f\) that prevent the algorithm from converging on solutions that exhibit undesirable behavior (P. S. Thomas et al., 2019a). However, in practice and as explained, this might require extensive domain expertise and data analysis in order to properly balance the relative importance of the objective function and the constraints, which can be at odds with each other. These techniques may also require knowledge of the probability distribution from which the data is sampled, which is not always available and limits applications to parametric statistics.

Seldonian algorithms address this problem precisely by allowing probabilistic constraints on undesirable behavior to be placed more easily without detailed knowledge of the specific problem or the distribution of the data, shifting the burden from the domain experts who use these tools to the experts in ML and statistics (P. S. Thomas et al., 2019a). It's named after Isaac Asimov's fictional character, Hari Seldon \footnote{In the fictional book, Hari Seldon was a resident of a fictional planet where he develops psycho-history, an algorithmic science that allows him to predict the future in probabilistic terms.} (Asimov, 1994). It's important to note that while Seldonian algorithms allow for more seamless implementation in practice, domain experts are still needed to define the relevant fairness constraints for a given context.

\hypertarget{seldonian}{%
\section{The Seldonian Framework}\label{seldonian}}

The first step of the Seldonian framework is to define mathematically the goal of the algorithm design (P. S. Thomas et al., 2019b). Define \(\textbf{D}\) as the set of all possible inputs (data sets) to the algorithm. \(\Theta\), as previously defined, is the set of all possible outputs (solutions) of the algorithm. Each solution is referred to as \(\theta \in \Theta\). \(D\) is the data set (input) given to the algorithm and is the only random variable. Now, \(a: \textbf{D} \rightarrow \Theta\) is a machine learning algorithm which takes in a data set \(D \in \textbf{D}\) as an input and returns a solution \(\theta \in \Theta\) as an output. \(\textbf{A}\) is the set of all possible machine learning algorithms. Synthesizing this, \(f: \textbf{A} \rightarrow \textbf{R}\) is the objective function of the algorithm design, where \(f(a) \in \textbf{R}\) is a real-valued measure of the utility of the algorithm, such as the value of the objective function for the solution returned by this algorithm. This objective function is optimized -- either minimized or maximized -- to select a desired machine learning algorithm from the set \(\textbf{A}\).

Contrary to the standard ML approach, however, \(n\) behavioral constraints can then be specified (P. S. Thomas et al., 2019b). Specifically, \((g_i, \delta_i)_{i=1}^{n}\) can be defined as a set of \(n\) constraints, each of which contains a constraint function \(g_i: \Theta \rightarrow \textbf{R}\) and a desired confidence level \(\delta_i\). The constraint function takes in a solution returned from the chosen machine learning algorithm as an input and returns a real value encoding the ``fairness'' of the algorithm according to the fairness definition defined by the function. \((g_i, \delta_i)_{i=1}^{n}\) is defined such that:
\begin{itemize}
\item
  The \(i^{th}\) constraint function measures an undesirable behavior. Specifically, \(\theta \in \Theta\) produces undesirable behavior if and only if \(g_i(\theta) > 0\). This is to ensure that undesirable behavior is defined in a mathematically tractable way such as how the discrimination statistic \(d(\theta)\) was defined in Section \ref{standardlimitations}.
\item
  The \(i^{th}\) confidence level specifies the maximum probability that an algorithm can return a solution \(\theta\) where \(g_i(\theta) > 0\). In other words, \(1 - \delta_i\) specifies the minimum probability that desirable behavior (\(g_i(\theta) \leq 0\)) is met. Smaller values of \(\delta_i\) are preferred.
\end{itemize}
In summary, a Seldonian algorithm ensures that for all \(i \in \{1,2,\ldots,n\}\):
\begin{equation}
\label{ch2eq6}
P(g_i(a(D)) \leq 0) \geq 1 - \delta_i.
\end{equation}
Section \ref{sop} goes into further detail about the Seldonian framework and how these probabilistic behavioral constraints are guaranteed.

\hypertarget{sop}{%
\subsection{The Seldonian Optimization Problem}\label{sop}}

As detailed, the Seldonian framework is different from current potential remedies of undesirable behavior because it defines a search over a possible set of algorithms with constraints, rather than over a possible set of solutions. This means that the constraints require that the probability that a machine learning algorithm returns an unsafe solution be bounded by some desired level of confidence, rather than the probability that a solution itself is unsafe. In summary, the Seldonian optimization problem (SOP) can be written as (P. S. Thomas et al., 2019a):
\begin{equation}
\label{ch2eq7}
\underset{a \in \textbf{A}}{\text{ arg max }} f(a)
\end{equation}
\[\text{ s.t. } \forall \text{ } i \in \{1, 2, \ldots, n\} \text{, } P(g_i(a(D)) \leq 0) \geq 1 - \delta_i.\]

A Seldonian algorithm \(a\), thus, returns, with high probability, a solution that guarantees desirable behavior. If one were to apply machine algorithm \(a\) to obtain a solution from a large number of different data sets \(D\) drawn from the same distribution, then it would be expected that at most \(100\delta_i \%\) solutions (models) would produce undesirable behavior.

Taking the previous regression example and turning it into a Seldonian optimization problem using the discrimination statistic in Section \ref{standardlimitations}, \(f\) would still be an objective function like the MSE, \(\Theta\) would still be the set of all possible linear models, and \(D\) would be the data set as described. There would be 1 behavioral constraint, \(g_1(a(D)) = |d(a(D))| - \epsilon\), to guarantee with probability at least \(1-\delta_1\), that the absolute value of the discrimination statistic would be at most \(\epsilon\), where \(\epsilon\) and \(\delta_1\) are chosen by domain experts based on the specific application. Note that the user of the machine learning algorithm need not perform data analysis to determine whether \(g_1(\theta) \leq 0\) for a particular solution \(\theta \in \Theta\) returned. The computation algorithm guarantees this with some desired level of probability.
\begin{figure}

{\centering \includegraphics[width=1\linewidth]{figures/seldonian1} 

}

\caption[Overview of the Seldonian Framework]{Overview of the Seldonian Framework (P. S. Thomas et al., 2019a)}\label{fig:seldonian1}
\end{figure}
Figure \ref{fig:seldonian1} illustrates how this is achieved at a high level. A Seldonian algorithm takes in \(n\) behavioral constraints \((g_i,\delta_i)_{i=1}^n\) and a data set \(D\) as the inputs and returns either a solution (model) \(\theta\) or \(NSF\), which means ``No Solution Found''. An NSF result means no algorithm was found that returned a model which satisfied the behavioral constraints with the desired probability, so solutions are not guaranteed when employing Seldonian algorithms.

First, the data \(D\) is partitioned into 2 sets \(D_1\) and \(D_2\) that essentially serve as train and test sets, respectively. \(D_1\) is then passed through the candidate selection mechanism, which performs a search over algorithms to settle on a candidate solution \(\theta_c\). \(\theta_c\) is selected not only so that it optimizes the primary objective function \(f\), but also so that it is predicted to pass the subsequent safety test. \(D_2\) is then passed through the safety test to check whether \(\theta_c\) indeed satisfies the \(n\) behavioral constraints with the desired confidence for each, that is \(P(g_i(\theta_c) \leq 0) \geq 1 - \delta_i\) for each constraint \(i \in \{ 1,2, \ldots, n \}\). If so, \(\theta_c\) is returned as the desired solution, and otherwise, NSF (P. S. Thomas et al., 2019a).

Note that finding exact confidence bounds may be impractical and require large amounts of data. Quasi-Seldonian algorithms, thus, are an extension of this idea that rely on standard statistical tools to transform sample statistics computed from \(D\) into approximate bounds on the probability of undesirable behavior (P. S. Thomas et al., 2019a). Section \ref{qsa} discusses the statistical framework employed to achieve this.

\hypertarget{qsa}{%
\subsection{Quasi-Seldonian Algorithms}\label{qsa}}

Recall that the Seldonian goal is to create an algorithm \(a\) that is an approximate solution to the Seldonian optimization problem defined in Equation \ref{ch2eq7}. This framework is non-parametric and relies on exact values of the objective function and fairness constraints without making any assumptions about the data. However, these values are often unknown and need to be estimated from the data provided. This may require making some assumptions about the underlying population distribution the sample was drawn from, hence the name \(\textit{quasi}\)-Seldonian.

For example, \(f(a)\), the objective function, can be estimated from the data provided such that \(\hat{f}: \Theta \text{ x } \textbf{D} \rightarrow \textbf{R}\) serves as a measure of the utility of the algorithm that returns a solution \(\theta\) when given input \(D\) (P. S. Thomas et al., 2019a). In a linear regression setting, the MSE for a data set of size \(m\) can be estimated by
\begin{equation}
\label{ch2eq9}
\hat{f}(\theta, D) = -\frac{1}{m} \sum_{i=1}^{m}(\hat{y}(X_i, \theta) - Y_i)^2.
\end{equation}
In a similar fashion, the following section discusses how the candidate selection and safety test mechanisms further employ statistical estimation techniques and assumptions of normality to estimate the confidence bounds of the fairness constraints and probabilistically guarantee safe behavior.

\hypertarget{safety}{%
\subsubsection{The Safety Test Mechanism}\label{safety}}

Seldonian algorithms ensure that \(P(g_i(\theta_c) \leq 0) \geq 1 - \delta_i\) for each constraint \(i \in \{ 1,2, \ldots, n \}\) and the safety test mechanism is the component that verifies whether these behavioral constraints actually hold (P. S. Thomas et al., 2019a). This is achieved by computing an upper bound for each \(g_i(\theta)\) using the data and a confidence interval derived from the Student \(t\)-statistic. If the high confidence upper bound is less than or equal to 0, then the solution is safe to return, otherwise, no solution will be returned.

Let \(X = (X_1, \ldots, X_m)\) be \(m\) independent and identically distributed (\(i.i.d.\)) random observations. Under the assumption that \(\frac{1}{m} \sum_{i=1}^m X_i\) is normally distributed or if \(m\) is sufficiently large -- by the Central Limit Theorem --, then the Student \(t\)-statistic can be used to compute an upper bound of the expected value of these random variables as follows:
\begin{equation}
\label{ch2eq10}
P(E[X_1] \leq \hat{\mu}(X) + \frac{\hat{\sigma}(X)}{\sqrt{m}}t_{1-\delta, m-1}) \geq 1 - \delta,
\end{equation}
where
\begin{itemize}
\item
  \(\hat{\mu}(X) = \bar{X}\) and \(\hat{\sigma}(X) = s\) are the sample mean and standard deviation, respectively, of a vector X. That is, \(\hat{\mu}(X) = \frac{1}{m}\sum_{i=1}^m X_i = \bar{X}\) and \(\hat{\sigma}(X) = \sqrt{\frac{\frac{1}{m}\sum_{i=1}^m (X_i - \hat{\mu}(X))^2}{m-1}} = s\).
\item
  \(t_{1-\delta, m-1}\) is the \(100(1-\delta)\) percentile of the Student \(t\)-distribution with \(m-1\) degrees of freedom.
\end{itemize}
Before constructing the safety test mechanism, recall from Section \ref{sop} that (P. S. Thomas et al., 2019b):
\begin{itemize}
\item
  The safety test will be applied to a single solution \(\theta_c\) selected by the candidate selection mechanism. This process is explained in the following section.
\item
  The safety data, \(D_2\), is used to verify that the behavioral constraints hold.
\item
  \(\hat{g_i}(\theta_c, D_2) = (\hat{g_{i,1}}(\theta_c, D_2), \ldots,\hat{g_{i,m}}(\theta_c, D_2))\) contains \(m\) \(i.i.d\) values of \(\hat{g_i}(\theta_c)\) for each of the \(m\) observations in \(D_2\). \(|D_2|\) will be used to denote the number of observations in \(D_2\) for consistency in notation.
\item
  \(E[\hat{g_i}(\theta_c, D_2)] = g_i(\theta_c)\).
\end{itemize}
By substituting the respective pieces into the Student \(t\) high confidence upper bound discussed above, then:
\begin{equation}
\label{ch2eq11}
P(g_i(\theta_c) \leq \hat{\mu}(\hat{g_i}(\theta_c, D_2)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_2))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1}) \geq 1 - \delta_i.
\end{equation}
Notice that \(\hat{\mu}(\hat{g_i}(\theta_c, D_2)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_2))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1}\) is an upper bound of the confidence interval with confidence \(1-\delta_i\). If this upper bound is less than or equal to zero, then the \(i^{th}\) behavioral constraint \(g_i(\theta_c)\) is less than or equal to zero with at least probability \(1-\delta_i\). Therefore, \(\theta_c\) is only returned if \(\hat{\mu}(\hat{g_i}(\theta_c, D_2)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_2))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1} \leq 0\). Specifically, this holds only under the assumption that \(\hat{\mu}(\hat{g_i}(\theta_c, D_2))\) is normally distributed or if the size of the safety data \(D_2\) is sufficiently large, hence the name \(\textit{quasi}\)-Seldonian (P. S. Thomas et al., 2019a). The next section now discusses precisely how \(\theta_c\) is selected before being passed into the safety test mechanism.

\hypertarget{candidate}{%
\subsubsection{The Candidate Selection Mechanism}\label{candidate}}

With the safety test in place, any algorithm will be Seldonian regardless of how \(\theta_c\) is computed, as long as \(\theta_c\) is computed using a different subset of the data, hence the partition into \(D_1\) (candidate data) and \(D_2\) (safety data) (P. S. Thomas et al., 2019b). However, if \(\theta_c\) is computed using the standard ML approach, then it will likely be unsafe as was illustrated in Section \ref{standardlimitations}, resulting in an NSF output. Instead, \(\theta_c\) will be computed as follows:
\begin{equation}
\label{ch2eq12}
\theta_c \in \underset{\theta \in \Theta}{\text{ arg max }} \hat{f}(\theta, D_1)
\end{equation}
\[\text{ s.t. } \theta_c \text{ is predicted to pass the safety test}.\]

Thus, only solutions likely to pass the safety test will be considered by predicting the result of the safety test using \(D_1\) (the candidate data) instead of \(D_2\). In formal notation,
\begin{equation}
\label{ch2eq13}
\theta_c \in \underset{\theta \in \Theta}{\text{ arg max }} \hat{f}(\theta, D_1)
\end{equation}
\[ \text{ s.t. } \forall \text{ } i \in \{1,2,\ldots,n\}, \text{  } \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1} \leq 0. \]

Notice that while the sample mean \(\hat{\mu}\) and the sample standard deviation \(\hat{\sigma}\) are computed over \(D_1\), the size of \(D_2\) is still used to correct the standard deviation and compute the Student \(t\) percentile, in order to ensure that the solution is properly predicted to pass the safety test.

The process defined can work well when the objective function \(f\) and the behavioral constraints are aligned. However, when they are in conflict, the candidate selection mechanism tends to be over-confident that \(\theta_c\) will pass the safety test and a safe solution will be returned (P. S. Thomas et al., 2019b). Doubling the width of the confidence level is a proposed solution to produce more conservative predictions and better guarantees of \(\theta_c\) passing the safety test. Therefore, a black-box optimization algorithm is used to compute
\begin{equation}
\label{ch2eq14}
\theta_c \in \underset{\theta \in \Theta}{\text{ arg max }} \hat{f}(\theta, D_1)
\end{equation}
\[\text{ s.t. } \forall \text{ } i \in \{1,2,\ldots,n\}, \text{  } \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + 2\frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}}t_{1-\delta_i, |D_2|-1} \leq 0.\]

\hfill\break

This concludes the discussion on the Seldonian theoretical framework at a high level. To conclude this chapter, Section \ref{toy} walks through how this is implemented computationally using a toy regression example.

\hypertarget{toy}{%
\section{Toy Example: A Quasi-Seldonian Regression Algorithm}\label{toy}}

The tutorial in this section follows the presentation by P. S. Thomas (n.d.) on the AI Safety webpage focusing on the key computational aspects of the Seldonian framework. Consistent with the linear regression set-up in this chapter, consider \(X, Y \in \textbf{R}\) as two dependent random variables with the goal of estimating \(Y\) given \(X\) through a sample of \(m\) observations. \(X\) is drawn synthetically from a \(N(0,1)\) distribution and \(Y\) is dependent on \(X\) with a \(N(X,1)\) distribution. Figure \ref{fig:fig2} displays 5000 such points. \(\hat{y}(X, \theta) = \theta_1X+\theta_2\) and \(MSE = E[(\hat{y}(X, \theta)-Y)^2]\) are computed as previously defined.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/fig2-1} 

}

\caption{Synthetic Data for Quasi-Seldonian Linear Regression Tutorial}\label{fig:fig2}
\end{figure}
In this example, the goal of the linear regression algorithm is two-fold: to minimize MSE (or equivalently, maximize -MSE) while ensuring, with probability at least 0.9, that \(1.25 < MSE < 2\). Note that this behavioral constraint is not practical in an application setting, but it's deliberately designed to be at odds with the objective function in order to test behavior when such conflict arises. Additionally, it'll be simple to verify satisfaction of the behavioral constraint for demonstration purposes. The behavioral constraint needs to be mathematically represented in a way that defines \(g(\theta) \leq 0\) as safe behavior. Therefore, \(n\) will be set to \(2\) such that:
\begin{itemize}
\item
  \(g_1(\theta) = MSE(\theta) - 2.0 \text{; } \delta_1 = 0.1\).
\item
  \(g_2(\theta) = 1.25 - MSE(\theta) \text{; } \delta_2 = 0.1\).
\end{itemize}
Unbiased estimates of the MSE and each \(g_i(\theta)\) will be computed from the data set as elucidated in Section \ref{qsa}. The Python code used to implement and compute the quasi-Seldonian linear regression algorithm is displayed in detail in Appendix \ref{appendix-b}. To reduce computational burden, all the computation was performed on the Amherst College High-Performance Computing System. In this case, a solution that minimizes the MSE while satisfying the 2 behavioral constraints, \(g_1(\theta)\) and \(g_2(\theta)\), was found -- the MSE was \(1.385\). Figure \ref{fig:fig3} visually compares the quasi-Seldonian solution (blue) with the ordinary least squares solution (red).
\begin{verbatim}
   A solution was found: [0.6050927032, 1.0201681981]
   fHat of solution (computed over all data, D): -1.3854947860210223
\end{verbatim}
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/fig3-1} 

}

\caption{Quasi-Seldonian Linear Regression}\label{fig:fig3}
\end{figure}
To conclude this chapter, Section \ref{exp} scales this process to repeatedly run a quasi-Seldonian linear regression algorithm using different amounts of data and analyzes the results.

\hypertarget{exp}{%
\subsection{Experimentation}\label{exp}}

Consistent with the set-up in Section \ref{toy} and following the presentation by P. S. Thomas (n.d.), the aim of the experimentation in this section is to assess three aspects of the quasi-Seldonian linear regression algorithm defined in Section \ref{toy}: performance loss, frequency of solutions, and frequency of undesirable behavior for varying sample sizes.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/fig4-1} 

}

\caption{QSA Experiment: Performance Loss}\label{fig:fig4}
\end{figure}
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/fig5-3} 

}

\caption{QSA Experiment: Probability of a Solution}\label{fig:fig5}
\end{figure}
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/fig6-5} 

}

\caption{QSA Experiment: Satisfaction of 1st Behavioral Constraint}\label{fig:fig6}
\end{figure}
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/fig7-7} 

}

\caption{QSA Experiment: Satisfaction of 2nd Behavioral Constraint}\label{fig:fig7}
\end{figure}
Each of the figures show how different properties of the quasi-Seldonian linear regression (QSLR) algorithm vary for different data set sizes, \(\textit{m}\). The horizontal axis is on a logarithmic scale, with values of \(\textit{m}\) starting at \(\textit{m} = 32\) and going up to \(\textit{m} = 65,536\), doubling each time such that \(\textit{m} \in \{32, 64, 128, \ldots, 65536 \}\). 50 trials are run for each value of \(m\). Each figure plots the mean and includes a standard error bar to visualize the variance of results.

\hypertarget{performance-loss}{%
\subsubsection{Performance Loss}\label{performance-loss}}

A Seldonian algorithm that has probabilistic guarantees of safe or fair behavior will typically perform worse, with regard to accuracy or error, than an algorithm that is purely focused on optimizing performance (P. S. Thomas, n.d.). In Figure \ref{fig:fig4}, the dotted lines represent the desired range for the MSE as defined by the behavioral constraints set in Section \ref{toy}. In this setting, MSE was forced to be higher. Notice that the QSLR did not return a solution for small amounts of data, but when a solution was returned, it was always within the MSE bounds set by the constraints. Additionally, the bigger the sample size, the closer the MSE was to the lower boundary. This highlights the role that the primary objective plays by encouraging solutions with lower error, albeit still within the desired window. In a different setting where the behavioral constraints are not at odds with the primary objective function, it would be worthwhile to investigate performance loss when using a QSLR algorithm rather than ordinary least squares (OLS). Chapter \ref{chap-3} attempts to answer this question by investigating performance loss for Seldonian algorithms in classification settings.

\hypertarget{probability-of-a-solution}{%
\subsubsection{Probability of a Solution}\label{probability-of-a-solution}}

Quasi-Seldonian algorithms don't always return a solution, especially with little data because there is insufficient confidence that any solution would satisfy the behavioral constraints. Figure \ref{fig:fig5} illustrates that with more data, there is a higher probability of a solution. Nevertheless, there isn't a definitive threshold ensuring a solution for data sizes exceeding it. However, when provided with ample data, the results suggested that the likelihood of obtaining a solution tends to exceed 0.8.

\hypertarget{probability-of-undesirable-behavior}{%
\subsubsection{Probability of Undesirable Behavior}\label{probability-of-undesirable-behavior}}

Finally, Figures \ref{fig:fig6} and \ref{fig:fig7} plot the probability that each algorithm produced undesirable behavior, that is, the probability that each algorithm had a solution with \(g_1(a(D)) > 0\) as in Figure \ref{fig:fig6} and the probability that each algorithm had a solution with \(g_2(a(D)) > 0\) as in Figure \ref{fig:fig7}. Recall that \(\delta_1 = \delta_2 = 0.1\), so for the QSA solution, the probability of undesirable behavior should lie below 0.1, or at least around 0.1. QSA always satisfied both constraints with probability at least 0.9 when a solution was returned. Notice, however, that since OLS does not take behavioral constraints into account, it always violated the second behavioral constraint that requires MSE to be greater than 1.25. This is expected because this constraint is in conflict with the primary objective function.

\(\\\)

The toy example in this Chapter illustrates some of the desirable qualities of Seldonian (or quasi-Seldonian) algorithms and some of the limitations. The focus was in the quantitative setting when using linear regression. Next, Chapter \ref{chap-3} delves into the properties of Seldonian algorithms within a classification setting.

\hypertarget{chap-3}{%
\chapter{Applying the Seldonian Framework in a Classification Setting}\label{chap-3}}

Chapter \ref{chap-2} introduced the Seldonian framework, which offers probabilistic guarantees for satisfying defined behavioral constraints. Although impractical, the toy linear regression example demonstrated how Seldonian algorithms may be employed in a setting with a continuous real-valued response variable.

However, machine learning algorithms deal with classification problems in many practical applications. Applications range from risk assessment tools like COMPAS, discussed in Chapter \ref{intro}, to credit scoring and employment prediction algorithms, to name a few. However, deploying such algorithms raises significant ethical concerns, as discussed in Chapters \ref{intro} and \ref{chap-2}, primarily regarding fairness and the potential reinforcement of discriminatory practices. Given the historical and social biases inherent in the data used to train these algorithms, there is a pressing need to assess their fairness and mitigate any potential harm they may cause disadvantaged groups.

To offer a path forward in addressing this issue, this chapter aims to apply the Seldonian framework to the COMPAS data set and assess its predictive outcomes compared to the COMPAS tool and logistic regression, a standard ML procedure. Specifically, the objective is to assess whether Seldonian approaches can produce fairer outcomes within the COMPAS setting, drawing on some of the statistical definitions of fairness defined in Chapter \ref{fairnessdefinitions}. This chapter will present a framework that can be emulated in other classification problems where fairness is a concern.

\hypertarget{the-compas-data-set}{%
\section{The COMPAS Data Set}\label{the-compas-data-set}}

The COMPAS data set, obtained from the ProPublica Data Store (Broward County Clerk's Office, Broward County Sherrif's Office, Florida Department of Corrections \& ProPublica, 2024), records information on defendants from Broward County, Florida, that were evaluated for the risk of recidivism by the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool in 2013 and 2014. A SQL query of the COMPAS database retrieved 12,160 observations (Appendix \ref{appendix-c}). Of 29 variables returned, 6 were identified to be useful for predictive analysis: age, age category, sex, marital status, whether the defendant had a history of juvenile offenses or not, and the total number of prior offenses committed by the defendant. The response variable records whether or not a defendant had recommitted a crime within two years. Finally, the protected attribute, race, has six levels: African-American, Caucasian, Hispanic, Asian, Native American, and Other.

Additionally, the COMPAS tool itself maps its raw scores into decile scores ranging from 1 to 10. The decile scores are directly associated with the risk of recidivism: scores between 1 and 4 are labeled as `low' risk, between 5 and 7 as `medium' risk, and between 8 and 10 as `high risk'. After cleaning the data to address anomalies, improve the data quality, and remove duplicate observations, the data set size was reduced to 9387 observations. Section \ref{descriptivestats} examines these variables in more detail and their relationships with each other.

Before proceeding with the analysis, it is crucial to consider that while the recidivism status, as recorded by the response variable, is treated as an objective gold standard truth of whether a defendant reoffended or not for the rest of this thesis, it is, in itself, likely biased by societal factors. For example, police officers are more likely to arrest Black defendants than White defendants for the same offense, and judges are more likely to convict and grant lengthier sentences to Black defendants than White defendants for the same charges (The Sentencing Project, 2018). While it is tangential to the theoretical underpinnings of the work in the proceeding sections, this context is, overall, important when applying mathematical, statistical, and computer science solutions to addressing algorithmic bias.

\hypertarget{descriptivestats}{%
\section{Descriptive Statistics}\label{descriptivestats}}

This section presents a holistic exploratory analysis of the COMPAS data set. The defendants' ages range from 18 to 96, with a median of 32 years and a mean of 34.8 years. There is a right skew, with the middle 50\% of the defendants being between the ages of 25 and 42. Similarly, the number of prior offenses has a significant right skew with a median of 1 offense and a mean of 3 offenses. The middle 50\% of the defendants have committed between 0 and 4 offenses, and the defendant with the most prior offenses has been convicted of committing 38 crimes.

Of the 9387 total defendants from Broward County, Florida, in 2013 and 2014, 79.4\% were male. About 21.4\% are \textless{} 25 years old, 57.2\% are between 25 and 45 years old, and
21.4\% are \textgreater{} 45 years old. The majority of defendants are single (76.7\%), with the next largest categories being married (12.1\%) and divorced (4.2\%). The other levels were significant other, separated, unknown, and widowed, respectively. Finally, 87.9\% of defendants had no record of juvenile offenses. The remaining 12.1\% ranged from having 1 juvenile offense to 21 juvenile offenses on record, with a median of 1, a mean of 1.96, and the middle 50\% recording 1 or 2 offenses.

\hypertarget{response}{%
\subsection{Response Variable}\label{response}}

The response variable records whether or not a defendant recommitted any crime within two years. About two-thirds (n = 6199) of the defendants did not recommit a crime within two years, while about one-third (n = 3188) did. The different proportion of observations in each level indicates class imbalance, which often affects the performance of machine learning classification algorithms. Given the class imbalance, analyzing performance relative to the classes will be important when assessing model performance in the proceeding sections.

\hypertarget{protectedattribute}{%
\subsection{Protected Attribute}\label{protectedattribute}}

Finally, the protected attribute in this analysis is race. Most of the defendants are African-American (49.8\%) and Caucasian (34.6\%), with only 8.7\% Hispanic, 0.5\% Asian, and finally, 0.3\% Native American. The remaining 6.1\% of defendants identify as `Other'.

\hypertarget{bivariateanalysis}{%
\subsection{Associations Between the Predictors and Response}\label{bivariateanalysis}}

With a thorough understanding of the variables, this section examines the relationship between the predictive variables and the response variable, recidivism.

Defendants who recidivated tended to be younger than than those who didn't (median age 29 versus 33, respectively; Figure \ref{fig:ch3fig5}A and \ref{fig:ch3fig6}A). Defendants who recidivated also tended to have more non-juvenile prior offenses than those who didn't recidivate (median prior offenses 3 versus 1, respectively; Figure \ref{fig:ch3fig5}B).
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig5-1} 

}

\caption{Distribution of Age (panel A) and Number of Prior Offenses (panel B) by Recidivism Status among 9,387 Defandants in Broward County Florida, 2013-2014}\label{fig:ch3fig5}
\end{figure}
Sex and marital status were similar between defendants who did and did not recidivate. Defendants who recidivated were more likely to have had a juvenile offense, although the vast majority of defendants in both cases had no juvenile offenses (Figure \ref{fig:ch3fig6}B).
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig6-1} 

}

\caption{Conditional Distribution of Age (panel A) and Juvenile Offense (panel B) by Recidivism Status among 9,387 Defandants in Broward County Florida, 2013-2014}\label{fig:ch3fig6}
\end{figure}
\hypertarget{multivariate-analysis}{%
\subsection{Multivariate Analysis}\label{multivariate-analysis}}

As detailed in Section \ref{bivariateanalysis}, some predictive variables are associated with the response variable, recidivism. A scatterplot matrix examining the relationship between the continuous variables, age and prior offenses, revealed weak correlations (Spearman's rho: \(\rho = 0.12\)) and nonlinear relationships. There are no significant concerns for multicollinearity. Figure \ref{fig:ch3fig7} illustrates this using Spearman's correlation. In addition, the correlation matrix incorporates the COMPAS tool decile scores to examine the predictive relationship between the continuous variables and the COMPAS tool results. Spearman's correlation revealed a moderate relationship of \(\rho = -0.44\) and \(\rho = 0.44\) for age and prior offenses, respectively.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig7-1} 

}

\caption{Spearman's Correlation Matrix for Age, Number of Prior Offenses, and the COMPAS Tool Decile Scores}\label{fig:ch3fig7}
\end{figure}
With a thorough understanding of the variables, Section \ref{fairnessanalysis} analyzes the data set and the COMPAS tool decile scores with a focus on fairness and an aim to examine the fairness concerns discussed in Chapters \ref{intro} and \ref{chap-2}.

\hypertarget{fairnessanalysis}{%
\section{Fairness and Demographic Analysis}\label{fairnessanalysis}}

Chapter \ref{algbias} introduced algorithmic bias as a situation when an algorithm's decisions are skewed towards a particular group of people, either positively or negatively. With race as the protected attribute, this section aims to examine the COMPAS tool results, the underlying proxy relationships between the variables and the sensitive attribute, race, and create a table similar to the one in Figure \ref{fig:compas1}, which highlights the discrepancy in false positive and false negative rates of the COMPAS tool for Black and White defendants.

\hypertarget{comptoolanalysis}{%
\subsection{COMPAS Tool Analysis}\label{comptoolanalysis}}

Recall that COMPAS decile scores of 1 to 4 are mapped as `low' risk, 5 to 7 as `medium' risk, and 8 to 10 as `high' risk. The median decile score for the entire data set is four, and Figure \ref{fig:ch3fig8} illustrates that the COMPAS tool classifies more than half of the defendants as low risk. In particular, the tool classifies 57.2\% as low risk and 17.9\% as high risk, with the remaining 24.9\% as medium risk. These predictions align with expectations since most defendants did not recommit a crime within the two-year time window, as observed in Section \ref{response}.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig8-1} 

}

\caption{Distribution of COMPAS Tool Decile Scores among 9,387 Defandants in Broward County Florida, 2013-2014}\label{fig:ch3fig8}
\end{figure}
However, when these scores are broken down by whether the defendant reoffended within two years, it is revealed that the COMPAS tool performs poorly in classifying participants who recidivate. Such participants are classified almost equally into the three risk categories: low, medium, and high. Furthermore, while most defendants who do not reoffend within two years are classified as low risk, a significant number of them are classified as `medium' and `high' risk. The decile scores of defendants who recidivate and those who do not also range from 1 to 10, although the median decile score is 6 for the former group and 3 for the latter group. Generally, these results raise concerns about the COMPAS tool's predictive performance.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig9-1} 

}

\caption{Distribution of COMPAS Tool Risk Scores among 9,387 Defandants in Broward County Florida, 2013-2014}\label{fig:ch3fig9}
\end{figure}
A breakdown by race, the protected attribute, will allow for further analysis of the performance of the COMPAS tool. Before that, converting this into a binary prediction problem will make analysis much more tractable. Decile scores of 1 to 5 will be mapped as `lower' risk, while those ranging from 6 to 10 will be mapped as `higher' risk.

Table \ref{tab:ch3table1} displays the confusion matrix of the COMPAS tool's results on this data set, which has 9387 total observations, with this binary definition of risk. The matrix reveals that the model has an overall accuracy of 66.61\% on this data set. However, 66.04\% of defendants do not recommit a crime within two years, as observed in Section \ref{response}. Therefore, a blind non-informative model that classifies every defendant into the negative class would attain an accuracy of 66\%, suggesting that the COMPAS tool's 66.61\% accuracy is a negligible improvement.

Notice, furthermore, that the model struggles more in predicting whether defendants will reoffend than in predicting whether defendants will not reoffend. This imbalance in error rates is expected because of the class imbalance observed when performing exploratory data analysis in Section \ref{descriptivestats} -- most defendants do not reoffend, so the model maximizes performance for those defendants. In terms of proportions, 49.25\% of defendants who reoffended are incorrectly labeled as lower risk (almost equivalent to a flip of a coin), compared to 25.23\% of defendants who did not reoffend but are incorrectly labeled as higher risk. This also raises the question of what type of prediction is more important: the risk of recidivism or the risk of non-recidivism. Is wrongly attributing a defendant as higher risk or wrongly attributing a defendant as lower risk worse for society?
\begin{table}

\caption{\label{tab:ch3table1}The Number of Defendants who were Labeled Higher/Lower Risk by the COMPAS Tool Stratified by Recidivism Status among 9,387 Defandants in Broward County Florida, 2013-2014}
\centering
\begin{tabular}[t]{lrr}
\toprule
Risk & Reoffended & Did Not Reoffend\\
\midrule
Higher & 1618 & 1564\\
Lower & 1570 & 4635\\
\bottomrule
\end{tabular}
\end{table}
\(\\\)

While the confusion matrix in Table \ref{tab:ch3table1} relays information on what types of errors the COMPAS tool tends to make and raises questions about the implications of that, Table \ref{tab:ch3table2} breaks that down further by race for a more granular assessment. While the overall FPR is 25.3\%, it is much higher for Black defendants (37.71\%) and much lower for White defendants (16.34\%). Similarly, while the overall FNR is 49.25\%, it is much higher for White defendants (62.26\%) and much lower for Black defendants (39.01\%) (Table \ref{tab:ch3table2}). The tool is more than twice as likely to classify a Black defendant who did not reoffend as higher risk compared to a White defendant. It makes the opposite mistake, where it is 1.6 times more likely to classify a White defendant who reoffended as lower risk compared to a Black defendant. This discrepancy aligns with ProPublica's findings in Figure \ref{fig:compas1} and is alarming given that race was not included in the model.

Including the other races reveals that Asian defendants who did not reoffend were the least likely to be labeled as higher risk -- Black defendants were the most likely. Conversely, White defendants who reoffended were the most likely to be labeled as lower risk -- Native Americans were the least likely. However, it is essential to consider the few observations in both the Asian (n = 48) and Native American groups (n = 27), as detailed in Section \ref{protectedattribute}.
\begin{table}

\caption{\label{tab:ch3table2}The Percent of Defendants who were Incorrectly Labeled Higher/Lower Risk by the COMPAS Tool Stratified by Recidivism Status and Race among 9,387 Defandants in Broward County Florida, 2013-2014}
\centering
\begin{tabular}[t]{llrrrrr}
\toprule
Recidivism Status & Predicted Risk & Black & White & Hispanic & Asian & Native American\\
\midrule
Did Not Reoffend & Higher & 37.7 & 16.3 & 14.3 & 7.89 & 21.1\\
Reoffended & Lower & 39.0 & 62.3 & 68.3 & 50.00 & 25.0\\
\bottomrule
\end{tabular}
\end{table}
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig11-1} 

}

\caption{Distribution of COMPAS Tool Decile Scores Stratified by Race among 9,387 Defandants in Broward County Florida, 2013-2014}\label{fig:ch3fig11}
\end{figure}
The analysis in this section provides evidence for disparities with favorable COMPAS outcomes for White and Asian defendants and unfavorable outcomes for Black and Native American defendants. To visualize these results, Figure \ref{fig:ch3fig11} displays the distribution of the decile scores for the two most populous races in the data set. While there is a similar distribution of recidivism for all races with most defendants not recidivating, Black defendants' decile scores are distributed almost uniformly among the ten decile scores. In contrast, the White defendants' decile scores have a significant right skew, with most observations in lower decile scores. This further emphasizes the racial disparity in employing these risk scores in judicial decisions and the present algorithmic bias.

\hypertarget{proxy-relationships}{%
\subsection{Proxy Relationships}\label{proxy-relationships}}

Section \ref{comptoolanalysis} revealed that the predictions of the COMPAS tool exhibit significant racial discrepancies. Yet, the COMPAS tool is a race-blind model. Race was not included as a variable, so how could a model result in such racially distinct outcomes? To answer this question, this section analyzes the relationship between race and the predictive variables to examine how much information regarding a defendant's race is incorporated into the model via other variables.

There are more male than female defendants for all races, and most defendants are single. However, African-American defendants tend to be, on average, the youngest compared to all the other races. Asian defendants, followed by Caucasian defendants, tend to be the oldest. Nevertheless, there is considerable overlap among all the races, and Figure \ref{fig:ch3fig10} visualizes the relationships. In looking at the age categories by race, more African-American defendants are \textless{} 25 than those who are \textgreater{} 45. The converse is true for Caucasians, with more defendants that are \textgreater{} 45 in comparison to those \textless{} 25. These distributional differences illustrate that there is some relationship between age and race in this data set, particularly for Black versus White defendants.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig10-1} 

}

\caption{Distribution of COMPAS Tool Proxy Variables Stratified by Race among 9,387 Defandants in Broward County Florida, 2013-2014}\label{fig:ch3fig10}
\end{figure}
Additionally, African-American defendants have the most prior offenses, on average, followed by their Native-American counterparts. When compared with Caucasian defendants, African Americans have almost twice as many prior offenses, suggesting a strong proxy relationship between race and prior offenses. Asian defendants have the least prior offenses. This is an important result that illustrates how a system that predisposes certain races to prison can perpetuate that discriminatory trend by using those same variables to predict the risk of committing another crime. The boxplot in Figure \ref{fig:ch3fig10} helps to visualize this relationship more clearly.

This section illustrates that the protected attribute, race (\(A\)), has proxy relationships with some predictor variables (\(X\)). Even a group-blind classifier will not be entirely blind to race because of the correlations present and the information it gains about race from the proxy variables.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig13-1} 

}

\caption{Distribution of Recidivism Prevalence Stratified by Race among 9,387 Defandants in Broward County Florida, 2013-2014}\label{fig:ch3fig13}
\end{figure}
Most defendants do not recommit another crime, even when broken down by race as visualized in Figure \ref{fig:ch3fig13}. Because the class imbalance is in the same direction, a `fair' model should not misclassify defendants in different directions based on race. However, the nature of the relationship between race and the predictor variables results in discriminatory and unfair outcomes, as elucidated in Section \ref{comptoolanalysis}.

Before employing the Seldonian framework on this data set, Section \ref{logreg} fits a logistic regression to assess how similarly the COMPAS tool performs to a standard ML procedure.

\hypertarget{logreg}{%
\section{Logistic Regression}\label{logreg}}

Logistic regression is a statistical generalized linear model (GLM) specifically designed to predict dichotomous/ binary outcomes. In this case, logistic regression was used to model the probability of a defendant re-committing a crime within two years in Broward County, Florida, using the COMPAS data set. With a cutoff of 0.5, the resulting probabilities were divided into two bins: lower risk (\(p < 0.5\)) and higher risk (\(p >= 0.5\)). Given that logistic regression is one of the most widely used classification algorithms, this analysis will provide insights into how state-of-the-art traditional algorithms that follow the standard ML procedure described in Chapter \ref{standardml} and do not consider fairness guarantees may perform on this data set.

\hypertarget{fitting-the-logistic-regression-model}{%
\subsection{Fitting the Logistic Regression Model}\label{fitting-the-logistic-regression-model}}

Recall that if every observation were classified in the majority class, an accuracy of 66\% would be expected. This is a benchmark for the race-blind logistic regression model implemented in this section. The model will be trained on 70\% of the data and evaluated on the remaining 30\%. Only the five predictors (sex, age, marital status, juvenile offense, and prior offenses) will be fit to predict recidivism, the R code for which is displayed in Appendix \ref{appendix-d}.

The accuracy was 70.2\% on the training set, which is \textasciitilde3\% higher than the COMPAS tool's predictions and suggests that \textasciitilde30\% of defendants are misclassified. Overall accuracy was consistent at 70.1\% when evaluated on the testing set, indicating that the model did not over-fit on the training set and had good generalization performance. However, the ROC curve on Figure \ref{fig:ch3fig12} indicates concerns about the model's sensitivity \((1 - FNR)\) and specificity \((1 - FPR)\). The diagonal line shows how the model would perform with random predictions. Curves that budge closer to the top left are preferred because those types of curves maximize the area under the curve (AUC), and the sensitivity and specificity of the model are closer to 100\%. However, the AUC of this model is 68.8\%.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch3fig12-1} 

}

\caption{COMPAS Logistic Regression Receiver Operating Characteristic (ROC) Curve}\label{fig:ch3fig12}
\end{figure}
\hypertarget{evaluating-the-equality-of-odds}{%
\subsection{Evaluating the Equality of Odds}\label{evaluating-the-equality-of-odds}}

Despite the improved accuracy, examining the direction of these error rates and performing a demographic analysis by race to assess the model's `fairness' along racial lines reveals the same discrepancies observed in the COMPAS tool results.

Table \ref{tab:ch3table3} displays the overall false positive and false negative rates. As expected, the model performs better in classifying defendants who do not reoffend than those who do since that is the majority class. When evaluated on the test set, 91.05\% of defendants who did not reoffend are correctly classified as low-risk. Thus, only 8.95\% of participants who do not reoffend are incorrectly classified as high-risk. This is a lower FPR than COMPAS. However, only 29\% of defendants who reoffended are correctly classified as high-risk. Thus, 71\% of defendants who reoffended are incorrectly classified as low-risk. This is a higher FNR than COMPAS.

Table \ref{tab:ch3table4} further breaks these results down, focusing on Black and White defendants. While the overall FPR for the model is 8.95\%, notice that the FPR for Black defendants is 13.93\%, compared to only 5.09\% for White defendants. Similar to the COMPAS tool, Black defendants who do not reoffend are incorrectly misclassified as high risk at twice the rate that White defendants are. On the flip side, while the overall FNR for the model is 71\%, notice that the FNR for Black defendants is 60.82\%, compared to 86.17\% for White defendants. Similar to the COMPAS tool, the logistic regression makes the opposite mistake in predicting recidivism for Black v White defendants, with White defendants being more likely to be incorrectly classified as low risk than their Black counterparts.

These results serve to highlight the real danger with using standard ML procedures, even when the sensitive variable itself is not included. These models have potential to perpetuate, and even introduce, harmful and discriminatory practices as observed in this Chapter. Although the details of Northepointe's COMPAS algorithm are kept secret, it's clear that traditional algorithms like logistic regression lead to comparable outcomes. In fact, a probability cutoff of \(p = 0.34\), the probability of being in the positive class in this data set, on the logistic regression yielded the same accuracy (\(66.7\)\%) as the COMPAS tool. Additionally, comparing the COMPAS decile scores to the logistic regression (LR) results reveals that the median decile score is 8 in the high-risk LR prediction and 3 in the low-risk LR prediction, further highlighting the similarity of these results. The next section uses the logistic regression model as a starting point, but places fairness constraints to enforce equality of odds for Black and White defendants.
\begin{table}

\caption{\label{tab:ch3table3}COMPAS Logistic Regression Error Rates (Percentage)}
\centering
\begin{tabular}[t]{lrr}
\toprule
Risk & Reoffended & Did Not Reoffend\\
\midrule
High & 29 & 8.95\\
Low & 71 & 91.05\\
\bottomrule
\end{tabular}
\end{table}
\begin{table}

\caption{\label{tab:ch3table4}The Percent of Defendants who were Incorrectly Labeled Higher/Lower Risk by Logistic Regression Stratified by Recidivism Status and Race among 9,387 Defandants in Broward County Florida, 2013-2014}
\centering
\begin{tabular}[t]{llrr}
\toprule
Recidivism Status & Predicted Risk & Black & White\\
\midrule
Did Not Reoffend & High & 13.9 & 5.09\\
Reoffended & Low & 60.8 & 86.17\\
\bottomrule
\end{tabular}
\end{table}
\hypertarget{seldapp}{%
\section{Seldonian Classification}\label{seldapp}}

This section aims to conclude the chapter by illustrating how Seldonian algorithms can mitigate undesirable outcomes made by classifiers in practicality. Consistent with the notation in Chapter \ref{chap-2}, \(\Theta\) would be a set of classifiers and \(f\) a classifier performance measure, such as logistic loss or the log likelihood. For this application, the Seldonian algorithm will use the logistic loss as its primary objective function. The goal is to use the Seldonian framework to create a model that makes predictions about recidivism that are fair with respect to race, which will be simplified to just two levels: Black and White. Further research can extend this work to include more races.

\hypertarget{formulating-the-seldonian-problem}{%
\subsection{Formulating the Seldonian Problem}\label{formulating-the-seldonian-problem}}

Without fairness constraints, the standard ML problem is finding a solution \(\theta\) that minimizes logistic loss. However, such a solution, as illustrated in Section \ref{logreg} when fitting a logistic regression model, results in unequal error rates between Black and White defendants when using the COMPAS data set. Defining the discrimination statistic \(d(\theta)\) to measure the difference in error rates as in Equation \ref{ch3eq2}, then \(d(\theta_{LR}) = 0.34\) (or \(34.18\)\%). Similarly, for the COMPAS tool, \(d(\theta_{COMPAS}) = 0.45\) (or \(44.7\)\%) using this data set. This illustrates that the logistic regression performed slightly better than the COMPAS tool, both in overall accuracy and error rate disparities. However, there is still a significant discrepancy between the error rates for both races.
\begin{equation}
\label{ch3eq2}
d(\theta) = abs[(FPR | \text{Black} - FPR | \text{White}) + (FNR | \text{White} - FNR | \text{Black})]
\end{equation}
\noindent To minimize \(d(\theta)\), \(g(\theta)\) will be defined, for some \(\epsilon\), as:
\begin{equation}
\label{ch3eq3}
g(\theta) = abs[(FPR | \text{Black} - FPR | \text{White}) + (FNR | \text{White} - FNR | \text{Black})] - \epsilon.
\end{equation}
\noindent Recall that a Seldonian algorithm ensures that:
\begin{equation}
\label{ch3eq4}
P(g(\theta) \leq 0) \geq 1 - \delta.
\end{equation}
\noindent The problem can now be fully formulated as a Seldonian machine learning problem. That is, using gradient descent, an optimization algorithm for finding a local minimum of a differentiable function, the Seldonian objective is to minimize logistic loss subject to the constraint:
\begin{equation}
\label{ch3eq5}
P\{abs[(FPR | \text{Black} - FPR | \text{White}) + (FNR | \text{White} - FNR | \text{Black})]  - \epsilon \leq 0 \} \geq 1 - \delta \textit{; } \delta = 0.05.
\end{equation}
\(\delta\) will be set to 0.05 to attain 95\% confidence that separation (equalized odds) as defined in Chapter \ref{classfairdef} is satisfied, that is, \(d(\theta) \leq \epsilon\). The data will be partitioned into a training set that will be passed into the candidate selection mechanism to compute \(\theta_c\). The remaining partition will be used in the safety test to ensure probabilistic satisfaction of the constraint. \(\epsilon\) will be set to four values: \(0.2, 0.1, 0.05, \text{and } 0.01\). The code to obtain a solution is available as part of the \texttt{seldonian-engine} Python library and is displayed in Appendix \ref{appendix-e}, along with the data pre-processing.

\hypertarget{evaluating-performance-and-fairness}{%
\subsection{Evaluating Performance and Fairness}\label{evaluating-performance-and-fairness}}

A solution that passed the safety test was obtained for all four values of \(\epsilon\). However, as \(\epsilon\) got smaller, the logistic loss increased, and the overall accuracy decreased. The accuracy was 68.2\%, 65.59\%, 64.7\%, and 64.4\% respectively for \(\epsilon = 0.2, 0.1, 0.05, 0.01\). Additionally, the models got progressively worse in correctly classifying observations into the positive class, with the final model classifying every observation into the negative class. Each model had higher overall FNR and lower overall FPR than both the COMPAS tool and the logistic regression model. Tables \ref{tab:ch3table5}, \ref{tab:ch3table6}, \ref{tab:ch3table7}, and \ref{tab:ch3table8} break up the FPR and FNR for Black and White defendants in each of these four cases. When \(\epsilon = 0.2, 0.1, 0.05, \text{and } 0.01\), note that \(d(\theta) = 0.22, 0.06, 0.007, \text{and } 0\) respectively.
\begin{table}

\caption{\label{tab:ch3table5}The Percent of Defendants who were Incorrectly Labeled Higher/Lower Risk by a Seldonian Algorithm ($\epsilon$ = 0.2) Stratified by Recidivism Status and Race among 9,387 Defandants in Broward County Florida, 2013-2014}
\centering
\begin{tabular}[t]{llrr}
\toprule
Recidivism Status & Predicted Risk & Black & White\\
\midrule
Did Not Reoffend & High & 8.88 & 2.24\\
Reoffended & Low & 73.82 & 88.82\\
\bottomrule
\end{tabular}
\end{table}
\begin{table}

\caption{\label{tab:ch3table6}The Percent of Defendants who were Incorrectly Labeled Higher/Lower Risk by a Seldonian Algorithm ($\epsilon$ = 0.1) Stratified by Recidivism Status and Race among 9,387 Defandants in Broward County Florida, 2013-2014}
\centering
\begin{tabular}[t]{llrr}
\toprule
Recidivism Status & Predicted Risk & Black & White\\
\midrule
Did Not Reoffend & High & 1.87 & 0.3\\
Reoffended & Low & 93.13 & 97.4\\
\bottomrule
\end{tabular}
\end{table}
\begin{table}

\caption{\label{tab:ch3table7}The Percent of Defendants who were Incorrectly Labeled Higher/Lower Risk by a Seldonian Algorithm ($\epsilon$ = 0.05) Stratified by Recidivism Status and Race among 9,387 Defandants in Broward County Florida, 2013-2014}
\centering
\begin{tabular}[t]{llrr}
\toprule
Recidivism Status & Predicted Risk & Black & White\\
\midrule
Did Not Reoffend & High & 0.29 & 0.39\\
Reoffended & Low & 98.73 & 98.17\\
\bottomrule
\end{tabular}
\end{table}
\begin{table}

\caption{\label{tab:ch3table8}The Percent of Defendants who were Incorrectly Labeled Higher/Lower Risk by a Seldonian Algorithm ($\epsilon$ = 0.01) Stratified by Recidivism Status and Race among 9,387 Defandants in Broward County Florida, 2013-2014}
\centering
\begin{tabular}[t]{llrr}
\toprule
Recidivism Status & Predicted Risk & Black & White\\
\midrule
Reoffended & Low & 100 & 100\\
\bottomrule
\end{tabular}
\end{table}
\(\\\)

In conclusion, the Seldonian algorithms successfully met the fairness constraints defined, with the FPR and FNR gaining more parity along racial lines for Black and White defendants as \(\epsilon\) became more conservative. However, there was a tradeoff with model informativeness and performance in enforcing greater fairness. The FNR got larger as the FPR got smaller until, ultimately, the model was non-informative despite perfectly satisfying the constraints (100\% FNR and 0\% FPR for both races). Accuracy also dropped as the value of \(\epsilon\) decreased, although the original data set could have been more informative. Notably, considering all races, the lowest achievable accuracy would only be 4\% lower than the highest accuracy attained by the logistic regression model.

Looking ahead, there are several avenues for future research. One direction involves extending this study to data sets with less class imbalance and higher overall predictive performance. By doing so, the tradeoffs inherent in incorporating fairness constraints into the traditional objective function in a practical setting can be better elucidated. Chapter \ref{chap-4} contributes to this understanding by conducting a simulation study to explore the theoretical and practical implications of the Seldonian framework in the classification setting.

\hypertarget{chap-4}{%
\chapter{Seldonian Algorithms for Classification: A Simulation Study}\label{chap-4}}

Chapter \ref{chap-2} introduced the Seldonian framework, which offers probabilistic guarantees for satisfying defined behavioral constraints. However, the toy linear regression example demonstrated some of the limitations of Seldonian algorithms, particularly in scenarios with limited sample sizes where convergence may pose a challenge. In practice, sample sizes vary. Furthermore, Chapter \ref{chap-3} applied the Seldonian framework to a classification setting using the COMPAS data set. The goal was to produce recidivism predictions that elicit fairer outcomes along racial lines, with a specific focus on Black and White defendants. However, the application illustrated that while the behavioral constraint was satisfied in all four cases, there was a tradeoff in the model's accuracy. In other words, a non-informative model can be perfectly Seldonian, but what value does a theoretically fair non-informative model add?

To address this concern and further study these tradeoffs, this chapter investigates the efficacy and applicability of Seldonian algorithms in practical classification settings with class balance and better predictive performance than COMPAS. By conducting a simulation study, the aim is to evaluate whether Seldonian approaches can effectively produce fairer outcomes and mitigate discriminatory tendencies, drawing on the fairness notion of separation (or equality of odds) defined in Chapter \ref{fairnessdefinitions} and set up in Chapter \ref{seldapp}. Specifically, the objective is to assess the feasibility of leveraging Seldonian algorithms to enhance the fairness and equity of predictive models across various practical classification tasks.

\hypertarget{sim-design}{%
\section{Simulation Design}\label{sim-design}}

Before conducting the simulation study and analyzing the results, this section provides detailed explanations of the simulation set-up and design.

\hypertarget{aims}{%
\subsection{Aims}\label{aims}}

This simulation study presents a proof of concept for using the Seldonian framework in classification problems with robust predictive performance. Seldonian algorithms \(\textit{can}\) fail, especially with insufficient data, as elucidated in Chapter \ref{toy}. On the flip side, as elucidated in Chapter \ref{seldapp}, Seldonian algorithms can succeed in their objective for fairer outcomes but fail to produce a useful model. Solutions returned are also probabilistic and may not always satisfy the constraint despite passing the safety test. With these limitations in mind, this simulation study aims to empirically assess the predictive performance of Seldonian algorithms, compared to that of the standard ML approach, in practical classification settings.

\hypertarget{data-gen}{%
\subsection{Data-Generation Mechanism}\label{data-gen}}

Given that this is a proof-of-concept simulation study, the data-generation mechanism will follow a realistic design. Two of the most informative variables from the COMPAS data set were selected for a simpler set-up: the defendant's age (continuous) and whether or not the defendant had committed any prior offenses (binary). The choice to use the COMPAS data set as a starting point was so that the complex relationships between the predictor variables (\(X\)) and the protected attribute (\(A\)) may be retained, especially given that the data was collected for a practical application and elucidates relationships that may be expected in the real world. There are only two levels of the protected attribute, race, that were retained for the study: Black and White.

To achieve better predictive performance, a linear combination of age and prior offenses was chosen after searching through a range of plausible values: \(logit(p_i) = 5 - 0.2 \text{ }Age_i + 0.5 \text{ } \textit{PriorOffense}_i \text{ | } i \in \{1,2, \ldots, 9387\}\), where \(p_i\) is the probability of a defendant recommitting a crime within two years. The response variable -- whether or not a defendant recommitted a crime within two years -- was then drawn from a Bernoulli distribution with probability of reoffense equal to \(p_i\) for each defendant. Finally, class balance was induced on the data set by randomly sampling, with replacement, 1250 observations from each of the four intersections of race and recidivism status: Black and White defendants who recidivated and did not recidivate. Note that this scheme (Appendix \ref{appendix-f}) assumes equal prevalence of the response variable, \(Y\), for both levels of the protected attribute.

The parent simulation data set, thus, contains 5000 observations, achieves perfect class balance, and has a baseline logistic regression accuracy of 77.96\%. This offers a realistic improvement from the accuracy of 70.2\% obtained from the COMPAS logistic regression. However, because of the class balance, the lowest achievable accuracy is now 50\% (a random/ coin-flip model) rather than 64.4\%. This will allow more room for the Seldonian algorithm model accuracies to vary.

One thousand total data sets of size \(n = 500\), \(n = 1000\), \(n = 2500\), and \(n = 5000\) will be sampled, with replacement, from this parent data set for the simulation study, with 250 data sets in each of the four partitions.

\hypertarget{target}{%
\subsection{Target}\label{target}}

The target of this simulation study is prediction, that is, to evaluate Seldonian algorithms for predictive performance in classification settings.

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

Because the Seldonian framework was designed to place probabilistic fairness constraints on traditional algorithms, solutions produced by Seldonian algorithms will be compared to those produced through the standard ML framework, specifically logistic regression, to assess the predictive performance and feasibility of the Seldonian framework.

Drawing from the fairness definitions described in Chapter \ref{fairnessdefinitions} with a particular focus on the COMPAS example in Chapter \ref{chap-3}, it may be less important that a model satisfies independence. That is, that the model, on average, has the same likelihood of a positive prediction for all levels of the protected attribute. Instead, it may be more realistic to expect that the model be equally wrong or equally correct in its predictions of \(Y\) for each protected attribute. For this reason, one fairness constraint will be set on the Seldonian algorithms to satisfy separation, otherwise known as equality of the error rates or equality of odds, as defined in Equation \ref{ch4eq1} within some margin \(\epsilon\) and with \(1 - \delta\) \% confidence. Similar to Chapter \ref{seldapp}, \(\epsilon\) will be set to four levels: \(0.2, 0.1, 0.05, \text{and } 0.01\). \(\delta\) will be set to \(0.05\) to guarantee 95\% confidence.
\begin{equation}
\label{ch4eq1}
g(\theta): abs[(FPR | \text{A = a} - FPR | \text{A = b}) + (FNR | \text{A = a} - FNR | \text{A = b})] - \epsilon.
\end{equation}
Simulated data sets will be generated independently before being fed into a logistic regression algorithm and the four Seldonian algorithms. The relevant performance measures, as detailed in Section \ref{performancemeasures}, will be recorded for each trial. Two hundred and fifty trials will be run for each sample size as described in Section \ref{data-gen}. The code will be run in Python using the Rstudio software interface and the Amherst College High-Performance Computing System. The \texttt{seldonian} package is readily available for installation in Python and is equipped with functions that allow for the implementation of Seldonian algorithms. Other Python packages, such as \texttt{pandas}, \texttt{numpy}, and \texttt{sklearn}, will help conduct the rest of the simulation study.

\hypertarget{performancemeasures}{%
\subsection{Performance Measures}\label{performancemeasures}}

Compared to the logistic regression models, the predictive performance of the Seldonian models will be assessed along three dimensions: the probability of a Seldonian solution, the accuracy of the solutions, and the satisfaction or violation of the behavioral constraints set across all trials within a specific setting (sample size).

It is essential to account for the fact that the Seldonian algorithms will not always return a solution. Recording the probability of a solution being returned in each sample setting will be crucial for evaluating the practical feasibility of this framework. Additionally, for fair statistical comparisons, the first and second performance measures will be compared only with Seldonian solutions that converge. However, the performance of the logistic regression models in trials where no solutions were found using the Seldonian framework will be analyzed to elucidate learnings about the nature of those trials.

Additionally, for each simulation trial, the overall accuracy of both the convergent Seldonian models and the logistic regression model will be recorded and eventually averaged over the number of data sets in the specific sample size. Both the mean and the standard error will be reported in tabular format and visualized graphically to compare the models' predictive performances and, potentially, evaluate the trade-offs that may occur by employing the Seldonian framework and enforcing behavioral constraints.

Finally, the satisfaction or violation of the behavioral constraint identified in Equation \ref{ch4eq1} will be assessed in two ways. First, for each sample size as described above, a count of the times both frameworks satisfied the behavioral constraint, by some margin \(\epsilon\), will be reported in tabular format. Additionally, the discrimination statistics as defined in Equation \ref{ch4eq2} will be recorded for each simulation trial. The mean and standard error will be reported and visualized for each sample setting to compare the magnitude and direction of the models' unfairness with regard to the separation fairness definition.
\begin{equation}
\label{ch4eq2}
d(\theta) = abs[(FPR | \text{A = a} - FPR | \text{A = b}) + (FNR | \text{A = a} - FNR | \text{A = b})]
\end{equation}
\hypertarget{sim-results}{%
\section{Simulation Results}\label{sim-results}}

As described in Section \ref{sim-design}, the simulation results will be analyzed along 3 key performance measures: convergence, discrimination, and accuracy.

\hypertarget{probability-of-a-seldonian-solution}{%
\subsection{Probability of a Seldonian Solution}\label{probability-of-a-seldonian-solution}}

All logistic regression trials are expected to return a solution, thus a 100\% convergence rate. However, while all the Seldonian trials will propose a candidate solution predicted to pass the safety test with 95\% confidence, not all candidate solutions will pass the safety test itself. Table \ref{tab:ch4tab1} illustrates this. For \(\epsilon = 0.2, 0.1, 0.05\), the probability of a Seldonian solution was \(> \text{~}90\)\% for all sample sizes. In fact, for sample sizes \(n = 1000, 2500, 5000\), the probability of a Seldonian solution was \(> 96\)\%, suggesting a higher convergence rate for looser fairness constraints and data set sizes greater than \(n = 500\). This is consistent with the results in Figure 2.6 from Chapter \ref{exp}. Regardless, a small data set size did not matter for the loosest constraint of \(\epsilon = 0.2\).

However, when \(\epsilon = 0.01\), which was the tightest constraint, the probability of a solution dropped significantly, and there were no consistent trends across sample sizes, suggesting that the models had trouble passing the safety test in this case, regardless of sample size, likely because the constraint was too strict and not feasible for this data set. Surprisingly, the lowest observed probability of a solution (\(24.8\)\%) is recorded when \(\epsilon = 0.01, n = 5000\).

Overall, across all sample sizes, the probability of a solution generally reduced as the constraint tightened.
\begin{table}

\caption{\label{tab:ch4tab1}Probability of Obtaining a Seldonian Solution}
\centering
\begin{tabular}[t]{rrrrrr}
\toprule
Sample Size & LR & SA (0.2) & SA (0.1) & SA (0.05) & SA (0.01)\\
\midrule
500 & 100 & 99.6 & 93.2 & 89.6 & 71.2\\
1000 & 100 & 100.0 & 98.8 & 97.6 & 86.8\\
2500 & 100 & 99.6 & 100.0 & 98.8 & 56.8\\
5000 & 100 & 99.6 & 99.6 & 96.4 & 24.8\\
\bottomrule
\end{tabular}
\end{table}
\(\\\)

However, the probability of a solution that passes the safety test is not the end of the story. Table \ref{tab:ch4tab2} records the proportion of solutions that satisfied the behavioral constraint among those that passed the safety test (Table \ref{tab:ch4tab1}). When \(\epsilon = 0.01\), although a low probability of solution was previously observed, \(> 92\)\% of returned solutions satisfied the behavioral constraint. Additionally, the probability was largest in the \(n = 5000\) case, which previously had the lowest probability of returning a solution across the entire simulation study. Except for \(n = 5000\), the probability of a Seldonian solution satisfying the behavioral constraint generally increased as the constraint got tighter, suggesting some trade-off between the probability of returning a solution and the probability of that solution satisfying the behavioral constraints. However, there were no consistent trends as sample size increased across all four levels of \(\epsilon\), although the probability actually decreased for \(\epsilon = 0.1, 0.05\). This is inconsistent with expectations, but perhaps the increased variability introduced by more observations makes it more difficult to constrain the models' unfairness.
\begin{table}

\caption{\label{tab:ch4tab2}Satisfaction of the Behavioral Constraint by Seldonian Solutions that Passed the Safety Test}
\centering
\begin{tabular}[t]{rrrrr}
\toprule
Sample Size & SA (0.2) & SA (0.1) & SA (0.05) & SA (0.01)\\
\midrule
500 & 83.5 & 88.0 & 88.4 & 93.8\\
1000 & 85.2 & 83.4 & 88.1 & 93.1\\
2500 & 56.2 & 63.2 & 76.1 & 92.2\\
5000 & 90.8 & 24.9 & 61.4 & 95.2\\
\bottomrule
\end{tabular}
\end{table}
\(\\\)

Finally, Figure \ref{fig:ch4fig1} visualizes these results to elucidate the convergence of Seldonian algorithms better. As discussed, the probability of a solution passing the safety test (light blue) generally decreases as the constraint gets tighter, regardless of sample size. Notably, the decrease is most drastic for the tightest constraint (\(\epsilon = 0.01\)), which has a better probability of a solution for smaller sample sizes than larger sample sizes. However, looking at the proportion of solutions that both pass the safety test and satisfy the defined behavioral constraint (dark blue) raises concerns about the efficacy of Seldonian algorithms, especially for the \(n = 2500, 5000\) cases.
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch4fig1-1} 

}

\caption{Probability of Returning a Solution and Satisfying the Constraint by Sample Size}\label{fig:ch4fig1}
\end{figure}
\(\\\)

The following three sections further assess the discrimination and accuracy of the Seldonian models that passed the safety test.

\hypertarget{discrimination}{%
\subsection{Discrimination}\label{discrimination}}

Seldonian algorithms are a proposed method to set probabilistic fairness constraints on traditional ML algorithms, so it is of utmost importance that they produce fairer results. Table \ref{tab:ch4tab3} displays the average discrimination statistic, defined in Equation \ref{ch4eq2}, for all combinations of \(n\) and \(\epsilon\). Observe that the mean of \(d(\theta_{LR})\) was consistently at 0.24 for all sample sizes. However, the mean discrimination statistic is lower in all Seldonian algorithms. Better yet, the mean discrimination statistic is equal to or lower than the defined fairness constraint in all cases except the \(\epsilon = 0.1, n = 5000\) case where the mean of \(d(\theta) = 0.14\). Another important observation is that, in most cases, the mean discrimination statistic tended to stay constant or increase as the sample size increased, supporting previous findings that higher sample sizes have a more difficult time constraining model fairness. Overall, the Seldonian algorithms met the defined fairness constraints on average. However, this is just an average, and the standard deviation values in Table \ref{tab:ch4tab3}, as well as the interquartile range (IQR) and outlying observations visualized in Figure \ref{fig:ch4fig2}, illustrate the previous findings that are not all Seldonian algorithms satisfy the behavioral constraint.
\begin{table}

\caption{\label{tab:ch4tab3}Mean Discrimination Statistic of Convergent Seldonian Solutions}
\centering
\begin{tabular}[t]{rrrrrrrrrrr}
\toprule
Sample Size & LR & sd & SA (0.2) & sd  & SA (0.1) & sd   & SA (0.05) & sd    & SA (0.01) & sd     \\
\midrule
500 & 0.24 & 0.07 & 0.04 & 0.07 & 0.02 & 0.06 & 0.02 & 0.06 & 0.01 & 0.05\\
1000 & 0.24 & 0.05 & 0.09 & 0.07 & 0.02 & 0.06 & 0.02 & 0.05 & 0.01 & 0.03\\
2500 & 0.24 & 0.03 & 0.18 & 0.06 & 0.09 & 0.06 & 0.02 & 0.04 & 0.01 & 0.03\\
5000 & 0.24 & 0.02 & 0.12 & 0.06 & 0.14 & 0.08 & 0.05 & 0.05 & 0.00 & 0.02\\
\bottomrule
\end{tabular}
\end{table}
\begin{figure}

{\centering \includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/ch4fig2-1} 

}

\caption{The Distribution of the Discrimination Statistic of Convergent Seldonian Solutions by Sample Size}\label{fig:ch4fig2}
\end{figure}
\hypertarget{accuracy}{%
\subsection{Accuracy}\label{accuracy}}

\hypertarget{the-trade-off-between-discrimination-and-accuracy}{%
\subsection{The Trade-Off Between Discrimination and Accuracy}\label{the-trade-off-between-discrimination-and-accuracy}}

reference n = 5000, e = 0.01 case

\hypertarget{non-convergent-seldonian-models}{%
\subsection{Non-Convergent Seldonian Models}\label{non-convergent-seldonian-models}}

\hypertarget{sim-disc}{%
\section{Discussion}\label{sim-disc}}

\appendix

\hypertarget{appendix-a}{%
\chapter{Sufficiency v Separation Fairness Conflict Equation}\label{appendix-a}}

Recall from Chapter \ref{intro} that (Castelnovo et al., 2022):

\[ PPV = P(Y=1|\hat{Y} = 1),\]
\[ FPR = P(\hat{Y} = 1| Y = 0),\]
\[FNR = P(\hat{Y} = 0| Y = 1),\]
\[ p = P(Y=1).\]

\noindent Using Bayes' rule,

\[PPV = P(Y=1|\hat{Y} = 1) = \frac{P(\hat{Y} = 1|Y=1)P(Y=1)}{P(\hat{Y} = 1|Y=1)P(Y=1) + P(\hat{Y} = 1|Y=0)P(Y=0)} \]
\[\Rightarrow PPV = \frac{P(\hat{Y} = 1|Y=1)p}{P(\hat{Y} = 1|Y=1)p + P(\hat{Y} = 1|Y=0)(1-p)}\]

\[\Rightarrow PPV = \frac{(1-FNR)p}{(1-FNR)p + FPR(1-p)}\]

\[\Rightarrow (1-FNR)p + FPR(1-p) = \frac{(1-FNR)p}{PPV}\]
\[\Rightarrow FPR(1-p) = \frac{(1-FNR)p}{PPV} - (1-FNR)p\]
\[\Rightarrow FPR = \frac{(1-FNR)p}{PPV(1-p)} - \frac{(1-FNR)p}{(1-p)}\]
\[\Rightarrow FPR = \frac{p}{1-p} \left[  \frac{(1-FNR)}{PPV} - (1-FNR) \right]\]
\[\Rightarrow FPR = \frac{p}{1-p} \left[  \frac{(1-FNR) - PPV(1-FNR)}{PPV} \right]\]
\[\Rightarrow FPR = \frac{p}{1-p} \left[  \frac{(1-FNR) (1 - PPV)}{PPV} \right]\]
\[\Rightarrow FPR = \frac{p}{1-p} \frac{1 - PPV}{PPV} (1-FNR) \blacksquare.\]

\noindent A similar equation can be derived relating \(NPV = P(Y=0|\hat{Y} = 0)\) and both FPR and FNR.

\(\\\)

\noindent Additionally, in conventional statistics notation, the sensitivity of a prediction tool can be defined as \(P(\hat{Y}=1|Y=1) = 1 - FNR\) and its specificity can be defined as \(P(\hat{Y}=0|Y=0) = 1 - FPR\). Given a prevalence \(p\), sensitivity \(s_e\), and specificity \(s_p\), then:

\[PPV = \frac{s_ep}{s_ep + (1-s_p)(1-p)}.\]

\noindent Similarly, it can shown that:

\[NPV = \frac{s_p(1-p)}{(1-s_e)p + s_p(1-p)}.\]

\noindent The R code chunk below fixes arbitrary sensitivity (1 - FNR) and specificity (1 - FPR) values to illustrate through the proceeding plots that as prevalence varies, then PPV/ NPV varies and cannot be equal as long as sensitivity and specificity are held constant, hence a conflict.
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(gridExtra)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ppv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(p, sens, spec)\{}
\NormalTok{  ppv }\OtherTok{\textless{}{-}}\NormalTok{ (sens}\SpecialCharTok{*}\NormalTok{p)}\SpecialCharTok{/}\NormalTok{((sens}\SpecialCharTok{*}\NormalTok{p) }\SpecialCharTok{+}\NormalTok{ ((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{spec)}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)))}
  \FunctionTok{return}\NormalTok{(ppv)}
\NormalTok{\}}

\NormalTok{npv }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(p, sens, spec)\{}
\NormalTok{  npv }\OtherTok{\textless{}{-}}\NormalTok{ (spec}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p))}\SpecialCharTok{/}\NormalTok{(((}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{sens)}\SpecialCharTok{*}\NormalTok{p) }\SpecialCharTok{+}\NormalTok{ (spec}\SpecialCharTok{*}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{p)))}
  \FunctionTok{return}\NormalTok{(npv)}
\NormalTok{\}}

\NormalTok{dat\_8080 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{prevalence =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{sens=}\FloatTok{0.80}
\NormalTok{                       , }\AttributeTok{spec=}\FloatTok{0.80}
\NormalTok{                       , }\AttributeTok{ppv =} \FunctionTok{ppv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.80}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.80}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{npv =} \FunctionTok{npv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.80}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.80}\NormalTok{))}

\NormalTok{dat\_9090 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{prevalence =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{sens=}\FloatTok{0.90}
\NormalTok{                       , }\AttributeTok{spec=}\FloatTok{0.90}
\NormalTok{                       , }\AttributeTok{ppv =} \FunctionTok{ppv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.90}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.90}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{npv =} \FunctionTok{npv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.90}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.90}\NormalTok{))}

\NormalTok{dat\_9070 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{prevalence =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{sens=}\FloatTok{0.90}
\NormalTok{                       , }\AttributeTok{spec=}\FloatTok{0.70}
\NormalTok{                       , }\AttributeTok{ppv =} \FunctionTok{ppv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.90}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.70}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{npv =} \FunctionTok{npv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.90}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.70}\NormalTok{))}

\NormalTok{dat\_7090 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{prevalence =} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{sens=}\FloatTok{0.70}
\NormalTok{                       , }\AttributeTok{spec=}\FloatTok{0.90}
\NormalTok{                       , }\AttributeTok{ppv =} \FunctionTok{ppv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.70}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.90}\NormalTok{)}
\NormalTok{                       , }\AttributeTok{npv =} \FunctionTok{npv}\NormalTok{(}\AttributeTok{p=}\FunctionTok{seq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\FloatTok{0.95}\NormalTok{,}\FloatTok{0.05}\NormalTok{), }
                                   \AttributeTok{sens=}\FloatTok{0.70}\NormalTok{, }
                                   \AttributeTok{spec=}\FloatTok{0.90}\NormalTok{))}

\NormalTok{dat\_all }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(dat\_8080, dat\_7090, dat\_9070, dat\_9090) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{sens\_spec =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Sensitivity: "}\NormalTok{, sens, }
                            \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{ Specificity: "}\NormalTok{, spec)}
\NormalTok{         , }\AttributeTok{fpr =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ spec}
\NormalTok{         , }\AttributeTok{fnr =} \DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ sens)}

\NormalTok{g1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(dat\_all, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{prevalence, }\AttributeTok{y=}\NormalTok{ppv)) }\SpecialCharTok{+}
        \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
        \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"prevalence"}\NormalTok{, }\AttributeTok{y=}\StringTok{"positive predictive value"}\NormalTok{,}
             \AttributeTok{title =} \StringTok{"PPV{-}FPR{-}FNR Conflict"}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sens\_spec)}

\NormalTok{g2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(dat\_all, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{prevalence, }\AttributeTok{y=}\NormalTok{npv)) }\SpecialCharTok{+}
        \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
        \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"prevalence"}\NormalTok{, }\AttributeTok{y=}\StringTok{"negative predictive value"}\NormalTok{,}
             \AttributeTok{title =} \StringTok{"NPV{-}FPR{-}FNR Conflict"}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sens\_spec)}

\FunctionTok{grid.arrange}\NormalTok{(g1,g2, }\AttributeTok{nrow=}\DecValTok{1}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{center}\includegraphics{Dasha-Asienga_StatThesis_files/figure-latex/unnamed-chunk-51-1} \end{center}

\hypertarget{appendix-b}{%
\chapter{Quasi-Seldonian Linear Regression Theoretical and Python Implementation}\label{appendix-b}}

The \texttt{reticulate} package is useful for setting up the correct Python environment within RStudio.
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(reticulate)}
\FunctionTok{use\_python}\NormalTok{(}\StringTok{"/cm/shared/apps/amh{-}Rstudio/python{-}3.11.4/bin/python3"}\NormalTok{, }
           \AttributeTok{required =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#py\_config()}
\CommentTok{\#conda\_list()}
\end{Highlighting}
\end{Shaded}
\noindent Python packages not already pre-installed need to be imported into the \texttt{reticulate} package first before being imported into the Python environment, as illustrated using the \texttt{sklearn} package below.
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sklearn }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"sklearn"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ sklearn}
\end{Highlighting}
\end{Shaded}
\noindent \texttt{math} provides access to the standard mathematical functions. \texttt{numpy} supports large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. \texttt{sys} provides functions and variables used to manipulate different parts of the Python run-time environment. \texttt{sklearn} features various classification, regression and clustering algorithms. \texttt{scipy.stats} contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more. \texttt{scipy.optimize} provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programing, constrained and nonlinear least-squares, root finding, and curve fitting.
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ math}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ sys}
\ImportTok{from}\NormalTok{ sklearn.linear\_model }\ImportTok{import}\NormalTok{ LinearRegression}
\ImportTok{from}\NormalTok{ sklearn.model\_selection }\ImportTok{import}\NormalTok{ train\_test\_split}
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ t}
\ImportTok{from}\NormalTok{ scipy.optimize }\ImportTok{import}\NormalTok{ minimize}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# display 5 decimal places for all results}
\NormalTok{np.set\_printoptions(precision}\OperatorTok{=}\DecValTok{5}\NormalTok{, suppress}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{tinv} function returns the inverse of \texttt{Student\textquotesingle{}s\ t} CDF using the \texttt{nu} degrees of freedom for the corresponding probabilities \texttt{p}.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ tinv(p, nu):}
    \ControlFlowTok{return}\NormalTok{ t.ppf(p, nu)}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{stddev} function computes the sample standard deviation of the vector \(v\), with Bessel's correction. In statistics, Bessel's correction is the use of \(n-1\) instead of \(n\) in the formula for the sample variance and sample standard deviation, where n is the number of observations in a sample. This method corrects the bias in the estimation of the population variance.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ stddev(v):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ v.size}
\NormalTok{    variance }\OperatorTok{=}\NormalTok{ (np.var(v) }\OperatorTok{*}\NormalTok{ n) }\OperatorTok{/}\NormalTok{ (n}\OperatorTok{{-}}\DecValTok{1}\NormalTok{) }
    \ControlFlowTok{return}\NormalTok{ np.sqrt(variance) }
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{ttestUpperBound} function computes a (\(1 - \delta\))-confidence upper bound on the expected value of a random variable using Student's t-test. It analyzes the data in \(v\), which holds i.i.d. samples of the random variable. The upper confidence bound is given by \(\text{sampleMean} + \frac{\text{sampleStandardDeviation}}{\sqrt(n)} * tinv(1-\delta, n-1)\), where \(n\) is the number of observations in \(v\).
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ ttestUpperBound(v, delta):}
\NormalTok{    n  }\OperatorTok{=}\NormalTok{ v.size}
\NormalTok{    res }\OperatorTok{=}\NormalTok{ v.mean() }\OperatorTok{+}\NormalTok{ stddev(v) }\OperatorTok{/}\NormalTok{ math.sqrt(n) }\OperatorTok{*}\NormalTok{ tinv(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ delta, }
\NormalTok{    n }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ res}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{predictTTestUpperBound} function works similarly to \texttt{ttestUpperBound}, but returns a more conservative upper bound. This function uses data in the vector \(v\) to compute all relevant statistics (mean and standard deviation) but assumes that the number of points being analyzed is \(k\) instead of \(|v|\).

This function is used to estimate what the output of \texttt{ttestUpperBound} would be if it were to be run on a new vector, \(v\), containing values sampled from the same distribution as the points in \(v\). The 2.0 factor in the calculation is used to double the width of the confidence interval when predicting the outcome of the safety test in order to make the algorithm less confident/ more conservative.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ predictTTestUpperBound(v, delta, k):}
    
\NormalTok{    res }\OperatorTok{=}\NormalTok{ v.mean() }\OperatorTok{+} \FloatTok{2.0} \OperatorTok{*}\NormalTok{ stddev(v) }\OperatorTok{/}\NormalTok{ math.sqrt(k) }\OperatorTok{*}\NormalTok{ tinv(}\FloatTok{1.0} \OperatorTok{{-}}\NormalTok{ delta, }
\NormalTok{    k }\OperatorTok{{-}} \DecValTok{1}\NormalTok{)}
    \ControlFlowTok{return}\NormalTok{ res}
\end{Highlighting}
\end{Shaded}
\noindent The function \texttt{main()} below is set up to run a simple experiment.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ main():}
\NormalTok{    np.random.seed(}\DecValTok{123}\NormalTok{)  }
\NormalTok{    numPoints }\OperatorTok{=} \DecValTok{5000}   

\NormalTok{    (X,Y)  }\OperatorTok{=}\NormalTok{ generateData(numPoints)  }

\NormalTok{    gHats  }\OperatorTok{=}\NormalTok{ [gHat1, gHat2] }
\NormalTok{    deltas }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{]}

\NormalTok{    (result, found) }\OperatorTok{=}\NormalTok{ QSA(X, Y, gHats, deltas) }
    
    \ControlFlowTok{if}\NormalTok{ found:}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"A solution was found: [}\SpecialCharTok{\%.10f}\StringTok{, }\SpecialCharTok{\%.10f}\StringTok{]"} \OperatorTok{\%}\NormalTok{ (result[}\DecValTok{0}\NormalTok{], }
\NormalTok{        result[}\DecValTok{1}\NormalTok{]))}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"fHat of solution (computed over all data, D):"}\NormalTok{, }
\NormalTok{        fHat(result, X, Y))}
    \ControlFlowTok{else}\NormalTok{:}
        \BuiltInTok{print}\NormalTok{(}\StringTok{"No solution found"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{generateData} function samples data as desired for the specific problem.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ generateData(numPoints):}
\NormalTok{    X }\OperatorTok{=}\NormalTok{     np.random.normal(}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, numPoints) }
\NormalTok{    Y }\OperatorTok{=}\NormalTok{ X }\OperatorTok{+}\NormalTok{ np.random.normal(}\FloatTok{0.0}\NormalTok{, }\FloatTok{1.0}\NormalTok{, numPoints) }
    \ControlFlowTok{return}\NormalTok{ (X,Y)}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{predict} function takes in a solution \(\theta\) and an input \(X\), and produces as output the prediction of \(Y\). In other words, this function will implement \(\hat{y}(X, \theta)\).

\noindent Recall \(\hat{y}(X, \theta) = \theta_1 X + \theta_2\).
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ predict(theta, x):}
    \ControlFlowTok{return}\NormalTok{ theta[}\DecValTok{0}\NormalTok{] }\OperatorTok{+}\NormalTok{ theta[}\DecValTok{1}\NormalTok{] }\OperatorTok{*}\NormalTok{ x}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{fHat} function specifies the primary objective: to minimize the sample mean squared error. Because the attempt is to maximize \(\hat{f}\), however, the negative sample mean squared error is returned, so that maximizing \(\hat{f}\) corresponds to minimizing the mean squared error.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ fHat(theta, X, Y):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.size          }
\NormalTok{    res }\OperatorTok{=} \FloatTok{0.0}           
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):  }
\NormalTok{        prediction }\OperatorTok{=}\NormalTok{ predict(theta, X[i])                }
\NormalTok{        res }\OperatorTok{+=}\NormalTok{ (prediction }\OperatorTok{{-}}\NormalTok{ Y[i]) }\OperatorTok{*}\NormalTok{ (prediction }\OperatorTok{{-}}\NormalTok{ Y[i]) }
\NormalTok{    res }\OperatorTok{/=}\NormalTok{ n            }
    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{res         }
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{gHat1} and \texttt{gHat2} functions set up the behavioral constraints.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ gHat1(theta, X, Y):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.size          }
\NormalTok{    res }\OperatorTok{=}\NormalTok{ np.zeros(n)   }
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        prediction }\OperatorTok{=}\NormalTok{ predict(theta, X[i])                   }
\NormalTok{        res[i] }\OperatorTok{=}\NormalTok{ (prediction }\OperatorTok{{-}}\NormalTok{ Y[i]) }\OperatorTok{*}\NormalTok{ (prediction }\OperatorTok{{-}}\NormalTok{ Y[i])  }
\NormalTok{    res }\OperatorTok{=}\NormalTok{ res }\OperatorTok{{-}} \FloatTok{2.0}     
    \ControlFlowTok{return}\NormalTok{ res}

\KeywordTok{def}\NormalTok{ gHat2(theta, X, Y):}
\NormalTok{    n }\OperatorTok{=}\NormalTok{ X.size          }
\NormalTok{    res }\OperatorTok{=}\NormalTok{ np.zeros(n)   }
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(n):}
\NormalTok{        prediction }\OperatorTok{=}\NormalTok{ predict(theta, X[i])                   }
\NormalTok{        res[i] }\OperatorTok{=}\NormalTok{ (prediction }\OperatorTok{{-}}\NormalTok{ Y[i]) }\OperatorTok{*}\NormalTok{ (prediction }\OperatorTok{{-}}\NormalTok{ Y[i])  }
\NormalTok{    res }\OperatorTok{=} \FloatTok{1.25} \OperatorTok{{-}}\NormalTok{ res   }
    \ControlFlowTok{return}\NormalTok{ res}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{leastSq} function implements least squares linear regression, which will be used as a starting point in the search for a candidate solution.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ leastSq(X, Y):}
\NormalTok{    X }\OperatorTok{=}\NormalTok{ np.expand\_dims(X, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) }
\NormalTok{    Y }\OperatorTok{=}\NormalTok{ np.expand\_dims(Y, axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) }
\NormalTok{    reg }\OperatorTok{=}\NormalTok{ LinearRegression().fit(X, Y)}
\NormalTok{    theta0 }\OperatorTok{=}\NormalTok{ reg.intercept\_[}\DecValTok{0}\NormalTok{]   }
\NormalTok{    theta1 }\OperatorTok{=}\NormalTok{ reg.coef\_[}\DecValTok{0}\NormalTok{][}\DecValTok{0}\NormalTok{]     }
    \ControlFlowTok{return}\NormalTok{ np.array([theta0, theta1])}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{QSA} function is the shell code that partitions the data set, gets a candidate solution, and runs the safety test.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ QSA(X, Y, gHats, deltas):}

\NormalTok{    candidateData\_len }\OperatorTok{=} \FloatTok{0.40}
\NormalTok{    candidateData\_X, safetyData\_X, candidateData\_Y, safetyData\_Y }\OperatorTok{=} 
\NormalTok{    train\_test\_split(X, Y, test\_size}\OperatorTok{=}\DecValTok{1}\OperatorTok{{-}}\NormalTok{candidateData\_len, shuffle}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
  
\NormalTok{    candidateSolution }\OperatorTok{=}\NormalTok{ getCandidateSolution(candidateData\_X, }
\NormalTok{    candidateData\_Y, gHats, deltas, safetyData\_X.size)}

\NormalTok{    passedSafety      }\OperatorTok{=}\NormalTok{ safetyTest(candidateSolution, safetyData\_X, }
\NormalTok{    safetyData\_Y, gHats, deltas)}

    \ControlFlowTok{return}\NormalTok{ [candidateSolution, passedSafety]}
\end{Highlighting}
\end{Shaded}
\noindent The \texttt{safetyTest} function uses the previously defined functions to implement the safety test.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ safetyTest(candidateSolution, safetyData\_X, safetyData\_Y, gHats, }
\NormalTok{deltas):}

    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(gHats)):  }
\NormalTok{        g         }\OperatorTok{=}\NormalTok{ gHats[i]  }
\NormalTok{        delta     }\OperatorTok{=}\NormalTok{ deltas[i] }

    
\NormalTok{        g\_samples }\OperatorTok{=}\NormalTok{ g(candidateSolution, safetyData\_X, safetyData\_Y) }

\NormalTok{        upperBound }\OperatorTok{=}\NormalTok{ ttestUpperBound(g\_samples, delta) }

        \ControlFlowTok{if}\NormalTok{ upperBound }\OperatorTok{\textgreater{}} \FloatTok{0.0}\NormalTok{: }
            \ControlFlowTok{return} \VariableTok{False}

    \ControlFlowTok{return} \VariableTok{True}
\end{Highlighting}
\end{Shaded}
\noindent Finally, the \texttt{candidateObjective} and \texttt{getCandidateSolution} functions use a black-box optimization algorithm to search for a candidate solution. The black box algorithm used to search for a candidate solution is called Powell, which is an algorithm designed for finding a local minimum of a function using a bi-directional linear search. Powell, however, is not a constrained algorithm. One way of addressing this limitation is by incorporating the constraint into the objective function as a barrier function. In constrained optimization, a field of mathematics, barrier functions are used to replace inequality constraints by a penalizing term in the objective function that is easier to handle. That is, an approximate solution to the following unconstrained problem:

\[
\theta_c \in arg \: \underset{\theta \in \mathbb{R}^2}{max} 
    \begin{cases} 
      \hat{f}(\theta, D_1)  \text{    if} \:\: \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + 2 \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}} t_{1-\delta_i, |D_2|-1} \leq 0 \forall i \in \{1,2,...,n\}\\
      -100,000 - \sum_{i=1}^n max(0, \hat{\mu}(\hat{g_i}(\theta_c, D_1)) + 2 \frac{\hat{\sigma}(\hat{g_i}(\theta_c, D_1))}{\sqrt{|D_2|}} t_{1-\delta_i, |D_2|-1})) \text{  otherwise.}
    \end{cases}
\]

\noindent In this case, solutions that are predicted not to pass the safety test will not be selected by the optimization algorithm because a large negative performance is assigned to them. This barrier functions encourages Powell to tend towards solutions that will pass the safety test.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ candidateObjective(thetaToEvaluate, candidateData\_X, candidateData\_Y, }
\NormalTok{gHats, deltas, safetyDataSize): }

\NormalTok{    result }\OperatorTok{=}\NormalTok{ fHat(thetaToEvaluate, candidateData\_X, candidateData\_Y)}

\NormalTok{    predictSafetyTest }\OperatorTok{=} \VariableTok{True}     
    
    \ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\BuiltInTok{len}\NormalTok{(gHats)):  }
\NormalTok{        g         }\OperatorTok{=}\NormalTok{ gHats[i]       }
\NormalTok{        delta     }\OperatorTok{=}\NormalTok{ deltas[i]      }

\NormalTok{        g\_samples }\OperatorTok{=}\NormalTok{ g(thetaToEvaluate, candidateData\_X, }
\NormalTok{        candidateData\_Y)}

\NormalTok{        upperBound }\OperatorTok{=}\NormalTok{ predictTTestUpperBound(g\_samples, delta, }
\NormalTok{        safetyDataSize)}

        \ControlFlowTok{if}\NormalTok{ upperBound }\OperatorTok{\textgreater{}} \FloatTok{0.0}\NormalTok{:}

            \ControlFlowTok{if}\NormalTok{ predictSafetyTest:}
\NormalTok{                predictSafetyTest }\OperatorTok{=} \VariableTok{False}  

\NormalTok{                result }\OperatorTok{=} \OperatorTok{{-}}\FloatTok{100000.0}    

\NormalTok{            result }\OperatorTok{=}\NormalTok{ result }\OperatorTok{{-}}\NormalTok{ upperBound}

    \ControlFlowTok{return} \OperatorTok{{-}}\NormalTok{result  }
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ getCandidateSolution(candidateData\_X, candidateData\_Y, gHats, }
\NormalTok{deltas, safetyDataSize):}
  
\NormalTok{    minimizer\_method }\OperatorTok{=} \StringTok{\textquotesingle{}Powell\textquotesingle{}}
\NormalTok{    minimizer\_options}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}disp\textquotesingle{}}\NormalTok{: }\VariableTok{False}\NormalTok{\}}

\NormalTok{    initialSolution }\OperatorTok{=}\NormalTok{ leastSq(candidateData\_X, candidateData\_Y)}

\NormalTok{    res }\OperatorTok{=}\NormalTok{ minimize(}
\NormalTok{      candidateObjective, }
\NormalTok{      x0}\OperatorTok{=}\NormalTok{initialSolution, }
\NormalTok{      method}\OperatorTok{=}\NormalTok{minimizer\_method, options}\OperatorTok{=}\NormalTok{minimizer\_options, }
\NormalTok{      args}\OperatorTok{=}\NormalTok{(candidateData\_X, candidateData\_Y, gHats, deltas, }
\NormalTok{      safetyDataSize)}
\NormalTok{    )}

    \ControlFlowTok{return}\NormalTok{ res.x}
\end{Highlighting}
\end{Shaded}
\noindent Calling \texttt{main()} returns either a solution or NSF.
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{main()}
\end{Highlighting}
\end{Shaded}
\noindent The following code chunks utilize the above functions to perform the experimentation in Chapter \ref{exp}. \texttt{timeit} allows timing of the execution of experiments. \texttt{numba} allows the use of a Just-in-Time (JIT) compiler to accelerate Python code.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#import necessary packages}
\ImportTok{import}\NormalTok{ timeit               }
\ImportTok{from}\NormalTok{ numba }\ImportTok{import}\NormalTok{ jit       }
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#path where experiment results are saved}
\NormalTok{bin\_path }\OperatorTok{=} 
\CommentTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Thesis/index/\textquotesingle{}}
\CommentTok{\textquotesingle{}experiment\_results/chapter\_2/\textquotesingle{}}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ run\_experiments(worker\_id, nWorkers, ms, numM, numTrials, mTest):}
    
    \CommentTok{\# Results of the Seldonian algorithm runs}
    \CommentTok{\#\# The following code initializes an array filled with 0\textquotesingle{}s. }
    \CommentTok{\#\# The resulting array will have numTrials rows (each trial) }
    \CommentTok{\#\# and numM columns (each data set size).}
    \CommentTok{\#\# Default is 0=False.}
    
\NormalTok{    seldonian\_solutions\_found }\OperatorTok{=}\NormalTok{ np.zeros((numTrials, numM)) }
\NormalTok{    seldonian\_failures\_g1     }\OperatorTok{=}\NormalTok{ np.zeros((numTrials, numM)) }
\NormalTok{    seldonian\_failures\_g2     }\OperatorTok{=}\NormalTok{ np.zeros((numTrials, numM)) }
\NormalTok{    seldonian\_fs              }\OperatorTok{=}\NormalTok{ np.zeros((numTrials, numM)) }
    
    \CommentTok{\# Results of the Least{-}Squares (LS) linear regression runs}
\NormalTok{    LS\_solutions\_found }\OperatorTok{=}\NormalTok{ np.ones((numTrials, numM))  }
\NormalTok{    LS\_failures\_g1     }\OperatorTok{=}\NormalTok{ np.zeros((numTrials, numM)) }
\NormalTok{    LS\_failures\_g2     }\OperatorTok{=}\NormalTok{ np.zeros((numTrials, numM)) }
\NormalTok{    LS\_fs              }\OperatorTok{=}\NormalTok{ np.zeros((numTrials, numM)) }
    
    
    \CommentTok{\# Prepares file where experiment results will be saved}
\NormalTok{    experiment\_number }\OperatorTok{=}\NormalTok{ worker\_id}
\NormalTok{    outputFile }\OperatorTok{=}\NormalTok{ bin\_path }\OperatorTok{+} \StringTok{\textquotesingle{}results}\SpecialCharTok{\%d}\StringTok{.npz\textquotesingle{}} \OperatorTok{\%}\NormalTok{ experiment\_number}
    
    
    \CommentTok{\# Generate the data used to evaluate the primary objective }
    \CommentTok{\# and failure rates}
\NormalTok{    np.random.seed( (experiment\_number}\OperatorTok{+}\DecValTok{1}\NormalTok{) }\OperatorTok{*} \DecValTok{9999}\NormalTok{ )}
\NormalTok{    (testX, testY) }\OperatorTok{=}\NormalTok{ generateData(mTest) }
    
    
    \ControlFlowTok{for}\NormalTok{ trial }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(numTrials):}\CommentTok{\#numTrials trials for each value of m }
        \ControlFlowTok{for}\NormalTok{ (mIndex, m) }\KeywordTok{in} \BuiltInTok{enumerate}\NormalTok{(ms): }
          
            \CommentTok{\# Generate the training data, D}
\NormalTok{            base\_seed         }\OperatorTok{=}\NormalTok{ (experiment\_number }\OperatorTok{*}\NormalTok{ numTrials)}\OperatorTok{+}\DecValTok{1}
\NormalTok{            np.random.seed(base\_seed}\OperatorTok{+}\NormalTok{trial) }
\NormalTok{            (trainX, trainY)  }\OperatorTok{=}\NormalTok{ generateData(m)}
            
            \CommentTok{\# Run the Quasi{-}Seldonian algorithm}
\NormalTok{            (result, passedSafetyTest) }\OperatorTok{=}\NormalTok{ QSA(trainX, trainY, gHats, }
\NormalTok{            deltas)}
            
            \ControlFlowTok{if}\NormalTok{ passedSafetyTest:}
\NormalTok{                seldonian\_solutions\_found[trial, mIndex] }\OperatorTok{=} \DecValTok{1}                        
\NormalTok{                trueMSE }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{fHat(result, testX, testY)                               }
\NormalTok{                seldonian\_failures\_g1[trial, mIndex] }\OperatorTok{=} \DecValTok{1} 
                \ControlFlowTok{if}\NormalTok{ trueMSE }\OperatorTok{\textgreater{}} \FloatTok{2.0}  \ControlFlowTok{else} \DecValTok{0}   
\NormalTok{                seldonian\_failures\_g2[trial, mIndex] }\OperatorTok{=} \DecValTok{1} 
                \ControlFlowTok{if}\NormalTok{ trueMSE }\OperatorTok{\textless{}} \FloatTok{1.25} \ControlFlowTok{else} \DecValTok{0}   
\NormalTok{                seldonian\_fs[trial, mIndex] }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{trueMSE                              }
                
            \ControlFlowTok{else}\NormalTok{:}
\NormalTok{                seldonian\_solutions\_found[trial, mIndex] }\OperatorTok{=} \DecValTok{0}             
\NormalTok{                seldonian\_failures\_g1[trial, mIndex]     }\OperatorTok{=} \DecValTok{0}             
\NormalTok{                seldonian\_failures\_g2[trial, mIndex]     }\OperatorTok{=} \DecValTok{0}            
\NormalTok{                seldonian\_fs[trial, mIndex]              }\OperatorTok{=} \VariableTok{None}          

            \CommentTok{\# Run the Least Squares algorithm}
\NormalTok{            theta }\OperatorTok{=}\NormalTok{ leastSq(trainX, trainY)                              }
\NormalTok{            trueMSE }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{fHat(theta, testX, testY)                         }
\NormalTok{            LS\_failures\_g1[trial, mIndex] }\OperatorTok{=} \DecValTok{1} \ControlFlowTok{if}\NormalTok{ trueMSE }\OperatorTok{\textgreater{}} \FloatTok{2.0}  \ControlFlowTok{else} \DecValTok{0}   
\NormalTok{            LS\_failures\_g2[trial, mIndex] }\OperatorTok{=} \DecValTok{1} \ControlFlowTok{if}\NormalTok{ trueMSE }\OperatorTok{\textless{}} \FloatTok{1.25} \ControlFlowTok{else} \DecValTok{0}   
\NormalTok{            LS\_fs[trial, mIndex] }\OperatorTok{=} \OperatorTok{{-}}\NormalTok{trueMSE                             }
        
        
        
    \CommentTok{\# Save the arrays in a compressed format}
\NormalTok{    np.savez(outputFile, }
\NormalTok{             ms}\OperatorTok{=}\NormalTok{ms, }
\NormalTok{             seldonian\_solutions\_found}\OperatorTok{=}\NormalTok{seldonian\_solutions\_found,}
\NormalTok{             seldonian\_fs}\OperatorTok{=}\NormalTok{seldonian\_fs, }
\NormalTok{             seldonian\_failures\_g1}\OperatorTok{=}\NormalTok{seldonian\_failures\_g1, }
\NormalTok{             seldonian\_failures\_g2}\OperatorTok{=}\NormalTok{seldonian\_failures\_g2,}
\NormalTok{             LS\_solutions\_found}\OperatorTok{=}\NormalTok{LS\_solutions\_found,}
\NormalTok{             LS\_fs}\OperatorTok{=}\NormalTok{LS\_fs,}
\NormalTok{             LS\_failures\_g1}\OperatorTok{=}\NormalTok{LS\_failures\_g1,}
\NormalTok{             LS\_failures\_g2}\OperatorTok{=}\NormalTok{LS\_failures\_g2)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the behavioral constraints}
\NormalTok{gHats  }\OperatorTok{=}\NormalTok{ [gHat1, gHat2]}
\NormalTok{deltas }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{]}

\CommentTok{\# Initialize one worker because we\textquotesingle{}re not using parallelization}
\NormalTok{nWorkers }\OperatorTok{=} \DecValTok{1}  

\CommentTok{\# sample sizes}
\NormalTok{ms   }\OperatorTok{=}\NormalTok{ [}\DecValTok{2}\OperatorTok{**}\NormalTok{i }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{17}\NormalTok{)]  }
\NormalTok{numM }\OperatorTok{=} \BuiltInTok{len}\NormalTok{(ms)}
    
\CommentTok{\# The number of trials}
\NormalTok{numTrials }\OperatorTok{=} \DecValTok{100}  

\NormalTok{mTest }\OperatorTok{=}\NormalTok{ ms[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{] }\OperatorTok{*} \DecValTok{100} \CommentTok{\# about 5,000,000 test samples}

\CommentTok{\# Run experiments sequentially without parallelization}
\NormalTok{tic }\OperatorTok{=}\NormalTok{ timeit.default\_timer()}
\ControlFlowTok{for}\NormalTok{ worker\_id }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, nWorkers }\OperatorTok{+} \DecValTok{1}\NormalTok{):}
\NormalTok{    run\_experiments(worker\_id, nWorkers, ms, numM, numTrials, mTest)}
\NormalTok{toc }\OperatorTok{=}\NormalTok{ timeit.default\_timer()}
\NormalTok{time\_sequential }\OperatorTok{=}\NormalTok{ toc }\OperatorTok{{-}}\NormalTok{ tic }\CommentTok{\# Elapsed time in seconds}
\end{Highlighting}
\end{Shaded}
\noindent Finally, the following code chunks compile the results from the experiments and presents them visually. \texttt{csv} implements classes to read and write tabular data in CSV format. \texttt{glob} finds all the path names matching a specified pattern according to the rules used by the Unix shell. This will be useful for referencing file paths and names. \texttt{re} provides regular expression matching operations similar to those found in Perl.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#import necessary packages}
\ImportTok{import}\NormalTok{ csv }
\ImportTok{import}\NormalTok{ glob }
\ImportTok{import}\NormalTok{ re }
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt }
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#specify path names}
\NormalTok{bin\_path }\OperatorTok{=} 
\CommentTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Thesis/index/\textquotesingle{}}
\CommentTok{\textquotesingle{}experiment\_results/chapter\_2/\textquotesingle{}}
\NormalTok{csv\_path }\OperatorTok{=} 
\CommentTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Thesis/index/\textquotesingle{}}
\CommentTok{\textquotesingle{}experiment\_results/chapter\_2/csv/}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#parse through files to obtain the experiment numbers}
\KeywordTok{def}\NormalTok{ get\_existing\_experiment\_numbers():}
\NormalTok{    result\_files       }\OperatorTok{=}\NormalTok{ glob.glob(bin\_path }\OperatorTok{+} \StringTok{\textquotesingle{}results*.npz\textquotesingle{}}\NormalTok{)}
\NormalTok{    experiment\_numbers }\OperatorTok{=}\NormalTok{ [re.search(}\StringTok{\textquotesingle{}.*results([0{-}9]*).*\textquotesingle{}}\NormalTok{, }
\NormalTok{    fn, re.IGNORECASE) }\ControlFlowTok{for}\NormalTok{ fn }\KeywordTok{in}\NormalTok{ result\_files]}
\NormalTok{    experiment\_numbers }\OperatorTok{=}\NormalTok{ [}\BuiltInTok{int}\NormalTok{(i.group(}\DecValTok{1}\NormalTok{)) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ experiment\_numbers]}
\NormalTok{    experiment\_numbers.sort()}
    \ControlFlowTok{return}\NormalTok{ experiment\_numbers}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#generate the file names for the results}
\KeywordTok{def}\NormalTok{ genFilename(n):}
    \ControlFlowTok{return}\NormalTok{ bin\_path }\OperatorTok{+} \StringTok{\textquotesingle{}results}\SpecialCharTok{\%d}\StringTok{.npz\textquotesingle{}} \OperatorTok{\%}\NormalTok{ n}
\end{Highlighting}
\end{Shaded}
\noindent For the \texttt{addMoreResults} function, recall that:
\begin{itemize}
\tightlist
\item
  \texttt{ms}: data set size
\item
  \texttt{seldonian\_solutions\_found}: stores whether a solution was found (1=True,0=False)
\item
  \texttt{seldonian\_fs}: stores the primary objective values (fHat) if a solution was found
\item
  \texttt{seldonian\_failures\_g1}: stores whether Seldonian solution was unsafe, (1=True,0=False), for the 1st constraint, g\_1
\item
  \texttt{seldonian\_failures\_g2}: stores whether Seldonian solution was unsafe, (1=True,0=False), for the 2nd constraint, g\_2
\item
  \texttt{LS\_solutions\_found}: stores whether a solution was found. These will all be true (=1)
\item
  \texttt{LS\_fs}: stores the primary objective values (f)
\item
  \texttt{LS\_failures\_g1}: stores whether LS solution was unsafe, (1=True,0=False), for the 1st constraint, g\_1
\item
  \texttt{LS\_failures\_g2}: stores whether LS solution was unsafe, (1=True,0=False), for the 2nd constraint, g\_2
\end{itemize}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ addMoreResults(newFileId, ms, seldonian\_solutions\_found, seldonian\_fs,}
\NormalTok{seldonian\_failures\_g1, seldonian\_failures\_g2, LS\_solutions\_found, LS\_fs, }
\NormalTok{LS\_failures\_g1, LS\_failures\_g2):}

\NormalTok{    newFile }\OperatorTok{=}\NormalTok{ np.load(genFilename(newFileId))}
\NormalTok{    new\_ms                        }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}ms\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_seldonian\_solutions\_found }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}seldonian\_solutions\_found\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_seldonian\_fs              }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}seldonian\_fs\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_seldonian\_failures\_g1     }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}seldonian\_failures\_g1\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_seldonian\_failures\_g2     }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}seldonian\_failures\_g2\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_LS\_solutions\_found        }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}LS\_solutions\_found\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_LS\_fs                     }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}LS\_fs\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_LS\_failures\_g1            }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}LS\_failures\_g1\textquotesingle{}}\NormalTok{]}
\NormalTok{    new\_LS\_failures\_g2            }\OperatorTok{=}\NormalTok{ newFile[}\StringTok{\textquotesingle{}LS\_failures\_g2\textquotesingle{}}\NormalTok{]}

    \ControlFlowTok{if} \BuiltInTok{type}\NormalTok{(ms)}\OperatorTok{==}\BuiltInTok{type}\NormalTok{(}\VariableTok{None}\NormalTok{):}
        \ControlFlowTok{return}\NormalTok{ [new\_ms, new\_seldonian\_solutions\_found, new\_seldonian\_fs,}
\NormalTok{      new\_seldonian\_failures\_g1, new\_seldonian\_failures\_g2,}
\NormalTok{      new\_LS\_solutions\_found, new\_LS\_fs, new\_LS\_failures\_g1, }
\NormalTok{      new\_LS\_failures\_g2]}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        seldonian\_solutions\_found  }\OperatorTok{=} 
\NormalTok{        np.vstack([seldonian\_solutions\_found, new\_seldonian\_solutions\_found])}
\NormalTok{        seldonian\_fs               }\OperatorTok{=} 
\NormalTok{        np.vstack([seldonian\_fs,              new\_seldonian\_fs])}
\NormalTok{        seldonian\_failures\_g1      }\OperatorTok{=} 
\NormalTok{        np.vstack([seldonian\_failures\_g1,     new\_seldonian\_failures\_g1])}
\NormalTok{        seldonian\_failures\_g2      }\OperatorTok{=} 
\NormalTok{        np.vstack([seldonian\_failures\_g2,     new\_seldonian\_failures\_g2])}
\NormalTok{        LS\_solutions\_found         }\OperatorTok{=} 
\NormalTok{        np.vstack([LS\_solutions\_found,        new\_LS\_solutions\_found])}
\NormalTok{        LS\_fs                      }\OperatorTok{=} 
\NormalTok{        np.vstack([LS\_fs,                     new\_LS\_fs])}
\NormalTok{        LS\_failures\_g1             }\OperatorTok{=} 
\NormalTok{        np.vstack([LS\_failures\_g1,            new\_LS\_failures\_g1])}
\NormalTok{        LS\_failures\_g2             }\OperatorTok{=} 
\NormalTok{        np.vstack([LS\_failures\_g2,            new\_LS\_failures\_g2])}

        \ControlFlowTok{return}\NormalTok{ [ms, seldonian\_solutions\_found, seldonian\_fs, }
\NormalTok{        seldonian\_failures\_g1, seldonian\_failures\_g2, LS\_solutions\_found, }
\NormalTok{        LS\_fs, LS\_failures\_g1, LS\_failures\_g2]}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ stderror(v):}
\NormalTok{    non\_nan }\OperatorTok{=}\NormalTok{ np.count\_nonzero(}\OperatorTok{\textasciitilde{}}\NormalTok{np.isnan(v))        }
    \ControlFlowTok{return}\NormalTok{ np.nanstd(v, ddof}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{/}\NormalTok{ np.sqrt(non\_nan)}
\end{Highlighting}
\end{Shaded}
\noindent The output CSV file will have columns corresponding to:
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{m} -- the size of the data set
\item
  QSA mean value
\item
  QSA standard error bar size
\item
  LS mean value
\item
  LS standard error bar size
\end{enumerate}
\noindent There will be one column per value of \texttt{m} (amount of training data).
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ saveToCSV(ms, resultsQSA, resultsLS, filename):}
\NormalTok{    nCols }\OperatorTok{=}\NormalTok{ resultsQSA.shape[}\DecValTok{1}\NormalTok{]}

    \ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(filename, mode}\OperatorTok{=}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{) }\ImportTok{as} \BuiltInTok{file}\NormalTok{:}
\NormalTok{        writer }\OperatorTok{=}\NormalTok{ csv.writer(}\BuiltInTok{file}\NormalTok{, delimiter}\OperatorTok{=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{)}

        \ControlFlowTok{for}\NormalTok{ col }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(nCols):}

\NormalTok{            cur\_m          }\OperatorTok{=}\NormalTok{ ms[col]}
\NormalTok{            seldonian\_data }\OperatorTok{=}\NormalTok{ resultsQSA[:,col]}
\NormalTok{            LS\_data        }\OperatorTok{=}\NormalTok{ resultsLS[:,col]}

\NormalTok{            non\_nan }\OperatorTok{=}\NormalTok{ np.count\_nonzero(}\OperatorTok{\textasciitilde{}}\NormalTok{np.isnan(seldonian\_data))}
            \ControlFlowTok{if}\NormalTok{ non\_nan }\OperatorTok{\textgreater{}} \DecValTok{0}\NormalTok{:}
\NormalTok{                seldonian\_mean     }\OperatorTok{=}\NormalTok{ np.nanmean(seldonian\_data)}
\NormalTok{                seldonian\_stderror }\OperatorTok{=}\NormalTok{ stderror(seldonian\_data)}
            \ControlFlowTok{else}\NormalTok{:}
\NormalTok{                seldonian\_mean     }\OperatorTok{=} \StringTok{\textquotesingle{}NaN\textquotesingle{}}
\NormalTok{                seldonian\_stderror }\OperatorTok{=} \StringTok{\textquotesingle{}NaN\textquotesingle{}}

\NormalTok{            LS\_mean     }\OperatorTok{=}\NormalTok{ np.mean(LS\_data)}
\NormalTok{            LS\_stderror }\OperatorTok{=}\NormalTok{ stderror(LS\_data)}

\NormalTok{            writer.writerow([cur\_m, seldonian\_mean, seldonian\_stderror, }
\NormalTok{            LS\_mean, LS\_stderror])}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#gather the results and compile into CSV file}
\KeywordTok{def}\NormalTok{ gather\_results():}
\NormalTok{    ms                        }\OperatorTok{=} \VariableTok{None}
\NormalTok{    seldonian\_solutions\_found }\OperatorTok{=} \VariableTok{None}
\NormalTok{    seldonian\_fs              }\OperatorTok{=} \VariableTok{None}
\NormalTok{    seldonian\_failures\_g1     }\OperatorTok{=} \VariableTok{None}
\NormalTok{    seldonian\_failures\_g2     }\OperatorTok{=} \VariableTok{None}
\NormalTok{    LS\_solutions\_found        }\OperatorTok{=} \VariableTok{None}
\NormalTok{    LS\_fs                     }\OperatorTok{=} \VariableTok{None}
\NormalTok{    LS\_failures\_g1            }\OperatorTok{=} \VariableTok{None}
\NormalTok{    LS\_failures\_g2            }\OperatorTok{=} \VariableTok{None}

\NormalTok{    experiment\_numbers }\OperatorTok{=}\NormalTok{ get\_existing\_experiment\_numbers()}

    \ControlFlowTok{for}\NormalTok{ file\_idx }\KeywordTok{in}\NormalTok{ experiment\_numbers:}
\NormalTok{        res }\OperatorTok{=}\NormalTok{ addMoreResults(file\_idx, }
\NormalTok{            ms, }
\NormalTok{            seldonian\_solutions\_found, }
\NormalTok{            seldonian\_fs, seldonian\_failures\_g1, seldonian\_failures\_g2, }
\NormalTok{            LS\_solutions\_found, LS\_fs, LS\_failures\_g1, LS\_failures\_g2)}
        
\NormalTok{        [ms, }
\NormalTok{        seldonian\_solutions\_found, seldonian\_fs, seldonian\_failures\_g1,}
\NormalTok{        seldonian\_failures\_g2, LS\_solutions\_found, LS\_fs, LS\_failures\_g1, }
\NormalTok{        LS\_failures\_g2] }\OperatorTok{=}\NormalTok{ res}

\NormalTok{    saveToCSV(ms,  }
    \OperatorTok{{-}}\DecValTok{1}\OperatorTok{*}\NormalTok{seldonian\_fs,           }
    \OperatorTok{{-}}\DecValTok{1}\OperatorTok{*}\NormalTok{LS\_fs,           }
\NormalTok{    csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}fs.csv\textquotesingle{}}\NormalTok{) }\CommentTok{\# return MSE rather than negative MSE}
    
\NormalTok{    saveToCSV(ms,  }
\NormalTok{    seldonian\_solutions\_found,  }
\NormalTok{    LS\_solutions\_found, }
\NormalTok{    csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}solutions\_found.csv\textquotesingle{}}\NormalTok{)}
    
\NormalTok{    saveToCSV(ms,  }
\NormalTok{    seldonian\_failures\_g1,      }
\NormalTok{    LS\_failures\_g1,     }
\NormalTok{    csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}failures\_g1.csv\textquotesingle{}}\NormalTok{)}
    
\NormalTok{    saveToCSV(ms,  }
\NormalTok{    seldonian\_failures\_g2,      }
\NormalTok{    LS\_failures\_g2,     }
\NormalTok{    csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}failures\_g2.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{csv\_path }\OperatorTok{=} 
\CommentTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Thesis/index/\textquotesingle{}}
\CommentTok{\textquotesingle{}experiment\_results/chapter\_2/csv/\textquotesingle{}}
\NormalTok{img\_path }\OperatorTok{=} 
\CommentTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Thesis/index/\textquotesingle{}}
\CommentTok{\textquotesingle{}experiment\_results/chapter\_2/images/\textquotesingle{}}
\end{Highlighting}
\end{Shaded}
\noindent Finally, the results can be plotted as shown below and in Figures \ref{fig:fig4}, \ref{fig:fig5}, \ref{fig:fig6}, and \ref{fig:fig7}.
\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{def}\NormalTok{ loadAndPlotResults(fileName, ylabel, output\_file, is\_yAxis\_prob, }
\NormalTok{legend\_loc):}
  
\NormalTok{    file\_ms, file\_QSA, file\_QSA\_stderror, file\_LS, }
\NormalTok{    file\_LS\_stderror }\OperatorTok{=}\NormalTok{ np.loadtxt(fileName, delimiter}\OperatorTok{=}\StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{, unpack}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\NormalTok{    fig }\OperatorTok{=}\NormalTok{ plt.figure()}

\NormalTok{    plt.xlim(}\BuiltInTok{min}\NormalTok{(file\_ms), }\BuiltInTok{max}\NormalTok{(file\_ms))}
\NormalTok{    plt.xlabel(}\StringTok{"Amount of data (m)"}\NormalTok{, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{    plt.xscale(}\StringTok{\textquotesingle{}log\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.xticks(fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{    plt.ylabel(ylabel, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}

    \ControlFlowTok{if}\NormalTok{ is\_yAxis\_prob:}
\NormalTok{        plt.ylim(}\OperatorTok{{-}}\FloatTok{0.1}\NormalTok{, }\FloatTok{1.1}\NormalTok{)}
    \ControlFlowTok{else}\NormalTok{:}
\NormalTok{        plt.ylim(}\OperatorTok{{-}}\FloatTok{0.2}\NormalTok{, }\FloatTok{2.2}\NormalTok{)}
\NormalTok{        plt.plot([}\DecValTok{1}\NormalTok{, }\DecValTok{100000}\NormalTok{], [}\FloatTok{1.25}\NormalTok{, }\FloatTok{1.25}\NormalTok{], }\StringTok{\textquotesingle{}:k\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\NormalTok{        plt.plot([}\DecValTok{1}\NormalTok{, }\DecValTok{100000}\NormalTok{], [}\FloatTok{2.1}\NormalTok{,  }\FloatTok{2.1}\NormalTok{],  }\StringTok{\textquotesingle{}:k\textquotesingle{}}\NormalTok{)}\OperatorTok{;}      

\NormalTok{    plt.plot(     file\_ms, file\_QSA, }\StringTok{\textquotesingle{}b{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{3}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}QSA\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.errorbar( file\_ms, file\_QSA, yerr}\OperatorTok{=}\NormalTok{file\_QSA\_stderror, fmt}\OperatorTok{=}\StringTok{\textquotesingle{}.k\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
\NormalTok{    plt.plot(     file\_ms, file\_LS,  }\StringTok{\textquotesingle{}r{-}\textquotesingle{}}\NormalTok{, linewidth}\OperatorTok{=}\DecValTok{3}\NormalTok{, label}\OperatorTok{=}\StringTok{\textquotesingle{}LS\textquotesingle{}}\NormalTok{)}
\NormalTok{    plt.errorbar( file\_ms, file\_LS,  yerr}\OperatorTok{=}\NormalTok{file\_LS\_stderror, fmt}\OperatorTok{=}\StringTok{\textquotesingle{}.k\textquotesingle{}}\NormalTok{)}\OperatorTok{;}
    
\NormalTok{    plt.legend(loc}\OperatorTok{=}\NormalTok{legend\_loc, fontsize}\OperatorTok{=}\DecValTok{12}\NormalTok{)}
\NormalTok{    plt.tight\_layout()}

\NormalTok{    plt.savefig(output\_file)}
\NormalTok{    plt.show(block}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gather\_results()}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loadAndPlotResults(csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}fs.csv\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}Mean Squared Error\textquotesingle{}}\NormalTok{, }
\NormalTok{img\_path}\OperatorTok{+}\StringTok{\textquotesingle{}tutorial7MSE\_py.png\textquotesingle{}}\NormalTok{, }
\VariableTok{False}\NormalTok{, }
\StringTok{\textquotesingle{}lower right\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loadAndPlotResults(csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}solutions\_found.csv\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}Probability of Solution\textquotesingle{}}\NormalTok{,   }
\NormalTok{img\_path}\OperatorTok{+}\StringTok{\textquotesingle{}tutorial7PrSoln\_py.png\textquotesingle{}}\NormalTok{,  }
\VariableTok{True}\NormalTok{,  }
\StringTok{\textquotesingle{}best\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loadAndPlotResults(csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}failures\_g1.csv\textquotesingle{}}\NormalTok{,     }
\VerbatimStringTok{r\textquotesingle{}Probability of $g\_1(a(D))\textgreater{}0$\textquotesingle{}}\NormalTok{, }
\NormalTok{img\_path}\OperatorTok{+}\StringTok{\textquotesingle{}tutorial7PrFail1\_py.png\textquotesingle{}}\NormalTok{, }
\VariableTok{True}\NormalTok{,  }
\StringTok{\textquotesingle{}best\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loadAndPlotResults(csv\_path}\OperatorTok{+}\StringTok{\textquotesingle{}failures\_g2.csv\textquotesingle{}}\NormalTok{,     }
\VerbatimStringTok{r\textquotesingle{}Probability of $g\_2(a(D))\textgreater{}0$\textquotesingle{}}\NormalTok{, }
\NormalTok{img\_path}\OperatorTok{+}\StringTok{\textquotesingle{}tutorial7PrFail2\_py.png\textquotesingle{}}\NormalTok{, }
\VariableTok{True}\NormalTok{,  }
\StringTok{\textquotesingle{}best\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\hypertarget{appendix-c}{%
\chapter{SQL Script to Retrieve COMPAS Data}\label{appendix-c}}

\noindent The following SQL script was used to retrieve the COMPAS data set used in Chapters \ref{chap-3} and \ref{chap-4} from the public COMPAS database available through the ProPublica Data Store and accessible through GitHub. The database contains seven tables, although the \texttt{summary} table is empty. The SQLite code below returns 12160 rows of defendants from Broward County, Florida, who were assessed using the COMPAS tool for risk of recidivating. There are 29 variables of interest returned, though many more could be selected if interested.
Further data wrangling and data analysis are conducted in R, and significant findings are communicated in Chapters \ref{chap-3} and \ref{chap-4}.
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{select }

\NormalTok{\# select variables of interest }
\NormalTok{people.id,}
\NormalTok{compas.compas\_person\_id,}
\NormalTok{people.name,}
\NormalTok{people.first,}
\NormalTok{people.last,}
\NormalTok{people.sex,}
\NormalTok{people.race, }
\NormalTok{people.age,}
\NormalTok{people.age\_cat,}
\NormalTok{compas.marital\_status,}
\NormalTok{compas.custody\_status,}
\NormalTok{people.juv\_fel\_count,}
\NormalTok{people.juv\_misd\_count,}
\NormalTok{people.juv\_other\_count,}
\NormalTok{people.priors\_count,}
\NormalTok{people.days\_b\_screening\_arrest,}
\NormalTok{people.c\_days\_from\_compas,}
\NormalTok{people.c\_charge\_degree,}
\NormalTok{people.c\_charge\_desc,}
\NormalTok{compas.type\_of\_assessment,}
\NormalTok{compas.raw\_score,}
\NormalTok{people.decile\_score,}
\NormalTok{compas.score\_text,}
\NormalTok{people.is\_violent\_recid,}
\NormalTok{people.num\_vr\_cases,}
\NormalTok{people.is\_recid,}
\NormalTok{people.num\_r\_cases,}
\NormalTok{round(jail\_agg.days\_in\_jail) as days\_in\_jail,}
\NormalTok{round(prison\_agg.days\_in\_prison) as days\_in\_prison}

\NormalTok{from compas}

\NormalTok{\# join the \textquotesingle{}people\textquotesingle{} table}
\NormalTok{inner join people}
\NormalTok{on compas.person\_id = people.id}
\NormalTok{and compas.first = people.first}
\NormalTok{and compas.last = people.last}
\NormalTok{and compas.decile\_score = people.decile\_score}

\NormalTok{\# join jail history for each defendant}
\NormalTok{left join  (}

\NormalTok{select }

\NormalTok{jailhistory.person\_id,}
\NormalTok{jailhistory.first,}
\NormalTok{jailhistory.last,}
\NormalTok{jailhistory.out\_custody,}
\NormalTok{jailhistory.in\_custody,}
\NormalTok{sum(}
\NormalTok{  distinct(}
\NormalTok{    julianday(}
\NormalTok{      jailhistory.out\_custody}
\NormalTok{      ) {-} julianday(}
\NormalTok{        jailhistory.in\_custody}
\NormalTok{        )}
\NormalTok{    )}
\NormalTok{  ) as days\_in\_jail}

\NormalTok{from jailhistory}
    
\NormalTok{inner join compas }
\NormalTok{on compas.person\_id = jailhistory.person\_id}
\NormalTok{and compas.first = jailhistory.first}
\NormalTok{and compas.last = jailhistory.last}
\NormalTok{and jailhistory.in\_custody \textless{}= compas.screening\_date}
    
\NormalTok{group by 1,2,3}
    
\NormalTok{) as jail\_agg }
\NormalTok{on jail\_agg.person\_id = people.id}
\NormalTok{and jail\_agg.first = people.first}
\NormalTok{and jail\_agg.last = people.last}

\NormalTok{\# join prison history for each defendant}
\NormalTok{left join  (}

\NormalTok{select }

\NormalTok{prisonhistory.person\_id,}
\NormalTok{prisonhistory.first,}
\NormalTok{prisonhistory.last,}
\NormalTok{prisonhistory.out\_custody,}
\NormalTok{prisonhistory.in\_custody,}
\NormalTok{sum(}
\NormalTok{  distinct(}
\NormalTok{    julianday(}
\NormalTok{      prisonhistory.out\_custody}
\NormalTok{      ) {-} julianday(}
\NormalTok{        prisonhistory.in\_custody}
\NormalTok{        )}
\NormalTok{    )}
\NormalTok{  ) as days\_in\_prison}

\NormalTok{from prisonhistory}
    
\NormalTok{inner join compas }
\NormalTok{on compas.person\_id = prisonhistory.person\_id}
\NormalTok{and compas.first = prisonhistory.first}
\NormalTok{and compas.last = prisonhistory.last}
\NormalTok{and prisonhistory.in\_custody \textless{}= compas.screening\_date}
    
\NormalTok{group by 1,2,3}
    
\NormalTok{) as prison\_agg }
\NormalTok{on prison\_agg.person\_id = people.id}
\NormalTok{and prison\_agg.first = people.first}
\NormalTok{and prison\_agg.last = people.last}

\NormalTok{\# filter only for risk of recidivism}
\NormalTok{where type\_of\_assessment = \textquotesingle{}Risk of Recidivism\textquotesingle{}}
\end{Highlighting}
\end{Shaded}
\hypertarget{appendix-d}{%
\chapter{Logistic Regression on the COMPAS Data Set}\label{appendix-d}}

\noindent The R code chunk below fits a logistic regression model on the COMPAS data set.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# drop missing observations}
\NormalTok{compas }\OtherTok{\textless{}{-}}\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{drop\_na}\NormalTok{(compas) }

\CommentTok{\# train and test split}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(compas)}
\NormalTok{train\_index }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FloatTok{0.70} \SpecialCharTok{*}\NormalTok{ n) }
\NormalTok{test\_index }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, train\_index)}
\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ compas[train\_index, ]}
\NormalTok{test }\OtherTok{\textless{}{-}}\NormalTok{ compas[test\_index, ]}

\CommentTok{\# fit the logistic regression model on identified predictors}
\NormalTok{glm2 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(is\_recid }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sex }\SpecialCharTok{+}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ marital\_status }\SpecialCharTok{+}
\NormalTok{              juv\_offense }\SpecialCharTok{+}\NormalTok{ priors\_count,}
            \AttributeTok{data =}\NormalTok{ train,}
            \AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(logit))}

\CommentTok{\# obtain the binary predictions on train set}
\NormalTok{glm2augment }\OtherTok{\textless{}{-}}\NormalTok{ glm2 }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  broom}\SpecialCharTok{::}\FunctionTok{augment}\NormalTok{(}\AttributeTok{type.predict =} \StringTok{"response"}\NormalTok{)}
\NormalTok{glm2augment }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(glm2augment, }\AttributeTok{binprediction =} \FunctionTok{round}\NormalTok{(.fitted, }\DecValTok{0}\NormalTok{)) }

\CommentTok{\# obtain the binary predictions on test set }
\NormalTok{preds }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(glm2, }\AttributeTok{newdata=}\NormalTok{test, }\AttributeTok{type=}\StringTok{"response"}\NormalTok{)}
\NormalTok{test2 }\OtherTok{\textless{}{-}}\NormalTok{ test }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{preds =}\NormalTok{ preds,}
         \AttributeTok{prediction =} \FunctionTok{round}\NormalTok{(preds, }\DecValTok{0}\NormalTok{)) }

\CommentTok{\# obtain the ROC curve}
\NormalTok{roccurve1 }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(test, }\FunctionTok{roc}\NormalTok{(is\_recid }\SpecialCharTok{\textasciitilde{}}\NormalTok{ preds))}
\end{Highlighting}
\end{Shaded}
\hypertarget{appendix-e}{%
\chapter{Seldonian Classification on the COMPAS Data Set}\label{appendix-e}}

\noindent This appendix section walks through how a Seldonian algorithm was fit on the COMPAS data set in Chapter \ref{chap-3} --- the algorithm aimed to produce less disparate outcomes in error rates between Black and White defendants.

\noindent First, the Python environment can be set up in R using the \texttt{reticulate} package. The necessary Python libraries can then be imported as shown.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set up Python environment in R}
\FunctionTok{library}\NormalTok{(reticulate)}
\FunctionTok{use\_python}\NormalTok{(}\StringTok{"/cm/shared/apps/amh{-}Rstudio/python{-}3.11.4/bin/python3"}\NormalTok{, }
           \AttributeTok{required =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# import necessary libraries}
\ImportTok{import}\NormalTok{ numpy }\ImportTok{as}\NormalTok{ np}
\ImportTok{import}\NormalTok{ os}

\ImportTok{import}\NormalTok{ pandas }\ImportTok{as}\NormalTok{ pd}
\ImportTok{import}\NormalTok{ json}

\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ LabelEncoder}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ OneHotEncoder}
\ImportTok{from}\NormalTok{ sklearn.preprocessing }\ImportTok{import}\NormalTok{ StandardScaler}
\ImportTok{from}\NormalTok{ sklearn.compose }\ImportTok{import}\NormalTok{ ColumnTransformer}

\ImportTok{from}\NormalTok{ seldonian.utils.io\_utils }\ImportTok{import}\NormalTok{ save\_json}
\ImportTok{from}\NormalTok{ seldonian.parse\_tree.parse\_tree }\ImportTok{import}\NormalTok{ (ParseTree,}
\NormalTok{    make\_parse\_trees\_from\_constraints)}
\ImportTok{from}\NormalTok{ seldonian.dataset }\ImportTok{import}\NormalTok{ DataSetLoader}
\ImportTok{from}\NormalTok{ seldonian.utils.io\_utils }\ImportTok{import}\NormalTok{ (load\_json,save\_pickle)}
\ImportTok{from}\NormalTok{ seldonian.spec }\ImportTok{import}\NormalTok{ SupervisedSpec}
\ImportTok{from}\NormalTok{ seldonian.models.models }\ImportTok{import}\NormalTok{ (}
\NormalTok{    BinaryLogisticRegressionModel }\ImportTok{as}\NormalTok{ LogisticRegressionModel) }
\ImportTok{from}\NormalTok{ seldonian.models }\ImportTok{import}\NormalTok{ objectives}

\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}
\noindent Next, the data set needs to be formatted as appropriate for the Seldonian framework.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# point to the appropriate data file}
\NormalTok{f\_orig }\OperatorTok{=} \StringTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Data Sets/\textquotesingle{}}
\CommentTok{\textquotesingle{}COMPAS/compas\_seldonian\_bw.csv\textquotesingle{}}

\CommentTok{\# list the column names for modeling}
\NormalTok{columns\_orig }\OperatorTok{=}\NormalTok{ [}
    \StringTok{"race"}\NormalTok{,}\StringTok{"sex"}\NormalTok{,}\StringTok{"age"}\NormalTok{,}
    \StringTok{"age\_cat"}\NormalTok{,}\StringTok{"marital\_status"}\NormalTok{,}\StringTok{"juv\_offense"}\NormalTok{,}
    \StringTok{"priors\_count"}\NormalTok{, }\StringTok{"is\_recid"}\NormalTok{]}
    
\CommentTok{\#read in the data }
\NormalTok{df }\OperatorTok{=}\NormalTok{ pd.read\_csv(f\_orig, header}\OperatorTok{=}\DecValTok{1}\NormalTok{, names}\OperatorTok{=}\NormalTok{columns\_orig)}

\CommentTok{\# split into inputs and outputs}
\NormalTok{X }\OperatorTok{=}\NormalTok{ df.drop(columns}\OperatorTok{=}\NormalTok{[}\StringTok{"is\_recid"}\NormalTok{])}
\NormalTok{y }\OperatorTok{=}\NormalTok{ df[}\StringTok{"is\_recid"}\NormalTok{]}

\CommentTok{\# one hot encode categorical features,}
\CommentTok{\# scale numerical features using standard scaler }
\NormalTok{ct }\OperatorTok{=}\NormalTok{ ColumnTransformer([(}\StringTok{\textquotesingle{}c\textquotesingle{}}\NormalTok{,OneHotEncoder(),}
\NormalTok{[}\StringTok{\textquotesingle{}race\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}sex\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}age\_cat\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}marital\_status\textquotesingle{}}\NormalTok{]), }
\NormalTok{(}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{,StandardScaler(),[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}priors\_count\textquotesingle{}}\NormalTok{])])}
    
\CommentTok{\# apply transformation}
\NormalTok{X\_transformed }\OperatorTok{=}\NormalTok{ ct.fit\_transform(X)}
    
\CommentTok{\# get names after one{-}hot encoding}
\NormalTok{output\_columns }\OperatorTok{=}\NormalTok{ ct.get\_feature\_names\_out(ct.feature\_names\_in\_)}
    
\CommentTok{\# make an output dataframe to save transformed X and y}
\NormalTok{outdf }\OperatorTok{=}\NormalTok{ pd.DataFrame(X\_transformed,columns}\OperatorTok{=}\NormalTok{output\_columns)}

\CommentTok{\# change names of columns }
\NormalTok{outdf.rename(columns}\OperatorTok{=}\NormalTok{\{}\StringTok{\textquotesingle{}c\_\_race\_African{-}American\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Black\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_race\_Caucasian\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}White\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\_\_sex\_Female\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Female\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_sex\_Male\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Male\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\_\_age\_cat\_25 {-} 45\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}25\_45\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_age\_cat\_Greater than 45\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Greater\_than\_45\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_age\_cat\_Less than 25\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Less\_than\_25\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_marital\_status\_Divorced\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Divorced\textquotesingle{}}\NormalTok{,}
\StringTok{\textquotesingle{}c\_\_marital\_status\_Married\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Married\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_marital\_status\_Separated\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Separated\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_marital\_status\_Significant Other\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Significant\_Other\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_marital\_status\_Single\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Single\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_marital\_status\_Unknown\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}marital\_status\_Unknown\textquotesingle{}}\NormalTok{,}
\StringTok{\textquotesingle{}c\_\_marital\_status\_Widowed\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}Widowed\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}c\_\_juv\_offense\_0\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}juv\_offense\_0\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}c\_\_juv\_offense\_1\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}juv\_offense\_1\textquotesingle{}}\NormalTok{, }
\StringTok{\textquotesingle{}n\_\_age\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}n\_\_priors\_count\textquotesingle{}}\NormalTok{:}\StringTok{\textquotesingle{}priors\_count\textquotesingle{}}\NormalTok{ \},}
\NormalTok{inplace}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
    
\CommentTok{\# add label column and \textasciigrave{}juv\_offense\textasciigrave{} back into final dataframe}
\NormalTok{outdf[}\StringTok{\textquotesingle{}juv\_offense\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[}\StringTok{\textquotesingle{}juv\_offense\textquotesingle{}}\NormalTok{]}
\NormalTok{outdf[}\StringTok{\textquotesingle{}is\_recid\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ y}

\CommentTok{\# save final data frame}
\NormalTok{output\_path\_data}\OperatorTok{=}\StringTok{"/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/\textquotesingle{}}
\ErrorTok{\textquotesingle{}Data Sets/COMPAS/compas\_seldonian\_numeric\_clean\_bw.csv"}

\ErrorTok{outdf.to\_csv}\NormalTok{(output\_path\_data,index}\OperatorTok{=}\VariableTok{False}\NormalTok{,header}\OperatorTok{=}\VariableTok{False}\NormalTok{)}

\CommentTok{\# save metadata json file}
\NormalTok{output\_path\_metadata}\OperatorTok{=}\StringTok{"/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/\textquotesingle{}}
\ErrorTok{\textquotesingle{}Data Sets/COMPAS/metadata\_compas\_seldonian\_bw.json"}

\ErrorTok{metadata\_dict = \{}
  \CommentTok{"regime"}\NormalTok{:}\StringTok{"supervised\_learning"}\NormalTok{,}
  \CommentTok{"sub\_regime"}\NormalTok{:}\StringTok{"classification"}\NormalTok{,}
  \CommentTok{"all\_col\_names"}\NormalTok{:}\BuiltInTok{list}\NormalTok{(outdf.columns),}
  \CommentTok{"label\_col\_names"}\NormalTok{:}\StringTok{"is\_recid"}\NormalTok{,}
  \CommentTok{"sensitive\_col\_names"}\NormalTok{:[}\StringTok{"Black"}\NormalTok{, }\StringTok{"White"}\NormalTok{]}
\NormalTok{\}}
    
\ControlFlowTok{with} \BuiltInTok{open}\NormalTok{(output\_path\_metadata,}\StringTok{\textquotesingle{}w\textquotesingle{}}\NormalTok{) }\ImportTok{as}\NormalTok{ outfile:}
\NormalTok{    json.dump(metadata\_dict,outfile,indent}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\noindent Next, a specificatio object is needed.
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{import}\NormalTok{ autograd.numpy }\ImportTok{as}\NormalTok{ np}

\NormalTok{data\_pth }\OperatorTok{=}\NormalTok{ output\_path\_data}
\NormalTok{metadata\_pth }\OperatorTok{=}\NormalTok{ output\_path\_metadata}
\NormalTok{save\_dir }\OperatorTok{=} \StringTok{"/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/\textquotesingle{}}
\ErrorTok{\textquotesingle{}Python/COMPAS Application/"}
\ErrorTok{os.makedirs}\NormalTok{(save\_dir,exist\_ok}\OperatorTok{=}\VariableTok{True}\NormalTok{)}

\CommentTok{\# create data set from data and metadata file}
\NormalTok{regime}\OperatorTok{=}\StringTok{\textquotesingle{}supervised\_learning\textquotesingle{}}
\NormalTok{sub\_regime}\OperatorTok{=}\StringTok{\textquotesingle{}classification\textquotesingle{}}

\NormalTok{loader }\OperatorTok{=}\NormalTok{ DataSetLoader(regime}\OperatorTok{=}\NormalTok{regime)}

\NormalTok{dataset }\OperatorTok{=}\NormalTok{ loader.load\_supervised\_dataset(}
\NormalTok{  filename}\OperatorTok{=}\NormalTok{data\_pth,}
\NormalTok{  metadata\_filename}\OperatorTok{=}\NormalTok{metadata\_pth,}
\NormalTok{  file\_type}\OperatorTok{=}\StringTok{\textquotesingle{}csv\textquotesingle{}}\NormalTok{)}
  
\NormalTok{sensitive\_col\_names }\OperatorTok{=}\NormalTok{ dataset.meta.sensitive\_col\_names}

\CommentTok{\# use logistic regression model as the starting point}
\NormalTok{model }\OperatorTok{=}\NormalTok{ LogisticRegressionModel()}
    
\CommentTok{\# set the primary objective to be log loss}
\NormalTok{primary\_objective }\OperatorTok{=}\NormalTok{ objectives.binary\_logistic\_loss}

\ImportTok{from}\NormalTok{ seldonian.spec }\ImportTok{import}\NormalTok{ createSupervisedSpec}

\CommentTok{\# define behavioral constraints}
\NormalTok{epsilon }\OperatorTok{=} \FloatTok{0.2}
\NormalTok{constraint\_name }\OperatorTok{=} \StringTok{"equalized\_odds"}
\ControlFlowTok{if}\NormalTok{ constraint\_name }\OperatorTok{==} \StringTok{"equalized\_odds"}\NormalTok{:}
\NormalTok{  constraint\_strs }\OperatorTok{=}\NormalTok{ [}\SpecialStringTok{f\textquotesingle{}abs((FNR | [Black]) {-} (FNR | [White]))\textquotesingle{}} 
  \StringTok{\textquotesingle{}+ abs((FPR | [Black]) {-} (FPR | [White])) \textless{}= }\SpecialCharTok{\{epsilon\}}\StringTok{\textquotesingle{}}\NormalTok{] }
\NormalTok{deltas }\OperatorTok{=}\NormalTok{ [}\FloatTok{0.05}\NormalTok{]}

\CommentTok{\# create spec file}
\NormalTok{save\_dir }\OperatorTok{=} \StringTok{"/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Python/\textquotesingle{}}
\ErrorTok{\textquotesingle{}COMPAS Application/equalized\_odds\_0.2"}

\ErrorTok{createSupervisedSpec}\NormalTok{(}
\NormalTok{            dataset}\OperatorTok{=}\NormalTok{dataset,}
\NormalTok{            metadata\_pth}\OperatorTok{=}\NormalTok{metadata\_pth,}
\NormalTok{            constraint\_strs}\OperatorTok{=}\NormalTok{constraint\_strs,}
\NormalTok{            deltas}\OperatorTok{=}\NormalTok{deltas,}
\NormalTok{            save\_dir}\OperatorTok{=}\NormalTok{save\_dir,}
\NormalTok{            save}\OperatorTok{=}\VariableTok{True}\NormalTok{,}
\NormalTok{            verbose}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\noindent With the specification object ready, a Seldonian algorithm can now be run.
\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ seldonian.seldonian\_algorithm }\ImportTok{import}\NormalTok{ SeldonianAlgorithm}
\ImportTok{from}\NormalTok{ seldonian.utils.io\_utils }\ImportTok{import}\NormalTok{ load\_pickle}

\CommentTok{\# load the spec file {-}{-} can be replicated for the different epsilon values}
\NormalTok{specfile }\OperatorTok{=} \StringTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/Python/\textquotesingle{}}
\CommentTok{\textquotesingle{}COMPAS Application/equalized\_odds\_0.2/spec.pkl\textquotesingle{}}
\NormalTok{spec }\OperatorTok{=}\NormalTok{ load\_pickle(specfile)}
\NormalTok{SA\_02 }\OperatorTok{=}\NormalTok{ SeldonianAlgorithm(spec)}
\NormalTok{passed\_safety, solution }\OperatorTok{=}\NormalTok{ SA\_02.run(write\_cs\_logfile}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{ passed\_safety:}
  \BuiltInTok{print}\NormalTok{(}\StringTok{"Passed safety test!"}\NormalTok{)}
\ControlFlowTok{else}\NormalTok{:}
  \BuiltInTok{print}\NormalTok{(}\StringTok{"Failed safety test"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{()}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Primary objective (log loss) evaluated on safety dataset:"}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(SA\_02.evaluate\_primary\_objective(branch}\OperatorTok{=}\StringTok{\textquotesingle{}safety\_test\textquotesingle{}}\NormalTok{,}
\NormalTok{theta}\OperatorTok{=}\NormalTok{solution))}
\end{Highlighting}
\end{Shaded}
\noindent Finally, we used the the inverse logistic function to obtain the predictive values for each observation from the solution return.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get the solution {-}{-} can be replicated for the different epsilon values}
\NormalTok{SA\_02.cs\_result[}\StringTok{"candidate\_solution"}\NormalTok{]}

\CommentTok{\# store the coefficients}
\NormalTok{coefficients }\OperatorTok{=}\NormalTok{ SA\_02.cs\_result[}\StringTok{"candidate\_solution"}\NormalTok{]}

\CommentTok{\# get the intercept}
\NormalTok{intercept }\OperatorTok{=}\NormalTok{ coefficients[}\DecValTok{0}\NormalTok{]}

\CommentTok{\# separate the predictor variables from }
\CommentTok{\# the sensitive variable and the response variable}
\NormalTok{X\_outdf }\OperatorTok{=}\NormalTok{ outdf.drop(columns }\OperatorTok{=}\NormalTok{ [}\StringTok{\textquotesingle{}is\_recid\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Black\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}White\textquotesingle{}}\NormalTok{])}
\NormalTok{X\_sens }\OperatorTok{=}\NormalTok{ outdf[[}\StringTok{\textquotesingle{}Black\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}White\textquotesingle{}}\NormalTok{]]}
\NormalTok{y\_outdf }\OperatorTok{=}\NormalTok{ outdf[}\StringTok{\textquotesingle{}is\_recid\textquotesingle{}}\NormalTok{]}

\CommentTok{\# compute the predictive values}
\NormalTok{linear\_combination }\OperatorTok{=}\NormalTok{ np.dot(X\_outdf, coefficients[}\DecValTok{1}\NormalTok{:]) }\OperatorTok{+}\NormalTok{ intercept}
\NormalTok{pred\_probs\_02 }\OperatorTok{=} \DecValTok{1} \OperatorTok{/}\NormalTok{ (}\DecValTok{1} \OperatorTok{+}\NormalTok{ np.exp(}\OperatorTok{{-}}\NormalTok{linear\_combination))}
\end{Highlighting}
\end{Shaded}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store the results }
\NormalTok{seldonian\_results }\OperatorTok{=}\NormalTok{ pd.DataFrame(\{}
  \StringTok{\textquotesingle{}is\_recid\textquotesingle{}}\NormalTok{: y\_outdf, }
  \StringTok{\textquotesingle{}pred\_0.2\textquotesingle{}}\NormalTok{: pred\_probs\_02, }
  \StringTok{\textquotesingle{}pred\_0.1\textquotesingle{}}\NormalTok{: pred\_probs\_01, }
  \StringTok{\textquotesingle{}pred\_0.05\textquotesingle{}}\NormalTok{: pred\_probs\_005, }
  \StringTok{\textquotesingle{}pred\_0.01\textquotesingle{}}\NormalTok{: pred\_probs\_001\})}
\NormalTok{seldonian\_results }\OperatorTok{=}\NormalTok{ pd.concat([X\_outdf, X\_sens, seldonian\_results], }
\NormalTok{axis }\OperatorTok{=} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\noindent Finally, the binary risk predictions can be obtained using a 0.5 probability threshold.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# define threshold}
\NormalTok{threshold }\OperatorTok{=} \FloatTok{0.5}

\CommentTok{\# create risk columns}
\NormalTok{risk\_02 }\OperatorTok{=}\NormalTok{ np.where(pred\_probs\_02 }\OperatorTok{\textgreater{}=}\NormalTok{ threshold, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{risk\_01 }\OperatorTok{=}\NormalTok{ np.where(pred\_probs\_01 }\OperatorTok{\textgreater{}=}\NormalTok{ threshold, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{risk\_005 }\OperatorTok{=}\NormalTok{ np.where(pred\_probs\_005 }\OperatorTok{\textgreater{}=}\NormalTok{ threshold, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{risk\_001 }\OperatorTok{=}\NormalTok{ np.where(pred\_probs\_001 }\OperatorTok{\textgreater{}=}\NormalTok{ threshold, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{\# add risk columns to data frame}
\NormalTok{seldonian\_results[}\StringTok{\textquotesingle{}risk\_0.2\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ risk\_02}
\NormalTok{seldonian\_results[}\StringTok{\textquotesingle{}risk\_0.1\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ risk\_01}
\NormalTok{seldonian\_results[}\StringTok{\textquotesingle{}risk\_0.05\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ risk\_005}
\NormalTok{seldonian\_results[}\StringTok{\textquotesingle{}risk\_0.01\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ risk\_001}

\CommentTok{\# write the data frame to a CSV file}
\NormalTok{seldonian\_results.to\_csv(}\StringTok{\textquotesingle{}/home/dasienga24/Statistics{-}Senior{-}Honors{-}Thesis/\textquotesingle{}}
\StringTok{\textquotesingle{}Data Sets/COMPAS/compas\_seldonian\_results\_bw.csv\textquotesingle{}}\NormalTok{, index}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\hypertarget{appendix-f}{%
\chapter{Generating the Simulation Parent Data Set}\label{appendix-f}}

\noindent This appendix section displays the R code used to generate the parent simulation data set for Chapter \ref{chap-4}.
\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\# define a linear combination of predictors as desired}
\NormalTok{linear\_combination }\OtherTok{=} \DecValTok{5}  \SpecialCharTok{{-}} \FloatTok{0.2} \SpecialCharTok{*}\NormalTok{ compas\_sim}\SpecialCharTok{$}\NormalTok{age }\SpecialCharTok{+} 
  \FunctionTok{ifelse}\NormalTok{(compas\_sim}\SpecialCharTok{$}\NormalTok{prior\_offense }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{0}\NormalTok{)}

\CommentTok{\# pass through an inverse{-}logit function}
\NormalTok{probs }\OtherTok{=} \FunctionTok{exp}\NormalTok{(linear\_combination) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(linear\_combination))}

\CommentTok{\# generate Bernoulli RVs for y}
\NormalTok{is\_recid\_sim }\OtherTok{=} \FunctionTok{rbinom}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(compas\_sim), }\DecValTok{1}\NormalTok{, probs)}

\CommentTok{\# join to original data frame}
\NormalTok{compas\_sim\_balanced\_final }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(compas\_sim, is\_recid\_sim)}

\CommentTok{\# induce balance}
\NormalTok{compas\_b\_y }\OtherTok{\textless{}{-}}\NormalTok{ compas\_sim\_balanced\_final }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(race }\SpecialCharTok{==} \StringTok{"African{-}American"} \SpecialCharTok{\&}\NormalTok{ is\_recid\_sim }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}

\NormalTok{compas\_b\_n }\OtherTok{\textless{}{-}}\NormalTok{ compas\_sim\_balanced\_final }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(race }\SpecialCharTok{==} \StringTok{"African{-}American"} \SpecialCharTok{\&}\NormalTok{ is\_recid\_sim }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}

\NormalTok{compas\_w\_y }\OtherTok{\textless{}{-}}\NormalTok{ compas\_sim\_balanced\_final }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(race }\SpecialCharTok{==} \StringTok{"Caucasian"} \SpecialCharTok{\&}\NormalTok{ is\_recid\_sim }\SpecialCharTok{==} \DecValTok{1}\NormalTok{)}

\NormalTok{compas\_w\_n }\OtherTok{\textless{}{-}}\NormalTok{ compas\_sim\_balanced\_final }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(race }\SpecialCharTok{==} \StringTok{"Caucasian"} \SpecialCharTok{\&}\NormalTok{ is\_recid\_sim }\SpecialCharTok{==} \DecValTok{0}\NormalTok{)}

\NormalTok{compas\_b\_y\_balanced }\OtherTok{\textless{}{-}}
\NormalTok{  compas\_b\_y[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(compas\_b\_y), }\DecValTok{1250}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), ]}
\NormalTok{compas\_b\_n\_balanced }\OtherTok{\textless{}{-}}
\NormalTok{  compas\_b\_n[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(compas\_b\_n), }\DecValTok{1250}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), ]}
\NormalTok{compas\_w\_y\_balanced }\OtherTok{\textless{}{-}}
\NormalTok{  compas\_w\_y[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(compas\_w\_y), }\DecValTok{1250}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), ]}
\NormalTok{compas\_w\_n\_balanced }\OtherTok{\textless{}{-}}
\NormalTok{  compas\_w\_n[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(compas\_w\_n), }\DecValTok{1250}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), ]}

\NormalTok{compas\_sim\_balanced\_final }\OtherTok{\textless{}{-}} \FunctionTok{rbind}\NormalTok{(}
\NormalTok{  compas\_b\_y\_balanced,}
\NormalTok{  compas\_b\_n\_balanced,}
\NormalTok{  compas\_w\_y\_balanced,}
\NormalTok{  compas\_w\_n\_balanced}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\backmatter

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\noindent

\setlength{\parindent}{-0.20in}
\setlength{\leftskip}{0.20in}
\setlength{\parskip}{8pt}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-agarwal2019fair}{}}%
Agarwal, A., Dudík, M., \& Wu, Z. S. (2019). Fair regression: Quantitative definitions and reduction-based algorithms. In \emph{International conference on machine learning} (pp. 120--129). PMLR.

\leavevmode\vadjust pre{\hypertarget{ref-angwin2016machine}{}}%
Angwin, J., Larson, J., Mattu, S., \& Kirchner, L. (2016). Machine bias risk assessments in criminal sentencing. \emph{ProPublica, May}, \emph{23}.

\leavevmode\vadjust pre{\hypertarget{ref-asimov1994forward}{}}%
Asimov, I. (1994). \emph{Forward the foundation} (Vol. 7). Spectra.

\leavevmode\vadjust pre{\hypertarget{ref-boyd2004convex}{}}%
Boyd, S. P., \& Vandenberghe, L. (2004). \emph{Convex optimization}. Cambridge university press.

\leavevmode\vadjust pre{\hypertarget{ref-compasdata}{}}%
Broward County Clerk's Office, Broward County Sherrif's Office, Florida Department of Corrections, \& ProPublica. (2024). {COMPAS Recidivism Risk Score Data} {[}Data set{]}. ProPublica Data Store. Retrieved from \url{https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis}

\leavevmode\vadjust pre{\hypertarget{ref-castelnovo2022clarification}{}}%
Castelnovo, A., Crupi, R., Greco, G., Regoli, D., Penco, I. G., \& Cosentini, A. C. (2022). A clarification of the nuances in the fairness metrics landscape. \emph{Scientific Reports}, \emph{12}(1), 4209.

\leavevmode\vadjust pre{\hypertarget{ref-chouldechova2017fair}{}}%
Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. \emph{Big Data}, \emph{5}(2), 153--163.

\leavevmode\vadjust pre{\hypertarget{ref-chouldechova2018frontiers}{}}%
Chouldechova, A., \& Roth, A. (2018). The frontiers of fairness in machine learning. \emph{arXiv Preprint arXiv:1810.08810}.

\leavevmode\vadjust pre{\hypertarget{ref-durahly2023fairness}{}}%
Durahly, L. (2023). A gentle introduction to ML fairness metrics. Retrieved from \url{https://superwise.ai/blog/gentle-introduction-ml-fairness-metrics/}

\leavevmode\vadjust pre{\hypertarget{ref-larson2016compas}{}}%
Larson, J., Mattu, S., Kirchner, L., \& Angwin, J. (2016). How we analyzed the COMPAS recidivism algorithm. \emph{ProPublica, May}, \emph{23}.

\leavevmode\vadjust pre{\hypertarget{ref-mehrabi2021survey}{}}%
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., \& Galstyan, A. (2021). A survey on bias and fairness in machine learning. \emph{ACM Computing Surveys (CSUR)}, \emph{54}(6), 1--35.

\leavevmode\vadjust pre{\hypertarget{ref-mohajon2021confmatrix}{}}%
Mohajon, J. (2021). Confusion matrix for your multi-class machine learning model. Retrieved from \url{https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826}

\leavevmode\vadjust pre{\hypertarget{ref-saeed2015evidence}{}}%
Saeed, S., Alireza, B., Mohamed, E., \& Ahmed, N. (2015). Evidence based emergency medicine part 2: Positive and negative predictive values of diagnostic tests.

\leavevmode\vadjust pre{\hypertarget{ref-DVNux2fO35FW8_2019}{}}%
Silva, B. C. da. (2019). {UFRGS Entrance Exam and GPA Data} (Version V2) {[}Data set{]}. Harvard Dataverse. http://doi.org/\href{https://doi.org/10.7910/DVN/O35FW8}{10.7910/DVN/O35FW8}

\leavevmode\vadjust pre{\hypertarget{ref-sentencingproject}{}}%
The Sentencing Project. (2018). Report to the united nations on racial disparities in the u.s. Criminal justice system. Retrieved from \url{https://www.sentencingproject.org/reports/report-to-the-united-nations-on-racial-disparities-in-the-u-s-criminal-justice-system/}

\leavevmode\vadjust pre{\hypertarget{ref-thomas2020housetestimony}{}}%
Thomas, P. (2020). Testimony to the house committee on financial services task force on artificial intelligence hearing: {``Equitable algorithms: Examining ways to reduce AI bias in financial services.''} Retrieved from \url{https://www.congress.gov/116/meeting/house/110499/witnesses/HHRG-116-BA00-Wstate-ThomasP-20200212.pdf}

\leavevmode\vadjust pre{\hypertarget{ref-aisafety}{}}%
Thomas, P. S. (n.d.). AI safety. \url{https://aisafety.cs.umass.edu/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-thomas2019preventing}{}}%
Thomas, P. S., Castro da Silva, B., Barto, A. G., Giguere, S., Brun, Y., \& Brunskill, E. (2019a). Preventing undesirable behavior of intelligent machines. \emph{Science}, \emph{366}(6468), 999--1004.

\leavevmode\vadjust pre{\hypertarget{ref-thomas2019supplementary}{}}%
Thomas, P. S., Castro da Silva, B., Barto, A. G., Giguere, S., Brun, Y., \& Brunskill, E. (2019b). Supplementary materials for preventing undesirable behavior of intelligent machines. \emph{Science}, \emph{366}(6468), 999--1004.

\end{CSLReferences}
% Index?

\end{document}
